[0m2021.02.25 09:53:55 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.53.2.[0m
[0m2021.02.25 09:53:55 INFO  time: initialize in 0.44s[0m
[0m2021.02.25 09:53:56 WARN  Build server is not auto-connectable.[0m
[0m2021.02.25 09:54:21 WARN  no build target for: /home/amburkee/scala_s3_read/project/plugins.sbt[0m
[0m2021.02.25 09:54:21 WARN  no build target for: /home/amburkee/scala_s3_read/project/plugins.sbt[0m
[0m2021.02.25 09:54:21 INFO  skipping build import with status 'Requested'[0m
[0m2021.02.25 09:54:22 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m




[0m2021.02.25 09:54:23 INFO  time: code lens generation in 2.04s[0m
[0m2021.02.25 09:54:26 WARN  no build target for: /home/amburkee/scala_s3_read/project/plugins.sbt[0m
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.15.0")
[0m2021.02.25 09:54:45 WARN  no build target for: /home/amburkee/scala_s3_read/build.sbt[0m
import Dependencies._

ThisBuild / scalaVersion     := "2.13.4"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.example"
ThisBuild / organizationName := "example"

lazy val root = (project in file("."))
  .settings(
    name := "scala_s3_read",
    libraryDependencies += scalaTest % Test
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.25 09:55:12 WARN  no build target for: /home/amburkee/scala_s3_read/build.sbt[0m
import Dependencies._

// Mac scalaVersion
// ThisBuild / scalaVersion := "2.12.13"
ThisBuild / scalaVersion := "2.11.12"
ThisBuild / version := "0.1.0-SNAPSHOT"
ThisBuild / organization := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.0.0"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.
[0m2021.02.25 09:55:12 INFO  skipping build import with status 'Dismissed'[0m
import Dependencies._

// Mac scalaVersion
// ThisBuild / scalaVersion := "2.12.13"
ThisBuild / scalaVersion := "2.11.12"
ThisBuild / version := "0.1.0-SNAPSHOT"
ThisBuild / organization := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.0.0"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.
[0m2021.02.25 09:55:24 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m



import Dependencies._

// Mac scalaVersion
// ThisBuild / scalaVersion := "2.12.13"
ThisBuild / scalaVersion := "2.11.12"
ThisBuild / version := "0.1.0-SNAPSHOT"
ThisBuild / organization := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.0.0"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.
Feb 25, 2021 9:55:26 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 30
import Dependencies._

// Mac scalaVersion
// ThisBuild / scalaVersion := "2.12.13"
ThisBuild / scalaVersion := "2.11.12"
ThisBuild / version := "0.1.0-SNAPSHOT"
ThisBuild / organization := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.0.0"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.
Feb 25, 2021 9:55:31 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 36










[0m2021.02.25 09:55:45 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
package com.revature


[0m2021.02.25 09:55:51 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("DAS_KEY_ID"))
    val secret = System.getenv(("DAS_SEC"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
[0m2021.02.25 09:56:28 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("ACCESS_TOKEN"))
    val secret = System.getenv(("ACCESS_TOKEN_SECRET"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
[0m2021.02.25 09:56:59 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://usf-210104-big-data/twitterstream/tweetstream-1613536993819-1")
    s3DataMaybe.show()
  }
}
[0m2021.02.25 10:00:41 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://big-data-p2-ambur/data/")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://big-data-p2-ambur/data/")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://big-data-p2-ambur/data/")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://big-data-p2-ambur/data/")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://big-data-p2-ambur/data/")
    s3DataMaybe.show()
  }
}
[0m2021.02.25 10:01:13 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://big-data-p2-ambur/data/")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://big-data-p2-ambur/data/")
    s3DataMaybe.show()
  }
}
[0m2021.02.25 10:02:04 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://big-data-p2-ambur/data/")
    s3DataMaybe.show()
  }
}
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://big-data-p2-ambur/data/")
    s3DataMaybe.show()
  }
}
[0m2021.02.25 10:03:51 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals5176954334607782277/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:03:52 WARN  no build target for: /home/amburkee/scala_s3_read/project/metals.sbt[0m
[0m2021.02.25 10:03:52 WARN  no build target for: /home/amburkee/scala_s3_read/project/project/metals.sbt[0m
// DO NOT EDIT! This file is auto-generated.
// This file enables sbt-bloop to create bloop config files.

addSbtPlugin("ch.epfl.scala" % "sbt-bloop" % "1.4.8")

[0m2021.02.25 10:03:51 INFO  skipping build import with status 'Started'[0m
// DO NOT EDIT! This file is auto-generated.
// This file enables sbt-bloop to create bloop config files.

addSbtPlugin("ch.epfl.scala" % "sbt-bloop" % "1.4.8")

[0m2021.02.25 10:03:52 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:03:53 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals6569913758293509645/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:03:53 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals1537362232558391835/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:03:53 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals6002052850508985726/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:03:54 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals6589229330179031388/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:03:54 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:03:55 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:03:56 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:03:56 INFO  sbt bloopInstall exit: 2[0m
[0m2021.02.25 10:03:56 INFO  time: ran 'sbt bloopInstall' in 2.17s[0m
[0m2021.02.25 10:03:56 ERROR sbt command failed: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals6589229330179031388/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.02.25 10:03:56 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7657550173048141748/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7657550173048141748/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7657550173048141748/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:03:56 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:03:56 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:03:56 INFO  time: Connected to build server in 0.46s[0m
[0m2021.02.25 10:03:56 INFO  Connected to Build server: Bloop v1.4.6-21-464e4ec4[0m
[0m2021.02.25 10:03:56 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:03:56 ERROR Empty build targets. Expected at least one build target identifier.[0m
[0m2021.02.25 10:03:56 ERROR Empty build targets. Expected at least one build target identifier.[0m
[0m2021.02.25 10:03:56 ERROR Empty build targets. Expected at least one build target identifier.[0m
[0m2021.02.25 10:03:57 INFO  time: Imported build in 0.13s[0m
[0m2021.02.25 10:03:57 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:03:57 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:03:57 INFO  time: indexed workspace in 0.28s[0m
[0m2021.02.25 10:03:56 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
[0m2021.02.25 10:03:58 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project/project[0m
[0m2021.02.25 10:03:58 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package com.revature

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val s3DataMaybe = spark.read.text("s3a://big-data-p2-ambur/data/")
    s3DataMaybe.show()
  }
}
[0m2021.02.25 10:03:58 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project/project[0m
[0m2021.02.25 10:03:58 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:03:58 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project/project[0m
[0m2021.02.25 10:03:59 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project/project[0m
[0m2021.02.25 10:04:02 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:04:02 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project[0m
[0m2021.02.25 10:04:04 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:04:04 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:04:04 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:04:04 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project[0m
[0m2021.02.25 10:04:04 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project[0m
[0m2021.02.25 10:04:04 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project[0m
[0m2021.02.25 10:04:04 ERROR [info] waiting for lock on /home/amburkee/.ivy2/.sbt.ivy.lock to be available...[0m
[0m2021.02.25 10:04:04 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 10:04:04 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 10:04:05 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 10:04:05 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.02.25 10:04:05 INFO  [success] Total time: 3 s, completed Feb 25, 2021 10:04:05 AM[0m
[0m2021.02.25 10:04:06 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.02.25 10:04:06 INFO  [info] loading settings for project scala_s3_read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 10:04:06 INFO  [success] Total time: 2 s, completed Feb 25, 2021 10:04:06 AM[0m
[0m2021.02.25 10:04:06 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project[0m
[0m2021.02.25 10:04:06 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 10:04:06 INFO  [info] loading settings for project scala_s3_read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 10:04:06 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project[0m
[0m2021.02.25 10:04:07 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.02.25 10:04:06 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 10:04:07 INFO  [success] Total time: 3 s, completed Feb 25, 2021 10:04:07 AM[0m
[0m2021.02.25 10:04:07 INFO  [info] loading settings for project scala_s3_read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 10:04:07 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project[0m
[0m2021.02.25 10:04:07 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 10:04:08 INFO  [success] Generated .bloop/scala_s3_read-build.json[0m
[0m2021.02.25 10:04:08 INFO  [info] compiling 1 Scala source to /home/amburkee/scala_s3_read/project/target/scala-2.12/sbt-1.0/classes ...[0m
[0m2021.02.25 10:04:09 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.02.25 10:04:09 INFO  [success] Total time: 5 s, completed Feb 25, 2021 10:04:09 AM[0m
[0m2021.02.25 10:04:09 INFO  user cancelled sbt bloopInstall[0m
[0m2021.02.25 10:04:09 INFO  time: ran 'sbt bloopInstall' in 15s[0m
[0m2021.02.25 10:04:09 INFO  [success] Generated .bloop/scala_s3_read-build.json[0m
[0m2021.02.25 10:04:09 INFO  [info] compiling 1 Scala source to /home/amburkee/scala_s3_read/project/target/scala-2.12/sbt-1.0/classes ...[0m
[0m2021.02.25 10:04:10 INFO  [success] Generated .bloop/scala_s3_read-build.json[0m
[0m2021.02.25 10:04:09 INFO  sbt bloopInstall exit: 143[0m
[0m2021.02.25 10:04:10 INFO  [info] compiling 1 Scala source to /home/amburkee/scala_s3_read/project/target/scala-2.12/sbt-1.0/classes ...[0m
[0m2021.02.25 10:04:09 INFO  user cancelled sbt bloopInstall[0m
[0m2021.02.25 10:04:10 INFO  time: ran 'sbt bloopInstall' in 17s[0m
[0m2021.02.25 10:04:10 INFO  sbt bloopInstall exit: 143[0m
[0m2021.02.25 10:04:12 INFO  [info] done compiling[0m
[0m2021.02.25 10:04:12 INFO  [success] Total time: 5 s, completed Feb 25, 2021 10:04:12 AM[0m
[0m2021.02.25 10:04:13 INFO  [info] done compiling[0m
[0m2021.02.25 10:04:13 INFO  [success] Total time: 5 s, completed Feb 25, 2021 10:04:13 AM[0m
[0m2021.02.25 10:04:14 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 10:04:14 INFO  [info] set current project to scalas3read (in build file:/home/amburkee/scala_s3_read/)[0m
[0m2021.02.25 10:04:15 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 10:04:15 INFO  [info] set current project to scalas3read (in build file:/home/amburkee/scala_s3_read/)[0m
[0m2021.02.25 10:04:18 INFO  user cancelled sbt bloopInstall[0m
[0m2021.02.25 10:04:18 INFO  time: ran 'sbt bloopInstall' in 25s[0m
[0m2021.02.25 10:04:18 INFO  sbt bloopInstall exit: 143[0m
[0m2021.02.25 10:04:30 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 10:04:30 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 10:04:30 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 10:04:30 INFO  [success] Total time: 16 s, completed Feb 25, 2021 10:04:30 AM[0m
[0m2021.02.25 10:04:30 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 10:04:30 INFO  time: ran 'sbt bloopInstall' in 38s[0m
[0m2021.02.25 10:04:30 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:04:30 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
[0m2021.02.25 10:04:30 INFO  Attempting to connect to the build server...[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2292127323735383211/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2292127323735383211/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2292127323735383211/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:04:30 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:04:37 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.25 10:04:37Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6860812383066706934/bsp.socket'... INFO 
 Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7789852905976514254/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6860812383066706934/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6860812383066706934/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7789852905976514254/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7789852905976514254/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:04:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:04:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:04:40 INFO  time: Connected to build server in 9.82s[0m
[0m2021.02.25 10:04:40 INFO  Connected to Build server: Bloop v1.4.6-21-464e4ec4[0m
[0m2021.02.25 10:04:42 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:04:44 INFO  time: indexed workspace in 4.34s[0m
[0m2021.02.25 10:04:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:04:50 INFO  time: compiled root in 4.69s[0m
[0m2021.02.25 10:05:40 INFO  compiling scala_s3_read-build (1 scala source)[0m
[0m2021.02.25 10:05:43 INFO  skipping build import with status 'Installed'[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

// Mac scalaVersion
// ThisBuild / scalaVersion := "2.12.13"
ThisBuild / scalaVersion := "2.11.12"
ThisBuild / version := "0.1.0-SNAPSHOT"
ThisBuild / organization := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.0.0"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.
[0m2021.02.25 10:05:43 INFO  time: compiled scala_s3_read-build in 2.83s[0m
[0m2021.02.25 10:05:44 INFO  time: code lens generation in 3.76s[0m
[0m2021.02.25 10:05:57 INFO  skipping build import with status 'Installed'[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild / scalaVersion := "2.11.12"
ThisBuild / version := "0.1.0-SNAPSHOT"
ThisBuild / organization := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.0.0"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.
Feb 25, 2021 10:06:10 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 434
Feb 25, 2021 10:08:04 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 527
Feb 25, 2021 10:08:04 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 532
[0m2021.02.25 10:09:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:09:09 INFO  time: compiled root in 0.46s[0m
Feb 25, 2021 10:09:26 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 715
Feb 25, 2021 10:09:26 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 714
[0m2021.02.25 10:09:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:09:38 INFO  time: compiled root in 1.09s[0m
[0m2021.02.25 10:17:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:17:25 INFO  time: compiled root in 0.98s[0m
Feb 25, 2021 10:18:03 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 794
Feb 25, 2021 10:18:08 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 801
[0m2021.02.25 10:18:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:18:21 INFO  time: compiled root in 0.96s[0m
Feb 25, 2021 10:20:13 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 928
[0m2021.02.25 10:20:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:20:41 INFO  time: compiled root in 0.78s[0m
Feb 25, 2021 10:22:41 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1025
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild  scalaVersion := "2.11.12"
ThisBuild  version := "0.1.0-SNAPSHOT"
ThisBuild  organization := "com.revature"
ThisBuild  organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.0.0"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.
Feb 25, 2021 10:23:25 AM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: file%3A%2F%2F%2Fhome%2Famburkee%2Fscala_s3_read%2Fbuild.sbt:9: error: ; expected but string constant found
ThisBuild  scalaVersion := "2.11.12"
                           ^
file%3A%2F%2F%2Fhome%2Famburkee%2Fscala_s3_read%2Fbuild.sbt:9: error: ; expected but string constant found
ThisBuild  scalaVersion := "2.11.12"
                           ^
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.ScalametaParser.syntaxErrorExpected(ScalametaParser.scala:835)
	at scala.meta.internal.parsers.ScalametaParser.accept(ScalametaParser.scala:841)
	at scala.meta.internal.parsers.ScalametaParser.acceptStatSep(ScalametaParser.scala:850)
	at scala.meta.internal.parsers.ScalametaParser.acceptStatSepOpt(ScalametaParser.scala:854)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4809)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$scriptSource$1(ScalametaParser.scala:4988)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.scriptSource(ScalametaParser.scala:4987)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.semanticdb.scalac.ParseOps$XtensionCompilationUnitSource.toSource(ParseOps.scala:17)
	at scala.meta.internal.semanticdb.scalac.TextDocumentOps$XtensionCompilationUnitDocument.toTextDocument(TextDocumentOps.scala:201)
	at scala.meta.internal.pc.SemanticdbTextDocumentProvider.textDocument(SemanticdbTextDocumentProvider.scala:52)

[0m2021.02.25 10:23:29 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals9116564708486859992/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:23:30 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:23:30 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:23:31 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project/project[0m
[0m2021.02.25 10:23:31 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:23:31 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project[0m
[0m2021.02.25 10:23:33 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.02.25 10:23:33 INFO  [success] Total time: 2 s, completed Feb 25, 2021 10:23:33 AM[0m
[0m2021.02.25 10:23:33 INFO  [info] loading settings for project scala_s3_read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 10:23:33 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project[0m
[0m2021.02.25 10:23:33 INFO  [success] Generated .bloop/scala_s3_read-build.json[0m
[0m2021.02.25 10:23:33 INFO  [info] compiling 1 Scala source to /home/amburkee/scala_s3_read/project/target/scala-2.12/sbt-1.0/classes ...[0m
[0m2021.02.25 10:23:37 INFO  [info] done compiling[0m
[0m2021.02.25 10:23:37 INFO  [success] Total time: 3 s, completed Feb 25, 2021 10:23:37 AM[0m
[0m2021.02.25 10:23:37 INFO  [error] [/home/amburkee/scala_s3_read/build.sbt]:3: ';' expected but string literal found.[0m
[0m2021.02.25 10:23:37 INFO  [error] [/home/amburkee/scala_s3_read/build.sbt]:4: ';' expected but string literal found.[0m
[0m2021.02.25 10:23:37 INFO  [error] [/home/amburkee/scala_s3_read/build.sbt]:5: ';' expected but string literal found.[0m
[0m2021.02.25 10:23:37 INFO  [error] [/home/amburkee/scala_s3_read/build.sbt]:6: ';' expected but string literal found.[0m
[0m2021.02.25 10:23:37 INFO  [warn] Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? (default: r)[0m
[0m2021.02.25 10:23:37 INFO  sbt bloopInstall exit: 1[0m
[0m2021.02.25 10:23:37 INFO  time: ran 'sbt bloopInstall' in 8.39s[0m
[0m2021.02.25 10:23:37 ERROR sbt command failed: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals9116564708486859992/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.02.25 10:23:37 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:23:37 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:23:37 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:23:37 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.02.25 10:23:37 INFO  Attempting to connect to the build server...[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2999504893273328226/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2999504893273328226/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2999504893273328226/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:23:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:23:37 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.25 10:23:37 INFO  Attempting to connect to the build server...[0mOpening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6933962344098202910/bsp.socket'...

Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7097961861977704382/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6933962344098202910/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6933962344098202910/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7097961861977704382/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7097961861977704382/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:23:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:23:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:23:37 INFO  time: Connected to build server in 0.16s[0m
[0m2021.02.25 10:23:37 INFO  Connected to Build server: Bloop v1.4.6-21-464e4ec4[0m
[0m2021.02.25 10:23:39 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:23:39 INFO  time: indexed workspace in 1.43s[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild  scalaVersion := "2.11.12"
ThisBuild  version := "0.1.0-SNAPSHOT"
ThisBuild  organization := "com.revature"
ThisBuild  organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.0.0"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.
Feb 25, 2021 10:23:39 AM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: file%3A%2F%2F%2Fhome%2Famburkee%2Fscala_s3_read%2Fbuild.sbt:9: error: ; expected but string constant found
ThisBuild  scalaVersion := "2.11.12"
                           ^
file%3A%2F%2F%2Fhome%2Famburkee%2Fscala_s3_read%2Fbuild.sbt:9: error: ; expected but string constant found
ThisBuild  scalaVersion := "2.11.12"
                           ^
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.ScalametaParser.syntaxErrorExpected(ScalametaParser.scala:835)
	at scala.meta.internal.parsers.ScalametaParser.accept(ScalametaParser.scala:841)
	at scala.meta.internal.parsers.ScalametaParser.acceptStatSep(ScalametaParser.scala:850)
	at scala.meta.internal.parsers.ScalametaParser.acceptStatSepOpt(ScalametaParser.scala:854)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4809)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$scriptSource$1(ScalametaParser.scala:4988)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.scriptSource(ScalametaParser.scala:4987)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.semanticdb.scalac.ParseOps$XtensionCompilationUnitSource.toSource(ParseOps.scala:17)
	at scala.meta.internal.semanticdb.scalac.TextDocumentOps$XtensionCompilationUnitDocument.toTextDocument(TextDocumentOps.scala:201)
	at scala.meta.internal.pc.SemanticdbTextDocumentProvider.textDocument(SemanticdbTextDocumentProvider.scala:52)

[0m2021.02.25 10:23:57 WARN  no build target for: /home/amburkee/Project1/hello-spark/build.sbt[0m
[0m2021.02.25 10:23:57 INFO  no build target: using presentation compiler with only scala-library: 2.12.12[0m
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "hello-spark",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided"
    // provided means the dep will be provided in the environment we run this project
    // Spark already has the spark dependencies, so we mark them as provided
  )


// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

Feb 25, 2021 10:23:59 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1068
Feb 25, 2021 10:24:01 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1074
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild /  scalaVersion := "2.11.12"
ThisBuild / version := "0.1.0-SNAPSHOT"
ThisBuild / organization := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "3.0.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.0.0"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.
[0m2021.02.25 10:24:11 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals89861593450511322/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:24:12 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
Feb 25, 2021 10:24:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1108
[0m2021.02.25 10:24:12 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:24:13 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project/project[0m
[0m2021.02.25 10:24:13 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:24:13 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project[0m
Feb 25, 2021 10:24:16 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1113
[0m2021.02.25 10:24:16 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.02.25 10:24:16 INFO  [success] Total time: 2 s, completed Feb 25, 2021 10:24:16 AM[0m
[0m2021.02.25 10:24:16 INFO  [info] loading settings for project scala_s3_read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 10:24:16 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project[0m
[0m2021.02.25 10:24:16 INFO  [success] Generated .bloop/scala_s3_read-build.json[0m
[0m2021.02.25 10:24:17 INFO  [success] Total time: 1 s, completed Feb 25, 2021 10:24:17 AM[0m
Feb 25, 2021 10:24:17 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1119
[0m2021.02.25 10:24:19 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 10:24:19 INFO  [info] set current project to scalas3read (in build file:/home/amburkee/scala_s3_read/)[0m
Feb 25, 2021 10:24:20 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1124
[0m2021.02.25 10:24:19 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 10:24:19 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 10:24:19 INFO  [success] Total time: 1 s, completed Feb 25, 2021 10:24:20 AM[0m
[0m2021.02.25 10:24:21 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 10:24:21 INFO  time: ran 'sbt bloopInstall' in 9.83s[0m
[0m2021.02.25 10:24:21 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:24:21 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
[0m2021.02.25 10:24:21 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:24:21 INFO  Shut down connection with build server.No more data in the client stdin, exiting...[0m

No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.02.25 10:24:21 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:24:21 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:24:21 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:24:21 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:24:21 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:24:21 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:24:21 INFO  time: Connected to build server in 0.16s[0m
[0m2021.02.25 10:24:21 INFO  Connected to Build server: Bloop v1.4.6-21-464e4ec4[0m
[0m2021.02.25 10:24:23 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:24:23 INFO  time: indexed workspace in 1.42s[0m
[0m2021.02.25 10:24:27 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:24:27 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:24:27 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:24:27 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:24:27 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:24:30 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:24:30 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:24:30 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:24:30 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:24:30 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:24:30 INFO  time: Connected to build server in 3.7s[0m
[0m2021.02.25 10:24:30 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 10:24:31 INFO  no build target: using presentation compiler with only scala-library: 2.12.12[0m
[0m2021.02.25 10:24:31 INFO  time: Imported build in 0.11s[0m
[0m2021.02.25 10:24:32 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:24:32 INFO  time: indexed workspace in 1.31s[0m
[0m2021.02.25 10:24:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:24:36 INFO  time: compiled root in 3.19s[0m
[0m2021.02.25 10:26:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:26:03 INFO  time: compiled root in 1.21s[0m
[0m2021.02.25 10:26:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:26:04 INFO  time: compiled root in 0.96s[0m
[0m2021.02.25 10:28:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:28:00 INFO  time: compiled root in 0.75s[0m
[0m2021.02.25 10:28:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:28:56 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 10:29:21 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals3497126368740903366/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:29:22 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:29:22 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:29:24 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project/project[0m
[0m2021.02.25 10:29:24 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:29:24 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project[0m
[0m2021.02.25 10:29:26 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.02.25 10:29:26 INFO  [success] Total time: 2 s, completed Feb 25, 2021 10:29:26 AM[0m
[0m2021.02.25 10:29:26 INFO  [info] loading settings for project scala_s3_read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 10:29:26 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project[0m
[0m2021.02.25 10:29:26 INFO  [success] Generated .bloop/scala_s3_read-build.json[0m
[0m2021.02.25 10:29:26 INFO  [info] compiling 1 Scala source to /home/amburkee/scala_s3_read/project/target/scala-2.12/sbt-1.0/classes ...[0m
[0m2021.02.25 10:29:29 INFO  [info] done compiling[0m
[0m2021.02.25 10:29:29 INFO  [success] Total time: 3 s, completed Feb 25, 2021 10:29:29 AM[0m
[0m2021.02.25 10:29:32 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 10:29:32 INFO  [info] set current project to scalas3read (in build file:/home/amburkee/scala_s3_read/)[0m
[0m2021.02.25 10:29:40 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 10:29:45 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 10:29:45 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 10:29:45 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 10:29:45 INFO  [success] Total time: 14 s, completed Feb 25, 2021 10:29:46 AM[0m
[0m2021.02.25 10:29:45 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 10:29:46 INFO  time: ran 'sbt bloopInstall' in 24s[0m
[0m2021.02.25 10:29:46 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:29:46 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:29:46 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:29:46 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:29:46 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:29:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:29:46 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:29:46 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:29:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:29:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:29:46 INFO  time: Connected to build server in 53ms[0m
[0m2021.02.25 10:29:46 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 10:29:47 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:29:47 INFO  time: indexed workspace in 1.19s[0m
[0m2021.02.25 10:29:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:29:48 INFO  time: compiled root in 0.56s[0m
[0m2021.02.25 10:29:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:29:49 INFO  time: compiled root in 0.72s[0m
[0m2021.02.25 10:47:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:47:51 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 10:52:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:52:39 INFO  time: compiled root in 0.79s[0m
[0m2021.02.25 10:53:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:53:07 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 10:55:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:55:15 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 11:08:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:08:04 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 11:08:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:08:50 INFO  time: compiled root in 0.59s[0m
[0m2021.02.25 11:10:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:10:27 INFO  time: compiled root in 0.62s[0m
[0m2021.02.25 11:11:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:11:01 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 11:11:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:11:52 INFO  time: compiled root in 0.59s[0m
[0m2021.02.25 11:11:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:11:55 INFO  time: compiled root in 0.61s[0m
[0m2021.02.25 11:11:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:11:59 INFO  time: compiled root in 0.62s[0m
[0m2021.02.25 11:59:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:59:16 INFO  time: compiled root in 0.26s[0m
[0m2021.02.25 11:59:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:59:29 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 11:59:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:59:36 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 12:01:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:01:27 INFO  time: compiled root in 0.59s[0m
[0m2021.02.25 12:32:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:32:46 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 12:33:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:33:20 INFO  time: compiled root in 0.39s[0m
[0m2021.02.25 12:35:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:35:01 INFO  time: compiled root in 0.47s[0m
[0m2021.02.25 12:35:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:35:27 INFO  time: compiled root in 0.48s[0m
[0m2021.02.25 12:36:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:36:04 INFO  time: compiled root in 0.42s[0m
[0m2021.02.25 12:36:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:36:21 ERROR /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala
org.scalameta.invariants.InvariantFailedException: invariant failed:
when verifying stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat))
found that stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat)) is false
where stats = List(import org.apache.spark.sql.SparkSession, import com.amazonaws.services.s3.AmazonS3Client, import com.amazonaws.auth.BasicAWSCredentials, val sc: sparkContext, val sqlContext = new org.apache.spark.sql.SQLContext(sc), import sqlContext.createSchemaRDD, object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val rddFromFile = spark.read.json( "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz" ) 
    // rddFromFile.printSchema()
    // rddFromFile.take(1).foreach(println)

    val query = sqlContext.sql("SELECT url, warc_filename, warc_record_offset, warc_record_length, content_charset FROM ccindex WHERE crawl = 'CC-MAIN-2020-24' AND subset = 'warc' AND url_host_tld = 'is' LIMIT 10")
  
}
})
	at org.scalameta.invariants.InvariantFailedException$.raise(Exceptions.scala:19)
	at scala.meta.Pkg$.internal$254(Trees.scala:443)
	at scala.meta.Pkg$.apply(Trees.scala:441)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.mtags.ScalaMtags.<init>(ScalaMtags.scala:40)
	at scala.meta.internal.metals.Docstrings$Deindexer.<init>(Docstrings.scala:96)
	at scala.meta.internal.metals.Docstrings.expireSymbolDefinition(Docstrings.scala:60)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2115)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2(MetalsLanguageServer.scala:2068)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2065)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.reindexWorkspaceSources(MetalsLanguageServer.scala:2065)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$2(MetalsLanguageServer.scala:1240)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 12:36:21 INFO  time: compiled root in 0.37s[0m
[0m2021.02.25 12:36:53 ERROR /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala
org.scalameta.invariants.InvariantFailedException: invariant failed:
when verifying stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat))
found that stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat)) is false
where stats = List(import org.apache.spark.sql.SparkSession, import com.amazonaws.services.s3.AmazonS3Client, import com.amazonaws.auth.BasicAWSCredentials, val sc: SparkContext, val sqlContext = new org.apache.spark.sql.SQLContext(sc), import sqlContext.createSchemaRDD, object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val rddFromFile = spark.read.json( "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz" ) 
    // rddFromFile.printSchema()
    // rddFromFile.take(1).foreach(println)

    val query = sqlContext.sql("SELECT url, warc_filename, warc_record_offset, warc_record_length, content_charset FROM ccindex WHERE crawl = 'CC-MAIN-2020-24' AND subset = 'warc' AND url_host_tld = 'is' LIMIT 10")
  
}
})
	at org.scalameta.invariants.InvariantFailedException$.raise(Exceptions.scala:19)
	at scala.meta.Pkg$.internal$254(Trees.scala:443)
	at scala.meta.Pkg$.apply(Trees.scala:441)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.mtags.ScalaMtags.<init>(ScalaMtags.scala:40)
	at scala.meta.internal.metals.Docstrings$Deindexer.<init>(Docstrings.scala:96)
	at scala.meta.internal.metals.Docstrings.expireSymbolDefinition(Docstrings.scala:60)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2115)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2(MetalsLanguageServer.scala:2068)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2065)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.reindexWorkspaceSources(MetalsLanguageServer.scala:2065)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$2(MetalsLanguageServer.scala:1240)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 12:36:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:36:53 INFO  time: compiled root in 0.38s[0m
[0m2021.02.25 12:37:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:37:12 INFO  time: compiled root in 0.38s[0m
[0m2021.02.25 12:37:18 ERROR /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala
org.scalameta.invariants.InvariantFailedException: invariant failed:
when verifying stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat))
found that stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat)) is false
where stats = List(import org.apache.spark.sql.SparkSession, import com.amazonaws.services.s3.AmazonS3Client, import com.amazonaws.auth.BasicAWSCredentials, val sc: SparkContext, val sqlContext = new org.apache.spark.sql.SQLContext(sc), import sqlContext.createSchemaRDD, object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val rddFromFile = spark.read.json( "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz" ) 
    // rddFromFile.printSchema()
    // rddFromFile.take(1).foreach(println)

    val query = sqlContext.sql("SELECT url, warc_filename, warc_record_offset, warc_record_length, content_charset FROM ccindex WHERE crawl = 'CC-MAIN-2020-24' AND subset = 'warc' AND url_host_tld = 'is' LIMIT 10")
  
}
})
	at org.scalameta.invariants.InvariantFailedException$.raise(Exceptions.scala:19)
	at scala.meta.Pkg$.internal$254(Trees.scala:443)
	at scala.meta.Pkg$.apply(Trees.scala:441)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.mtags.ScalaMtags.<init>(ScalaMtags.scala:40)
	at scala.meta.internal.metals.Docstrings$Deindexer.<init>(Docstrings.scala:96)
	at scala.meta.internal.metals.Docstrings.expireSymbolDefinition(Docstrings.scala:60)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2115)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2(MetalsLanguageServer.scala:2068)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2065)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.reindexWorkspaceSources(MetalsLanguageServer.scala:2065)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$2(MetalsLanguageServer.scala:1240)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 12:37:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:37:18 ERROR /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala
org.scalameta.invariants.InvariantFailedException: invariant failed:
when verifying stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat))
found that stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat)) is false
where stats = List(import org.apache.spark.sql.SparkSession, import com.amazonaws.services.s3.AmazonS3Client, import com.amazonaws.auth.BasicAWSCredentials, val sc: SparkContext, val sqlContext = new org.apache.spark.sql.SQLContext(sc), import sqlContext.createSchemaRDD, object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val rddFromFile = spark.read.json( "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz" ) 
    // rddFromFile.printSchema()
    // rddFromFile.take(1).foreach(println)

    val query = sqlContext.sql("SELECT url, warc_filename, warc_record_offset, warc_record_length, content_charset FROM ccindex WHERE crawl = 'CC-MAIN-2020-24' AND subset = 'warc' AND url_host_tld = 'is' LIMIT 10")
  
}
})
	at org.scalameta.invariants.InvariantFailedException$.raise(Exceptions.scala:19)
	at scala.meta.Pkg$.internal$254(Trees.scala:443)
	at scala.meta.Pkg$.apply(Trees.scala:441)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.mtags.ScalaMtags.<init>(ScalaMtags.scala:40)
	at scala.meta.internal.metals.Docstrings$Deindexer.<init>(Docstrings.scala:96)
	at scala.meta.internal.metals.Docstrings.expireSymbolDefinition(Docstrings.scala:60)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2115)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2(MetalsLanguageServer.scala:2068)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2065)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.reindexWorkspaceSources(MetalsLanguageServer.scala:2065)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$2(MetalsLanguageServer.scala:1240)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 12:37:18 INFO  time: compiled root in 0.25s[0m
[0m2021.02.25 12:37:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:37:18 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 12:37:39 ERROR /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala
org.scalameta.invariants.InvariantFailedException: invariant failed:
when verifying stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat))
found that stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat)) is false
where stats = List(import org.apache.spark.sql.SparkSession, import com.amazonaws.services.s3.AmazonS3Client, import com.amazonaws.auth.BasicAWSCredentials, val sc: SparkContext, val sqlContext = new org.apache.spark.sql.SQLContext(sc), import sqlContext.createSchemaRDD, object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val rddFromFile = spark.read.json( "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz" ) 
    // rddFromFile.printSchema()
    // rddFromFile.take(1).foreach(println)

    val query = sqlContext.sql("SELECT url, warc_filename, warc_record_offset, warc_record_length, content_charset FROM ccindex WHERE crawl = 'CC-MAIN-2020-24' AND subset = 'warc' AND url_host_tld = 'is' LIMIT 10")
  
}
})
	at org.scalameta.invariants.InvariantFailedException$.raise(Exceptions.scala:19)
	at scala.meta.Pkg$.internal$254(Trees.scala:443)
	at scala.meta.Pkg$.apply(Trees.scala:441)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.mtags.ScalaMtags.<init>(ScalaMtags.scala:40)
	at scala.meta.internal.metals.Docstrings$Deindexer.<init>(Docstrings.scala:96)
	at scala.meta.internal.metals.Docstrings.expireSymbolDefinition(Docstrings.scala:60)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2115)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2(MetalsLanguageServer.scala:2068)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2065)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.reindexWorkspaceSources(MetalsLanguageServer.scala:2065)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$2(MetalsLanguageServer.scala:1240)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 12:37:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:37:39 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 12:38:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:38:39 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 12:39:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:39:16 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 12:40:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:40:48 INFO  time: compiled root in 1s[0m
[0m2021.02.25 12:41:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:41:08 INFO  time: compiled root in 0.72s[0m
[0m2021.02.25 12:42:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:42:35 INFO  time: compiled root in 1.04s[0m
[0m2021.02.25 12:44:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:44:28 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 12:45:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:45:49 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 12:46:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:46:46 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 12:46:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:46:58 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 12:47:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:47:02 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 12:47:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:47:53 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 12:47:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:47:58 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 12:48:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:48:52 INFO  time: compiled root in 0.53s[0m
[0m2021.02.25 12:49:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:49:49 INFO  time: compiled root in 0.78s[0m
[0m2021.02.25 12:50:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:50:00 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 12:51:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:51:19 INFO  time: compiled root in 0.52s[0m
[0m2021.02.25 12:52:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:52:36 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 12:54:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:54:08 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 12:56:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:56:01 INFO  time: compiled root in 0.61s[0m
[0m2021.02.25 12:56:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:56:10 INFO  time: compiled root in 0.61s[0m
[0m2021.02.25 12:59:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:59:24 INFO  time: compiled root in 98ms[0m
[0m2021.02.25 12:59:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:59:26 INFO  time: compiled root in 0.59s[0m
[0m2021.02.25 13:01:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:01:06 INFO  time: compiled root in 0.56s[0m
[0m2021.02.25 13:02:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:02:15 INFO  time: compiled root in 0.52s[0m
[0m2021.02.25 13:03:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:03:42 INFO  time: compiled root in 0.51s[0m
[0m2021.02.25 13:04:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:04:02 INFO  time: compiled root in 0.56s[0m
[0m2021.02.25 13:05:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:05:10 INFO  time: compiled root in 0.55s[0m
[0m2021.02.25 13:05:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:05:20 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 13:06:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:06:31 INFO  time: compiled root in 0.62s[0m
[0m2021.02.25 13:06:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:06:39 INFO  time: compiled root in 0.55s[0m
[0m2021.02.25 13:09:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:09:16 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 13:09:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:30:10: stale bloop error: identifier expected but string literal found.
    ).as["rddDS"]
         ^[0m
[0m2021.02.25 13:09:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:34:1: stale bloop error: ']' expected but ';' found.
    rddFromFile.createOrReplaceTempView("rdd")
^[0m
[0m2021.02.25 13:09:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:30:10: stale bloop error: identifier expected but string literal found.
    ).as["rddDS"]
         ^[0m
[0m2021.02.25 13:09:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:34:1: stale bloop error: ']' expected but ';' found.
    rddFromFile.createOrReplaceTempView("rdd")
^[0m
[0m2021.02.25 13:09:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:09:32 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 13:09:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:09:38 INFO  time: compiled root in 0.64s[0m
[0m2021.02.25 13:10:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:10:52 INFO  time: compiled root in 0.53s[0m
[0m2021.02.25 13:11:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:11:01 INFO  time: compiled root in 0.54s[0m
[0m2021.02.25 13:12:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:12:24 INFO  time: compiled root in 0.5s[0m
[0m2021.02.25 13:12:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:12:59 INFO  time: compiled root in 0.53s[0m
[0m2021.02.25 13:16:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:16:22 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 13:17:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:17:18 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 13:18:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:18:15 INFO  time: compiled root in 0.57s[0m
[0m2021.02.25 13:18:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:18:32 INFO  time: compiled root in 0.56s[0m
[0m2021.02.25 13:18:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:18:35 INFO  time: compiled root in 0.52s[0m
[0m2021.02.25 13:18:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:18:42 INFO  time: compiled root in 0.57s[0m
[0m2021.02.25 13:18:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:18:46 INFO  time: compiled root in 0.61s[0m
[0m2021.02.25 13:21:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:21:59 INFO  time: compiled root in 0.58s[0m
[0m2021.02.25 13:25:12 INFO  shutting down Metals[0m
[0m2021.02.25 13:25:12 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:25:12 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:25:12 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:25:24 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.53.2.[0m
[0m2021.02.25 13:25:24 INFO  time: initialize in 0.47s[0m
[0m2021.02.25 13:25:25 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1348961055531692784/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.25 13:25:24 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
[0m2021.02.25 13:25:25 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.02.25 13:25:27 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val rddFromFile = spark.read.json(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz"
    )

    // "Container", "Envelope", "_corrupt_record"
    rddFromFile.columns.foreach(println)

    // rddFromFile.printSchema()
    // rddFromFile.take(1).foreach(println)

    rddFromFile.createOrReplaceTempView("rdd")
    val q = spark.sql(
        "SELECT Envelope " +
          "FROM rdd " +
          "LIMIT 20")
    q.show()
  }
}

Waiting for the bsp connection to come up...
[0m2021.02.25 13:25:30 INFO  time: code lens generation in 4.36s[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1348961055531692784/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1348961055531692784/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 13:25:30 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:25:30 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5228248880812170884/bsp.socket'...
[0m2021.02.25 13:25:30 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4058080545702235134/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4058080545702235134/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4058080545702235134/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 13:25:30 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5228248880812170884/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5228248880812170884/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 13:25:30 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:25:30 INFO  time: Connected to build server in 5.4s[0m
[0m2021.02.25 13:25:30 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 13:25:30 INFO  time: Imported build in 0.18s[0m
[0m2021.02.25 13:25:33 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 13:25:33 INFO  time: indexed workspace in 3.06s[0m
[0m2021.02.25 13:27:19 INFO  shutting down Metals[0m
[0m2021.02.25 13:27:19 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:27:19 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:27:19 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.02.25 13:28:52 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.53.2.[0m
[0m2021.02.25 13:28:53 INFO  time: initialize in 0.43s[0m
[0m2021.02.25 13:28:52 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1201141736872079912/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.25 13:28:53 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
[0m2021.02.25 13:28:53 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.02.25 13:28:55 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val rddFromFile = spark.read.json(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz"
    )

    // "Container", "Envelope", "_corrupt_record"
    rddFromFile.columns.foreach(println)

    // rddFromFile.printSchema()
    // rddFromFile.take(1).foreach(println)

    rddFromFile.createOrReplaceTempView("rdd")
    val q = spark.sql(
        "SELECT Envelope " +
          "FROM rdd " +
          "LIMIT 20")
    q.show()
  }
}

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1201141736872079912/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1201141736872079912/bsp.socket...
[0m2021.02.25 13:28:57 INFO  time: code lens generation in 3.18s[0m
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 13:28:57 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:28:57 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0mOpening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4294838983506993124/bsp.socket'...
2021.02.25 13:28:57 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7328067550705989877/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4294838983506993124/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4294838983506993124/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7328067550705989877/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7328067550705989877/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 13:28:57 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:28:57 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:28:57 INFO  time: Connected to build server in 3.73s[0m
[0m2021.02.25 13:28:57 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 13:28:57 INFO  time: Imported build in 0.15s[0m
[0m2021.02.25 13:28:59 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 13:28:59 INFO  time: indexed workspace in 1.97s[0m
[0m2021.02.25 13:30:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:30:16 INFO  time: compiled root in 12s[0m
Feb 25, 2021 1:30:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 65
[0m2021.02.25 13:31:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:31:07 INFO  time: compiled root in 1.12s[0m
[0m2021.02.25 13:31:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:31:16 INFO  time: compiled root in 1.08s[0m
[0m2021.02.25 13:32:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:32:36 INFO  time: compiled root in 1.01s[0m
[0m2021.02.25 13:33:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:33:45 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 13:35:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:35:14 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 13:35:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:35:25 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 13:35:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:35:33 INFO  time: compiled root in 0.86s[0m
[0m2021.02.25 13:43:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:43:28 INFO  time: compiled root in 0.93s[0m
[0m2021.02.25 13:44:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:44:39 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 13:45:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:45:30 INFO  time: compiled root in 1.01s[0m
[0m2021.02.25 13:45:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:45:46 INFO  time: compiled root in 1.31s[0m
Feb 25, 2021 1:46:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 621
[0m2021.02.25 13:46:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:46:51 INFO  time: compiled root in 0.75s[0m
[0m2021.02.25 13:48:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:48:44 INFO  time: compiled root in 0.78s[0m
[0m2021.02.25 13:49:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:49:43 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 13:50:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:50:53 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 13:52:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:52:05 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 13:53:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:53:01 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 13:53:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:53:05 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 13:55:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:55:34 INFO  time: compiled root in 0.25s[0m
[0m2021.02.25 13:56:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:56:06 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 13:56:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:56:28 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 13:56:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:56:32 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 13:58:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:58:10 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 13:58:19 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals6654570586879032162/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 13:58:19 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals6617355872956473883/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 13:58:19 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.02.25 13:58:20 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals5798914655226549300/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 13:58:21 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 13:58:21 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.02.25 13:58:22 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.02.25 13:58:22 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project/project[0m
[0m2021.02.25 13:58:23 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 13:58:23 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 13:58:23 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.02.25 13:58:23 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project[0m
[0m2021.02.25 13:58:24 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project/project[0m
[0m2021.02.25 13:58:24 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project/project[0m
[0m2021.02.25 13:58:26 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.02.25 13:58:26 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project[0m
[0m2021.02.25 13:58:26 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.02.25 13:58:26 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project/project[0m
[0m2021.02.25 13:58:26 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 13:58:26 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.02.25 13:58:26 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 13:58:26 INFO  [success] Total time: 3 s, completed Feb 25, 2021 1:58:27 PM[0m
[0m2021.02.25 13:58:26 INFO  [info] loading settings for project scala_s3_read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 13:58:26 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project[0m
[0m2021.02.25 13:58:26 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 13:58:28 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.02.25 13:58:28 INFO  [success] Total time: 2 s, completed Feb 25, 2021 1:58:28 PM[0m
[0m2021.02.25 13:58:28 INFO  [info] loading settings for project scala_s3_read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 13:58:28 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project[0m
[0m2021.02.25 13:58:28 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 13:58:30 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.02.25 13:58:30 INFO  [success] Total time: 4 s, completed Feb 25, 2021 1:58:30 PM[0m
[0m2021.02.25 13:58:30 INFO  [success] Generated .bloop/scala_s3_read-build.json[0m
[0m2021.02.25 13:58:30 INFO  [info] loading settings for project scala_s3_read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 13:58:30 INFO  [info] loading project definition from /home/amburkee/scala_s3_read/project[0m
[0m2021.02.25 13:58:30 ERROR [info] waiting for lock on /home/amburkee/.ivy2/exclude_classifiers.lock to be available...[0m
[0m2021.02.25 13:58:31 INFO  [success] Generated .bloop/scala_s3_read-build.json[0m
[0m2021.02.25 13:58:31 INFO  [success] Generated .bloop/scala_s3_read-build.json[0m
[0m2021.02.25 13:58:31 INFO  [success] Total time: 4 s, completed Feb 25, 2021 1:58:31 PM[0m
[0m2021.02.25 13:58:31 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 13:58:31 INFO  [info] set current project to scalas3read (in build file:/home/amburkee/scala_s3_read/)[0m
[0m2021.02.25 13:58:32 INFO  [success] Total time: 3 s, completed Feb 25, 2021 1:58:32 PM[0m
[0m2021.02.25 13:58:32 INFO  [success] Total time: 2 s, completed Feb 25, 2021 1:58:32 PM[0m
[0m2021.02.25 13:58:32 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 13:58:32 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 13:58:32 INFO  [info] set current project to scalas3read (in build file:/home/amburkee/scala_s3_read/)[0m
[0m2021.02.25 13:58:32 INFO  [info] set current project to scalas3read (in build file:/home/amburkee/scala_s3_read/)[0m
[0m2021.02.25 13:58:34 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 13:58:34 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 13:58:34 INFO  [success] Total time: 1 s, completed Feb 25, 2021 1:58:34 PM[0m
[0m2021.02.25 13:58:34 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 13:58:34 INFO  time: ran 'sbt bloopInstall' in 15s[0m
[0m2021.02.25 13:58:34 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 13:58:34 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:58:34 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:58:34 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.02.25 13:58:34 INFO  Attempting to connect to the build server...[0m
No more data in the client stdin, exiting...
[0m2021.02.25 13:58:34 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:58:34 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:58:34 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:58:34 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 13:58:34 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 13:58:34 INFO  [success] Total time: 2 s, completed Feb 25, 2021 1:58:34 PM[0m
[0m2021.02.25 13:58:34 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:58:34 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:58:34 INFO  time: Connected to build server in 0.18s[0m
[0m2021.02.25 13:58:34 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 13:58:35 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 13:58:35 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 13:58:35 INFO  [success] Total time: 2 s, completed Feb 25, 2021 1:58:35 PM[0m
[0m2021.02.25 13:58:34 INFO  time: Imported build in 0.3s[0m
[0m2021.02.25 13:58:35 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 13:58:35 INFO  time: ran 'sbt bloopInstall' in 16s[0m
[0m2021.02.25 13:58:34 INFO  time: ran 'sbt bloopInstall' in 15s[0m
[0m2021.02.25 13:58:34 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 13:58:35 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 13:58:34 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 13:58:35 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:58:35 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:58:35 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:58:35 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:58:35 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:58:35 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:58:35 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:58:35 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:58:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:58:36 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:58:36 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:58:35 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:58:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:58:36 INFO  time: Connected to build server in 0.15s[0m
[0m2021.02.25 13:58:36 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 13:58:35 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:58:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:58:35 INFO  time: Connected to build server in 0.14s[0m
[0m2021.02.25 13:58:35 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 13:58:36 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 13:58:36 INFO  time: indexed workspace in 1.66s[0m
[0m2021.02.25 13:58:37 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 13:58:37 INFO  time: indexed workspace in 1.39s[0m
[0m2021.02.25 13:58:37 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 13:58:37 INFO  time: indexed workspace in 1.44s[0m
[0m2021.02.25 13:58:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:58:50 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 13:59:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:59:47 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 14:01:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:01:17 INFO  time: compiled root in 0.75s[0m
[0m2021.02.25 14:01:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:01:31 INFO  time: compiled root in 0.25s[0m
[0m2021.02.25 14:02:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:02:27 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 14:02:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:02:29 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 14:02:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:02:34 INFO  time: compiled root in 0.26s[0m
[0m2021.02.25 14:02:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:02:40 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 14:02:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:02:57 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 14:03:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:03:05 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 14:03:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:03:15 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 14:04:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:04:10 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 14:04:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:04:27 INFO  time: compiled root in 0.93s[0m
[0m2021.02.25 14:04:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:04:41 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 14:08:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:08:32 INFO  time: compiled root in 1.14s[0m
[0m2021.02.25 14:09:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:09:38 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 14:09:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:09:49 INFO  time: compiled root in 0.7s[0m
[0m2021.02.25 14:11:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:15 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 14:11:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:50 INFO  time: compiled root in 0.86s[0m
[0m2021.02.25 14:14:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:14:24 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 14:16:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:16:03 INFO  time: compiled root in 0.99s[0m
[0m2021.02.25 14:19:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:19:19 INFO  time: compiled root in 0.25s[0m
[0m2021.02.25 14:19:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:19:47 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 14:20:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:20:02 INFO  time: compiled root in 1.14s[0m
[0m2021.02.25 14:23:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:39 INFO  time: compiled root in 0.93s[0m
[0m2021.02.25 14:28:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:28:13 INFO  time: compiled root in 0.63s[0m
[0m2021.02.25 14:28:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:28:32 INFO  time: compiled root in 0.63s[0m
[0m2021.02.25 14:30:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:30:10 INFO  time: compiled root in 0.7s[0m
[0m2021.02.25 14:32:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:32:21 INFO  time: compiled root in 0.86s[0m
[0m2021.02.25 14:33:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:33:40 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 14:34:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:34:54 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 14:36:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:36:07 INFO  time: compiled root in 0.97s[0m
[0m2021.02.25 14:36:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:36:12 INFO  time: compiled root in 0.92s[0m
[0m2021.02.25 14:37:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:37:12 INFO  time: compiled root in 0.85s[0m
[0m2021.02.25 14:37:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:37:35 INFO  time: compiled root in 0.83s[0m
[0m2021.02.25 14:37:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:37:40 INFO  time: compiled root in 1.08s[0m
[0m2021.02.25 14:41:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:41:14 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 14:41:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:41:19 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 14:41:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:41:25 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 14:42:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:42:58 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 14:43:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:43:03 INFO  time: compiled root in 0.59s[0m
[0m2021.02.25 14:45:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:45:01 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 14:45:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:45:28 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 14:45:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:45:33 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 14:45:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:45:45 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 14:46:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:46:11 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 14:46:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:46:57 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 14:49:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:49:14 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 14:49:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:49:47 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 14:50:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:50:08 INFO  time: compiled root in 0.65s[0m
[0m2021.02.25 14:54:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:54:07 INFO  time: compiled root in 0.73s[0m
[0m2021.02.25 14:54:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:54:45 INFO  time: compiled root in 0.68s[0m
[0m2021.02.25 14:55:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:55:24 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 14:55:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:55:41 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 14:56:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:56:04 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 14:56:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:56:30 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 14:57:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:57:56 INFO  time: compiled root in 0.64s[0m
[0m2021.02.25 14:59:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:59:34 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 14:59:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:59:45 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 15:00:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:00:38 INFO  time: compiled root in 0.72s[0m
[0m2021.02.25 15:01:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:01:12 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 15:01:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:01:29 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 15:02:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:02:37 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 15:03:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:03:07 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 15:03:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:03:42 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 15:03:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:03:51 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 15:07:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:07:53 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 15:07:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:07:57 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 15:08:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:08:02 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 15:08:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:08:19 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 15:08:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:08:22 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 15:09:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:09:15 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 15:09:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:09:30 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 15:09:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:09:35 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 15:10:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:10:02 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 15:11:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:11:07 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 15:13:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:13:00 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 15:13:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:13:35 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 15:14:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:14:16 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 15:15:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:15:12 INFO  time: compiled root in 0.65s[0m
[0m2021.02.25 15:16:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:16:27 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 15:16:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:16:32 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 15:17:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:17:07 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 15:17:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:17:38 INFO  time: compiled root in 0.83s[0m
[0m2021.02.25 15:19:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:19:12 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 15:19:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:19:18 INFO  time: compiled root in 0.73s[0m
[0m2021.02.25 15:20:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:20:17 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 15:20:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:20:55 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 15:21:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:21:13 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 15:21:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:21:32 INFO  time: compiled root in 0.67s[0m
[0m2021.02.25 15:22:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:22:03 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 15:22:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:22:34 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 15:22:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:22:42 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 15:22:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:22:52 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 15:23:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:23:33 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 15:24:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:24:21 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 15:25:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:25:09 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 15:25:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:25:28 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 15:25:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:25:35 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 15:26:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:26:02 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 15:26:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:26:04 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 15:26:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:26:31 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 15:26:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:26:40 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 15:29:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:29:26 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 15:29:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:29:39 INFO  time: compiled root in 1.62s[0m
[0m2021.02.25 15:30:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:30:30 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 15:32:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:32:06 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 15:32:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:32:59 INFO  time: compiled root in 0.73s[0m
[0m2021.02.25 15:33:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:33:55 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 15:34:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:34:31 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 15:34:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:34:35 INFO  time: compiled root in 0.79s[0m
[0m2021.02.25 15:35:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:35:26 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 15:35:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:35:33 INFO  time: compiled root in 0.75s[0m
[0m2021.02.25 15:35:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:35:45 INFO  time: compiled root in 0.67s[0m
[0m2021.02.25 15:35:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:35:58 INFO  time: compiled root in 0.73s[0m
[0m2021.02.25 15:38:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:38:16 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 15:38:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:38:28 INFO  time: compiled root in 0.67s[0m
[0m2021.02.25 15:38:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:38:35 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.02.25 15:38:35 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 15:38:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:38:54 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 15:39:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:39:50 INFO  time: compiled root in 0.85s[0m
[0m2021.02.25 15:41:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:41:24 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 15:41:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:41:34 INFO  time: compiled root in 0.7s[0m
[0m2021.02.25 15:45:07 INFO  shutting down Metals[0m
[0m2021.02.25 15:45:07 INFO  Shut down connection with build server.[0m
[0m2021.02.25 15:45:07 INFO  Shut down connection with build server.[0m
[0m2021.02.25 15:45:07 INFO  Shut down connection with build server.[0m
[0m2021.02.26 10:04:23 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.53.2.[0m
[0m2021.02.26 10:04:24 INFO  time: initialize in 0.42s[0m
[0m2021.02.26 10:04:23 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.26 10:04:24 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6243749889268559655/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.26 10:04:24 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.02.26 10:04:26 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.StructField
import com.fasterxml.jackson.databind.`type`.ArrayType
import org.apache.spark.sql.functions

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val rddFromFile = spark.read
      .json(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz"
      )

    // "Container", "Envelope", "_corrupt_record"
    // rddFromFile.show(false)
    // rddFromFile.printSchema()
    
    rddFromFile
      .filter(!functions.isnull($"envelope.format"))
      .select(
        rddFromFile.col("container.filename"),
        rddFromFile.col("envelope.format"),
        rddFromFile.col("envelope.payload-metadata.actual-content-length")
      )
      .show()

    // val q = rddFromFile.take(2000)
    // val dfFromArray = spark.sparkContext.parallelize(q).map(q => (q.getString(0), q.getString(1), q.getString(2))).toDF("Container","Envelope","Text")

    // rdd.createOrReplaceTempView("rdd")
    // val y = spark.sql(
    //   "SELECT container.filename, envelope.format, envelope.payload-metadata.actual-contet " +
    //     "FROM rdd " +
    //     "WHERE Container IS NOT null"
    // )
    // y.show(false)

  }
}

Waiting for the bsp connection to come up...
[0m2021.02.26 10:04:28 INFO  time: code lens generation in 3.29s[0m
[0m2021.02.26 10:04:28 INFO  time: code lens generation in 2.86s[0m
[0m2021.02.26 10:04:28 INFO  time: code lens generation in 1.49s[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6243749889268559655/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6243749889268559655/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.26 10:04:28 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.26 10:04:28 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.26 10:04:28 INFO  Attempting to connect to the build server...[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2369477497061236555/bsp.socket'...
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5831266720172138595/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2369477497061236555/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2369477497061236555/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5831266720172138595/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5831266720172138595/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.26 10:04:28 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.26 10:04:28 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.02.26 10:04:28 INFO  time: Connected to build server in 4.3s[0m
[0m2021.02.26 10:04:28 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.26 10:04:28 INFO  time: Imported build in 0.22s[0m
[0m2021.02.26 10:04:30 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.26 10:04:30 INFO  time: indexed workspace in 2.15s[0m
[0m2021.02.26 10:05:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:05:25 INFO  time: compiled root in 4.43s[0m
[0m2021.02.26 10:31:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:31:52 INFO  time: compiled root in 1.47s[0m
[0m2021.02.26 10:33:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:33:47 INFO  time: compiled root in 1.21s[0m
[0m2021.02.26 10:34:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:34:17 INFO  time: compiled root in 0.92s[0m
[0m2021.02.26 10:35:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:35:02 INFO  time: compiled root in 0.9s[0m
[0m2021.02.26 10:35:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:35:41 INFO  time: compiled root in 0.88s[0m
[0m2021.02.26 10:36:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:36:08 INFO  time: compiled root in 0.75s[0m
[0m2021.02.26 12:16:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:16:16 INFO  time: compiled root in 0.9s[0m
[0m2021.02.26 12:17:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:17:11 INFO  time: compiled root in 0.85s[0m
[0m2021.02.26 12:18:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:18:45 INFO  time: compiled root in 0.61s[0m
[0m2021.02.26 12:18:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:18:48 INFO  time: compiled root in 0.63s[0m
[0m2021.02.26 12:19:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:19:48 INFO  time: compiled root in 0.17s[0m
[0m2021.02.26 12:20:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:20:08 INFO  time: compiled root in 0.6s[0m
[0m2021.02.26 12:20:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:20:22 INFO  time: compiled root in 1.47s[0m
[0m2021.02.26 12:21:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:21:06 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 12:21:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:21:14 INFO  time: compiled root in 0.6s[0m
[0m2021.02.26 12:23:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:23:19 INFO  time: compiled root in 0.63s[0m
[0m2021.02.26 12:24:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:24:56 INFO  time: compiled root in 0.69s[0m
[0m2021.02.26 12:26:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:26:51 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 12:27:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:27:28 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 12:27:31 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:18: stale bloop error: ')' expected but string literal found.
        "WHERE $"url_path".contains("job")"
                 ^[0m
[0m2021.02.26 12:27:31 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:18: stale bloop error: ')' expected but string literal found.
        "WHERE $"url_path".contains("job")"
                 ^[0m
[0m2021.02.26 12:27:33 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:18: stale bloop error: ')' expected but string literal found.
        "WHERE $"url_path".contains("job")"
                 ^[0m
[0m2021.02.26 12:27:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:27:33 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:33 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:33 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:44: stale bloop error: unclosed string literal
        s"WHERE $"url_path".contains("job")"
                                           ^[0m
[0m2021.02.26 12:27:33 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 12:27:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:44: stale bloop error: unclosed string literal
        s"WHERE $"url_path".contains("job")"
                                           ^[0m
[0m2021.02.26 12:27:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:44: stale bloop error: unclosed string literal
        s"WHERE $"url_path".contains("job")"
                                           ^[0m
[0m2021.02.26 12:27:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:44: stale bloop error: unclosed string literal
        s"WHERE $"url_path".contains("job")"
                                           ^[0m
[0m2021.02.26 12:27:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:44: stale bloop error: unclosed string literal
        s"WHERE $"url_path".contains("job")"
                                           ^[0m
[0m2021.02.26 12:27:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:44: stale bloop error: unclosed string literal
        s"WHERE $"url_path".contains("job")"
                                           ^[0m
[0m2021.02.26 12:27:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:44: stale bloop error: unclosed string literal
        s"WHERE $"url_path".contains("job")"
                                           ^[0m
[0m2021.02.26 12:27:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:44: stale bloop error: unclosed string literal
        s"WHERE $"url_path".contains("job")"
                                           ^[0m
[0m2021.02.26 12:27:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:44: stale bloop error: unclosed string literal
        s"WHERE $"url_path".contains("job")"
                                           ^[0m
[0m2021.02.26 12:27:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
        s"WHERE $"url_path".contains("job")"
          ^[0m
[0m2021.02.26 12:27:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:44: stale bloop error: unclosed string literal
        s"WHERE $"url_path".contains("job")"
                                           ^[0m
[0m2021.02.26 12:27:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:27:45 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 12:28:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:35: stale bloop error: ')' expected but string literal found.
        "WHERE url_path.contains("job")"
                                  ^[0m
[0m2021.02.26 12:28:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:35: stale bloop error: ')' expected but string literal found.
        "WHERE url_path.contains("job")"
                                  ^[0m
[0m2021.02.26 12:28:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:35: stale bloop error: ')' expected but string literal found.
        "WHERE url_path.contains("job")"
                                  ^[0m
[0m2021.02.26 12:28:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:35: stale bloop error: ')' expected but string literal found.
        "WHERE url_path.contains("job")"
                                  ^[0m
[0m2021.02.26 12:28:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:28:03 INFO  time: compiled root in 0.62s[0m
[0m2021.02.26 12:32:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:32:08 INFO  time: compiled root in 0.63s[0m
[0m2021.02.26 12:33:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:33:16 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 12:33:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:33:36 INFO  time: compiled root in 0.63s[0m
[0m2021.02.26 12:33:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:33:39 INFO  time: compiled root in 0.57s[0m
[0m2021.02.26 12:37:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:37:56 INFO  time: compiled root in 0.65s[0m
[0m2021.02.26 12:40:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:40:06 INFO  time: compiled root in 0.61s[0m
[0m2021.02.26 12:40:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:40:35 INFO  time: compiled root in 0.57s[0m
[0m2021.02.26 12:41:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:41:05 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 12:48:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:48:08 INFO  time: compiled root in 0.7s[0m
[0m2021.02.26 12:48:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:48:18 INFO  time: compiled root in 0.78s[0m
[0m2021.02.26 12:51:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:51:01 INFO  time: compiled root in 0.54s[0m
[0m2021.02.26 12:52:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:52:42 INFO  time: compiled root in 0.58s[0m
[0m2021.02.26 12:54:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:54:04 INFO  time: compiled root in 0.58s[0m
[0m2021.02.26 12:56:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:56:31 INFO  time: compiled root in 0.65s[0m
[0m2021.02.26 12:57:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:57:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:105: stale bloop error: unclosed string literal
      .select("Envelope(Format), Payload-Metadata(HTTP-Response-Metadata(HTML-Metadata(Head(Scripts))))"")
                                                                                                        ^[0m
[0m2021.02.26 12:57:41 INFO  time: compiled root in 99ms[0m
[0m2021.02.26 12:57:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:105: stale bloop error: unclosed string literal
      .select("Envelope(Format), Payload-Metadata(HTTP-Response-Metadata(HTML-Metadata(Head(Scripts))))"")
                                                                                                        ^[0m
[0m2021.02.26 12:57:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:105: stale bloop error: unclosed string literal
      .select("Envelope(Format), Payload-Metadata(HTTP-Response-Metadata(HTML-Metadata(Head(Scripts))))"")
                                                                                                        ^[0m
[0m2021.02.26 12:57:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:57:45 INFO  time: compiled root in 0.6s[0m
[0m2021.02.26 12:57:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:57:59 INFO  time: compiled root in 0.61s[0m
[0m2021.02.26 12:58:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:58:04 INFO  time: compiled root in 0.65s[0m
[0m2021.02.26 12:59:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:59:06 INFO  time: compiled root in 0.66s[0m
[0m2021.02.26 13:00:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:00:30 INFO  time: compiled root in 0.62s[0m
[0m2021.02.26 13:00:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:00:48 INFO  time: compiled root in 0.56s[0m
[0m2021.02.26 13:01:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:01:07 INFO  time: compiled root in 0.6s[0m
[0m2021.02.26 13:04:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:04:50 INFO  time: compiled root in 0.66s[0m
[0m2021.02.26 13:06:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:06:53 INFO  time: compiled root in 0.57s[0m
[0m2021.02.26 13:08:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:08:49 INFO  time: compiled root in 0.61s[0m
[0m2021.02.26 13:09:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:09:38 INFO  time: compiled root in 0.66s[0m
[0m2021.02.26 13:12:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:12:06 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 13:12:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:12:54 INFO  time: compiled root in 0.6s[0m
[0m2021.02.26 13:13:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:13:05 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 13:17:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:17:09 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 13:19:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:19:08 INFO  time: compiled root in 0.62s[0m
[0m2021.02.26 13:21:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:21:07 INFO  time: compiled root in 0.95s[0m
[0m2021.02.26 13:23:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:23:17 INFO  time: compiled root in 0.61s[0m
[0m2021.02.26 13:23:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:23:45 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 13:25:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:25:46 INFO  time: compiled root in 0.56s[0m
[0m2021.02.26 13:27:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:27:40 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 13:30:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:30:32 INFO  time: compiled root in 0.53s[0m
[0m2021.02.26 13:30:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:30:42 INFO  time: compiled root in 0.62s[0m
[0m2021.02.26 13:33:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:33:28 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 13:33:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:33:43 INFO  time: compiled root in 0.67s[0m
[0m2021.02.26 13:35:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:35:50 INFO  time: compiled root in 0.61s[0m
[0m2021.02.26 13:38:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:38:02 INFO  time: compiled root in 0.63s[0m
[0m2021.02.26 13:38:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:38:09 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 13:40:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:40:34 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 13:40:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:40:40 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 13:42:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:42:58 INFO  time: compiled root in 0.15s[0m
/*
 * Scala (https://www.scala-lang.org)
 *
 * Copyright EPFL and Lightbend, Inc.
 *
 * Licensed under Apache License 2.0
 * (http://www.apache.org/licenses/LICENSE-2.0).
 *
 * See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.
 */

package scala
package collection

import generic._
import scala.annotation.tailrec

/** A template trait for indexed sequences of type `IndexedSeq[A]` which optimizes
 *  the implementation of several methods under the assumption of fast random access.
 *
 *  $indexedSeqInfo
 *
 *  @define willNotTerminateInf
 *  @define mayNotTerminateInf
 */
trait IndexedSeqOptimized[+A, +Repr] extends Any with IndexedSeqLike[A, Repr] { self =>

  override /*IterableLike*/
  def isEmpty: Boolean = { length == 0 }

  override /*IterableLike*/
  def foreach[U](f: A => U): Unit = {
    var i = 0
    val len = length
    while (i < len) { f(this(i)); i += 1 }
  }

  private def prefixLengthImpl(p: A => Boolean, expectTrue: Boolean): Int = {
    var i = 0
    while (i < length && p(apply(i)) == expectTrue) i += 1
    i
  }

  override /*IterableLike*/
  def forall(p: A => Boolean): Boolean = prefixLengthImpl(p, expectTrue = true) == length

  override /*IterableLike*/
  def exists(p: A => Boolean): Boolean = prefixLengthImpl(p, expectTrue = false) != length

  override /*IterableLike*/
  def find(p: A => Boolean): Option[A] = {
    val i = prefixLength(!p(_))
    if (i < length) Some(this(i)) else None
  }

  @tailrec
  private def foldl[B](start: Int, end: Int, z: B, op: (B, A) => B): B =
    if (start == end) z
    else foldl(start + 1, end, op(z, this(start)), op)

  @tailrec
  private def foldr[B](start: Int, end: Int, z: B, op: (A, B) => B): B =
    if (start == end) z
    else foldr(start, end - 1, op(this(end - 1), z), op)

  override /*TraversableLike*/
  def foldLeft[B](z: B)(op: (B, A) => B): B =
    foldl(0, length, z, op)

  override /*IterableLike*/
  def foldRight[B](z: B)(op: (A, B) => B): B =
    foldr(0, length, z, op)

  override /*TraversableLike*/
  def reduceLeft[B >: A](op: (B, A) => B): B =
    if (length > 0) foldl(1, length, this(0), op) else super.reduceLeft(op)

  override /*IterableLike*/
  def reduceRight[B >: A](op: (A, B) => B): B =
    if (length > 0) foldr(0, length - 1, this(length - 1), op) else super.reduceRight(op)

  override /*IterableLike*/
  def zip[A1 >: A, B, That](that: GenIterable[B])(implicit bf: CanBuildFrom[Repr, (A1, B), That]): That = that match {
    case that: IndexedSeq[_] =>
      val b = bf(repr)
      var i = 0
      val len = this.length min that.length
      b.sizeHint(len)
      while (i < len) {
        b += ((this(i), that(i).asInstanceOf[B]))
        i += 1
      }
      b.result()
    case _ =>
      super.zip[A1, B, That](that)(bf)
  }

  override /*IterableLike*/
  def zipWithIndex[A1 >: A, That](implicit bf: CanBuildFrom[Repr, (A1, Int), That]): That = {
    val b = bf(repr)
    val len = length
    b.sizeHint(len)
    var i = 0
    while (i < len) {
      b += ((this(i), i))
      i += 1
    }
    b.result()
  }

  override /*IterableLike*/
  def slice(from: Int, until: Int): Repr = {
    val lo    = math.max(from, 0)
    val hi    = math.min(math.max(until, 0), length)
    val elems = math.max(hi - lo, 0)
    val b     = newBuilder
    b.sizeHint(elems)

    var i = lo
    while (i < hi) {
      b += self(i)
      i += 1
    }
    b.result()
  }

  override /*IterableLike*/
  def head: A = if (isEmpty) super.head else this(0)

  override /*TraversableLike*/
  def tail: Repr = if (isEmpty) super.tail else slice(1, length)

  override /*TraversableLike*/
  def last: A = if (length > 0) this(length - 1) else super.last

  override /*IterableLike*/
  def init: Repr = if (length > 0) slice(0, length - 1) else super.init

  override /*TraversableLike*/
  def take(n: Int): Repr = slice(0, n)

  override /*TraversableLike*/
  def drop(n: Int): Repr = slice(n, length)

  override /*IterableLike*/
  def takeRight(n: Int): Repr = slice(length - math.max(n, 0), length)

  override /*IterableLike*/
  def dropRight(n: Int): Repr = slice(0, length - math.max(n, 0))

  override /*TraversableLike*/
  def splitAt(n: Int): (Repr, Repr) = (take(n), drop(n))

  override /*IterableLike*/
  def takeWhile(p: A => Boolean): Repr = take(prefixLength(p))

  override /*TraversableLike*/
  def dropWhile(p: A => Boolean): Repr = drop(prefixLength(p))

  override /*TraversableLike*/
  def span(p: A => Boolean): (Repr, Repr) = splitAt(prefixLength(p))

  override /*IterableLike*/
  def sameElements[B >: A](that: GenIterable[B]): Boolean = that match {
    case that: IndexedSeq[_] =>
      val len = length
      len == that.length && {
        var i = 0
        while (i < len && this(i) == that(i)) i += 1
        i == len
      }
    case _ =>
      super.sameElements(that)
  }

  override /*IterableLike*/
  def copyToArray[B >: A](xs: Array[B], start: Int, len: Int) {
    var i = 0
    var j = start
    val end = length min len min (xs.length - start)
    while (i < end) {
      xs(j) = this(i)
      i += 1
      j += 1
    }
  }

  // Overridden methods from Seq

  override /*SeqLike*/
  def lengthCompare(len: Int): Int = length - len

  override /*SeqLike*/
  def segmentLength(p: A => Boolean, from: Int): Int = {
    val len = length
    var i = from
    while (i < len && p(this(i))) i += 1
    i - from
  }

  private def negLength(n: Int) = if (n >= length) -1 else n

  override /*SeqLike*/
  def indexWhere(p: A => Boolean, from: Int): Int = {
    val start = math.max(from, 0)
    negLength(start + segmentLength(!p(_), start))
  }

  override /*SeqLike*/
  def lastIndexWhere(p: A => Boolean, end: Int): Int = {
    var i = math.min(end, length - 1)
    while (i >= 0 && !p(this(i))) i -= 1
    i
  }

  override /*SeqLike*/
  def reverse: Repr = {
    val b = newBuilder
    b.sizeHint(length)
    var i = length
    while (0 < i) {
      i -= 1
      b += this(i)
    }
    b.result()
  }

  override /*SeqLike*/
  def reverseIterator: Iterator[A] = new AbstractIterator[A] {
    private var i = self.length
    def hasNext: Boolean = 0 < i
    def next(): A =
      if (0 < i) {
        i -= 1
        self(i)
      } else Iterator.empty.next()
  }

  override /*SeqLike*/
  def startsWith[B](that: GenSeq[B], offset: Int): Boolean = that match {
    case that: IndexedSeq[_] =>
      var i = offset
      var j = 0
      val thisLen = length
      val thatLen = that.length
      while (i < thisLen && j < thatLen && this(i) == that(j)) {
        i += 1
        j += 1
      }
      j == thatLen
    case _ =>
      var i = offset
      val thisLen = length
      val thatElems = that.iterator
      while (i < thisLen && thatElems.hasNext) {
        if (this(i) != thatElems.next())
          return false

        i += 1
      }
      !thatElems.hasNext
  }

  override /*SeqLike*/
  def endsWith[B](that: GenSeq[B]): Boolean = that match {
    case that: IndexedSeq[_] =>
      var i = length - 1
      var j = that.length - 1

      (j <= i) && {
        while (j >= 0) {
          if (this(i) != that(j))
            return false
          i -= 1
          j -= 1
        }
        true
      }
    case _ =>
      super.endsWith(that)
  }

  override def toList: List[A] = {
    var i = length - 1
    var result: List[A] = Nil
    while (i >= 0) {
      result ::= apply(i)
      i -= 1
    }
    result
  }
}


[0m2021.02.26 13:45:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:45:12 INFO  time: compiled root in 0.9s[0m
[0m2021.02.26 13:45:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:45:15 INFO  time: compiled root in 0.7s[0m
[0m2021.02.26 13:48:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:48:03 INFO  time: compiled root in 0.67s[0m
[0m2021.02.26 13:48:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:48:13 INFO  time: compiled root in 0.55s[0m
[0m2021.02.26 13:50:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:50:19 INFO  time: compiled root in 0.57s[0m
[0m2021.02.26 13:52:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:52:32 INFO  time: compiled root in 0.61s[0m
[0m2021.02.26 13:54:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:54:53 INFO  time: compiled root in 0.57s[0m
[0m2021.02.26 13:56:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:56:31 INFO  time: compiled root in 0.68s[0m
[0m2021.02.26 13:58:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:58:57 INFO  time: compiled root in 0.6s[0m
[0m2021.02.26 13:59:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:59:15 INFO  time: compiled root in 0.17s[0m
[0m2021.02.26 13:59:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:59:27 INFO  time: compiled root in 0.6s[0m
[0m2021.02.26 14:01:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:01:28 INFO  time: compiled root in 0.58s[0m
[0m2021.02.26 14:01:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:01:32 INFO  time: compiled root in 0.55s[0m
[0m2021.02.26 14:04:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:04:01 INFO  time: compiled root in 0.85s[0m
[0m2021.02.26 14:04:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:04:24 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 14:05:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:05:37 INFO  time: compiled root in 0.63s[0m
[0m2021.02.26 14:07:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:07:20 INFO  time: compiled root in 0.65s[0m
[0m2021.02.26 14:08:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:08:08 INFO  time: compiled root in 0.62s[0m
[0m2021.02.26 14:09:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:09:04 INFO  time: compiled root in 0.73s[0m
[0m2021.02.26 14:11:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:11:44 INFO  time: compiled root in 0.63s[0m
[0m2021.02.26 14:14:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:14:15 INFO  time: compiled root in 0.69s[0m
[0m2021.02.26 14:14:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:14:42 INFO  time: compiled root in 0.58s[0m
Feb 26, 2021 2:15:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4558
[0m2021.02.26 14:15:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:15:28 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 14:15:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:15:39 INFO  time: compiled root in 0.69s[0m
[0m2021.02.26 14:16:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:16:23 INFO  time: compiled root in 0.62s[0m
[0m2021.02.26 14:16:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:16:31 INFO  time: compiled root in 97ms[0m
[0m2021.02.26 14:16:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:16:33 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 14:18:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:18:09 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 14:19:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:19:05 INFO  time: compiled root in 0.68s[0m
[0m2021.02.26 14:19:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:19:10 INFO  time: compiled root in 0.68s[0m
[0m2021.02.26 14:20:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:20:11 INFO  time: compiled root in 0.15s[0m
[0m2021.02.26 14:20:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:20:41 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 14:21:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:21:57 INFO  time: compiled root in 0.66s[0m
[0m2021.02.26 14:24:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:24:01 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 14:24:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:24:12 INFO  time: compiled root in 0.66s[0m
Feb 26, 2021 2:25:33 PM scala.meta.internal.pc.completions.Completions$class completionPosition
SEVERE: null
java.lang.NullPointerException
	at scala.meta.internal.pc.completions.MatchCaseCompletions$Parents.<init>(MatchCaseCompletions.scala:409)
	at scala.meta.internal.pc.completions.MatchCaseCompletions$Parents.<init>(MatchCaseCompletions.scala:399)
	at scala.meta.internal.pc.completions.MatchCaseCompletions$CaseKeywordCompletion.<init>(MatchCaseCompletions.scala:66)
	at scala.meta.internal.pc.completions.Completions$class.completionPositionUnsafe(Completions.scala:481)
	at scala.meta.internal.pc.MetalsGlobal.completionPositionUnsafe(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.completions.Completions$class.completionPosition(Completions.scala:396)
	at scala.meta.internal.pc.MetalsGlobal.completionPosition(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:441)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)
	at scala.meta.internal.pc.ScalaPresentationCompiler$$anonfun$complete$1.apply(ScalaPresentationCompiler.scala:124)
	at scala.meta.internal.pc.ScalaPresentationCompiler$$anonfun$complete$1.apply(ScalaPresentationCompiler.scala:124)
	at scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:137)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$1.apply(CompilerAccess.scala:87)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$onCompilerJobQueue$1.apply$mcV$sp(CompilerAccess.scala:197)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[0m2021.02.26 14:26:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:26:08 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 14:26:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:26:20 INFO  time: compiled root in 0.67s[0m
[0m2021.02.26 14:27:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:27:18 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 14:27:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:27:22 INFO  time: compiled root in 0.78s[0m
[0m2021.02.26 14:28:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:28:49 INFO  time: compiled root in 0.84s[0m
[0m2021.02.26 14:29:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:29:04 INFO  time: compiled root in 0.7s[0m
[0m2021.02.26 14:31:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:31:53 INFO  time: compiled root in 0.65s[0m
[0m2021.02.26 14:34:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:34:04 INFO  time: compiled root in 0.72s[0m
[0m2021.02.26 14:35:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:35:22 INFO  time: compiled root in 0.66s[0m
[0m2021.02.26 14:36:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:36:50 INFO  time: compiled root in 0.23s[0m
[0m2021.02.26 14:38:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:38:11 INFO  time: compiled root in 0.65s[0m
[0m2021.02.26 14:38:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:38:27 INFO  time: compiled root in 0.76s[0m
[0m2021.02.26 14:42:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:42:23 INFO  time: compiled root in 98ms[0m
[0m2021.02.26 14:42:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:42:28 INFO  time: compiled root in 95ms[0m
[0m2021.02.26 14:42:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:42:34 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 14:42:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:100: stale bloop error: ')' expected but string literal found.
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.(url.contains(\"job\")",
                                                                                                   ^[0m
[0m2021.02.26 14:42:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:42:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: unclosed string literal
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.url,
          ^[0m
[0m2021.02.26 14:42:40 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 14:42:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: unclosed string literal
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.url,
          ^[0m
[0m2021.02.26 14:42:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:64:11: stale bloop error: unclosed string literal
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Links.url,
          ^[0m
[0m2021.02.26 14:43:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:43:32 INFO  time: compiled root in 0.18s[0m
[0m2021.02.26 14:43:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:43:36 INFO  time: compiled root in 0.18s[0m
[0m2021.02.26 14:43:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:43:44 INFO  time: compiled root in 0.17s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.{typeTag, TypeTag}
import scala.util.Try
import scala.util.control.NonFatal

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.sql.api.java._
import org.apache.spark.sql.catalyst.ScalaReflection
import org.apache.spark.sql.catalyst.analysis.{Star, UnresolvedFunction}
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate._
import org.apache.spark.sql.catalyst.plans.logical.{HintInfo, ResolvedHint}
import org.apache.spark.sql.execution.SparkSqlParser
import org.apache.spark.sql.expressions.{SparkUserDefinedFunction, UserDefinedFunction}
import org.apache.spark.sql.internal.SQLConf
import org.apache.spark.sql.types._
import org.apache.spark.util.Utils


/**
 * Commonly used functions available for DataFrame operations. Using functions defined here provides
 * a little bit more compile-time safety to make sure the function exists.
 *
 * Spark also includes more built-in functions that are less common and are not defined here.
 * You can still access them (and all the functions defined here) using the `functions.expr()` API
 * and calling them through a SQL expression string. You can find the entire list of functions
 * at SQL API documentation.
 *
 * As an example, `isnan` is a function that is defined here. You can use `isnan(col("myCol"))`
 * to invoke the `isnan` function. This way the programming language's compiler ensures `isnan`
 * exists and is of the proper form. You can also use `expr("isnan(myCol)")` function to invoke the
 * same function. In this case, Spark itself will ensure `isnan` exists when it analyzes the query.
 *
 * `regr_count` is an example of a function that is built-in but not defined here, because it is
 * less commonly used. To invoke it, use `expr("regr_count(yCol, xCol)")`.
 *
 * @groupname udf_funcs UDF functions
 * @groupname agg_funcs Aggregate functions
 * @groupname datetime_funcs Date time functions
 * @groupname sort_funcs Sorting functions
 * @groupname normal_funcs Non-aggregate functions
 * @groupname math_funcs Math functions
 * @groupname misc_funcs Misc functions
 * @groupname window_funcs Window functions
 * @groupname string_funcs String functions
 * @groupname collection_funcs Collection functions
 * @groupname Ungrouped Support functions for DataFrames
 * @since 1.3.0
 */
@InterfaceStability.Stable
// scalastyle:off
object functions {
// scalastyle:on

  private def withExpr(expr: Expression): Column = Column(expr)

  private def withAggregateFunction(
    func: AggregateFunction,
    isDistinct: Boolean = false): Column = {
    Column(func.toAggregateExpression(isDistinct))
  }

  /**
   * Returns a [[Column]] based on the given column name.
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def col(colName: String): Column = Column(colName)

  /**
   * Returns a [[Column]] based on the given column name. Alias of [[col]].
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def column(colName: String): Column = Column(colName)

  /**
   * Creates a [[Column]] of literal value.
   *
   * The passed in object is returned directly if it is already a [[Column]].
   * If the object is a Scala Symbol, it is converted into a [[Column]] also.
   * Otherwise, a new [[Column]] is created to represent the literal value.
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def lit(literal: Any): Column = typedLit(literal)

  /**
   * Creates a [[Column]] of literal value.
   *
   * The passed in object is returned directly if it is already a [[Column]].
   * If the object is a Scala Symbol, it is converted into a [[Column]] also.
   * Otherwise, a new [[Column]] is created to represent the literal value.
   * The difference between this function and [[lit]] is that this function
   * can handle parameterized scala types e.g.: List, Seq and Map.
   *
   * @group normal_funcs
   * @since 2.2.0
   */
  def typedLit[T : TypeTag](literal: T): Column = literal match {
    case c: Column => c
    case s: Symbol => new ColumnName(s.name)
    case _ => Column(Literal.create(literal))
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Sort functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   df.sort(asc("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 1.3.0
   */
  def asc(columnName: String): Column = Column(columnName).asc

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   df.sort(asc_nulls_first("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def asc_nulls_first(columnName: String): Column = Column(columnName).asc_nulls_first

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   df.sort(asc_nulls_last("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def asc_nulls_last(columnName: String): Column = Column(columnName).asc_nulls_last

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   df.sort(asc("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 1.3.0
   */
  def desc(columnName: String): Column = Column(columnName).desc

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   df.sort(asc("dept"), desc_nulls_first("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def desc_nulls_first(columnName: String): Column = Column(columnName).desc_nulls_first

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   df.sort(asc("dept"), desc_nulls_last("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def desc_nulls_last(columnName: String): Column = Column(columnName).desc_nulls_last


  //////////////////////////////////////////////////////////////////////////////////////////////
  // Aggregate functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(e: Column): Column = approx_count_distinct(e)

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(columnName: String): Column = approx_count_distinct(columnName)

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(e: Column, rsd: Double): Column = approx_count_distinct(e, rsd)

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(columnName: String, rsd: Double): Column = {
    approx_count_distinct(Column(columnName), rsd)
  }

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(e: Column): Column = withAggregateFunction {
    HyperLogLogPlusPlus(e.expr)
  }

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(columnName: String): Column = approx_count_distinct(column(columnName))

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @param rsd maximum estimation error allowed (default = 0.05)
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(e: Column, rsd: Double): Column = withAggregateFunction {
    HyperLogLogPlusPlus(e.expr, rsd, 0, 0)
  }

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @param rsd maximum estimation error allowed (default = 0.05)
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(columnName: String, rsd: Double): Column = {
    approx_count_distinct(Column(columnName), rsd)
  }

  /**
   * Aggregate function: returns the average of the values in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def avg(e: Column): Column = withAggregateFunction { Average(e.expr) }

  /**
   * Aggregate function: returns the average of the values in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def avg(columnName: String): Column = avg(Column(columnName))

  /**
   * Aggregate function: returns a list of objects with duplicates.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_list(e: Column): Column = withAggregateFunction { CollectList(e.expr) }

  /**
   * Aggregate function: returns a list of objects with duplicates.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_list(columnName: String): Column = collect_list(Column(columnName))

  /**
   * Aggregate function: returns a set of objects with duplicate elements eliminated.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_set(e: Column): Column = withAggregateFunction { CollectSet(e.expr) }

  /**
   * Aggregate function: returns a set of objects with duplicate elements eliminated.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_set(columnName: String): Column = collect_set(Column(columnName))

  /**
   * Aggregate function: returns the Pearson Correlation Coefficient for two columns.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def corr(column1: Column, column2: Column): Column = withAggregateFunction {
    Corr(column1.expr, column2.expr)
  }

  /**
   * Aggregate function: returns the Pearson Correlation Coefficient for two columns.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def corr(columnName1: String, columnName2: String): Column = {
    corr(Column(columnName1), Column(columnName2))
  }

  /**
   * Aggregate function: returns the number of items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def count(e: Column): Column = withAggregateFunction {
    e.expr match {
      // Turn count(*) into count(1)
      case s: Star => Count(Literal(1))
      case _ => Count(e.expr)
    }
  }

  /**
   * Aggregate function: returns the number of items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def count(columnName: String): TypedColumn[Any, Long] =
    count(Column(columnName)).as(ExpressionEncoder[Long]())

  /**
   * Aggregate function: returns the number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  @scala.annotation.varargs
  def countDistinct(expr: Column, exprs: Column*): Column = {
    withAggregateFunction(Count.apply((expr +: exprs).map(_.expr)), isDistinct = true)
  }

  /**
   * Aggregate function: returns the number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  @scala.annotation.varargs
  def countDistinct(columnName: String, columnNames: String*): Column =
    countDistinct(Column(columnName), columnNames.map(Column.apply) : _*)

  /**
   * Aggregate function: returns the population covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_pop(column1: Column, column2: Column): Column = withAggregateFunction {
    CovPopulation(column1.expr, column2.expr)
  }

  /**
   * Aggregate function: returns the population covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_pop(columnName1: String, columnName2: String): Column = {
    covar_pop(Column(columnName1), Column(columnName2))
  }

  /**
   * Aggregate function: returns the sample covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_samp(column1: Column, column2: Column): Column = withAggregateFunction {
    CovSample(column1.expr, column2.expr)
  }

  /**
   * Aggregate function: returns the sample covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_samp(columnName1: String, columnName2: String): Column = {
    covar_samp(Column(columnName1), Column(columnName2))
  }

  /**
   * Aggregate function: returns the first value in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def first(e: Column, ignoreNulls: Boolean): Column = withAggregateFunction {
    new First(e.expr, ignoreNulls)
  }

  /**
   * Aggregate function: returns the first value of a column in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def first(columnName: String, ignoreNulls: Boolean): Column = {
    first(Column(columnName), ignoreNulls)
  }

  /**
   * Aggregate function: returns the first value in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def first(e: Column): Column = first(e, ignoreNulls = false)

  /**
   * Aggregate function: returns the first value of a column in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def first(columnName: String): Column = first(Column(columnName))

  /**
   * Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated
   * or not, returns 1 for aggregated or 0 for not aggregated in the result set.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping(e: Column): Column = Column(Grouping(e.expr))

  /**
   * Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated
   * or not, returns 1 for aggregated or 0 for not aggregated in the result set.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping(columnName: String): Column = grouping(Column(columnName))

  /**
   * Aggregate function: returns the level of grouping, equals to
   *
   * {{{
   *   (grouping(c1) <<; (n-1)) + (grouping(c2) <<; (n-2)) + ... + grouping(cn)
   * }}}
   *
   * @note The list of columns should match with grouping columns exactly, or empty (means all the
   * grouping columns).
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping_id(cols: Column*): Column = Column(GroupingID(cols.map(_.expr)))

  /**
   * Aggregate function: returns the level of grouping, equals to
   *
   * {{{
   *   (grouping(c1) <<; (n-1)) + (grouping(c2) <<; (n-2)) + ... + grouping(cn)
   * }}}
   *
   * @note The list of columns should match with grouping columns exactly.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping_id(colName: String, colNames: String*): Column = {
    grouping_id((Seq(colName) ++ colNames).map(n => Column(n)) : _*)
  }

  /**
   * Aggregate function: returns the kurtosis of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def kurtosis(e: Column): Column = withAggregateFunction { Kurtosis(e.expr) }

  /**
   * Aggregate function: returns the kurtosis of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def kurtosis(columnName: String): Column = kurtosis(Column(columnName))

  /**
   * Aggregate function: returns the last value in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def last(e: Column, ignoreNulls: Boolean): Column = withAggregateFunction {
    new Last(e.expr, ignoreNulls)
  }

  /**
   * Aggregate function: returns the last value of the column in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def last(columnName: String, ignoreNulls: Boolean): Column = {
    last(Column(columnName), ignoreNulls)
  }

  /**
   * Aggregate function: returns the last value in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def last(e: Column): Column = last(e, ignoreNulls = false)

  /**
   * Aggregate function: returns the last value of the column in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def last(columnName: String): Column = last(Column(columnName), ignoreNulls = false)

  /**
   * Aggregate function: returns the maximum value of the expression in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def max(e: Column): Column = withAggregateFunction { Max(e.expr) }

  /**
   * Aggregate function: returns the maximum value of the column in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def max(columnName: String): Column = max(Column(columnName))

  /**
   * Aggregate function: returns the average of the values in a group.
   * Alias for avg.
   *
   * @group agg_funcs
   * @since 1.4.0
   */
  def mean(e: Column): Column = avg(e)

  /**
   * Aggregate function: returns the average of the values in a group.
   * Alias for avg.
   *
   * @group agg_funcs
   * @since 1.4.0
   */
  def mean(columnName: String): Column = avg(columnName)

  /**
   * Aggregate function: returns the minimum value of the expression in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def min(e: Column): Column = withAggregateFunction { Min(e.expr) }

  /**
   * Aggregate function: returns the minimum value of the column in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def min(columnName: String): Column = min(Column(columnName))

  /**
   * Aggregate function: returns the skewness of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def skewness(e: Column): Column = withAggregateFunction { Skewness(e.expr) }

  /**
   * Aggregate function: returns the skewness of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def skewness(columnName: String): Column = skewness(Column(columnName))

  /**
   * Aggregate function: alias for `stddev_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev(e: Column): Column = withAggregateFunction { StddevSamp(e.expr) }

  /**
   * Aggregate function: alias for `stddev_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev(columnName: String): Column = stddev(Column(columnName))

  /**
   * Aggregate function: returns the sample standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_samp(e: Column): Column = withAggregateFunction { StddevSamp(e.expr) }

  /**
   * Aggregate function: returns the sample standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_samp(columnName: String): Column = stddev_samp(Column(columnName))

  /**
   * Aggregate function: returns the population standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_pop(e: Column): Column = withAggregateFunction { StddevPop(e.expr) }

  /**
   * Aggregate function: returns the population standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_pop(columnName: String): Column = stddev_pop(Column(columnName))

  /**
   * Aggregate function: returns the sum of all values in the expression.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sum(e: Column): Column = withAggregateFunction { Sum(e.expr) }

  /**
   * Aggregate function: returns the sum of all values in the given column.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sum(columnName: String): Column = sum(Column(columnName))

  /**
   * Aggregate function: returns the sum of distinct values in the expression.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sumDistinct(e: Column): Column = withAggregateFunction(Sum(e.expr), isDistinct = true)

  /**
   * Aggregate function: returns the sum of distinct values in the expression.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sumDistinct(columnName: String): Column = sumDistinct(Column(columnName))

  /**
   * Aggregate function: alias for `var_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def variance(e: Column): Column = withAggregateFunction { VarianceSamp(e.expr) }

  /**
   * Aggregate function: alias for `var_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def variance(columnName: String): Column = variance(Column(columnName))

  /**
   * Aggregate function: returns the unbiased variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_samp(e: Column): Column = withAggregateFunction { VarianceSamp(e.expr) }

  /**
   * Aggregate function: returns the unbiased variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_samp(columnName: String): Column = var_samp(Column(columnName))

  /**
   * Aggregate function: returns the population variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_pop(e: Column): Column = withAggregateFunction { VariancePop(e.expr) }

  /**
   * Aggregate function: returns the population variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_pop(columnName: String): Column = var_pop(Column(columnName))


  //////////////////////////////////////////////////////////////////////////////////////////////
  // Window functions
  //////////////////////////////////////////////////////////////////////////////////////////////
  /**
   * This function has been deprecated in Spark 2.4. See SPARK-25842 for more information.
   *
   * @group window_funcs
   * @since 2.3.0
   */
  @deprecated("Use Window.unboundedPreceding", "2.4.0")
  def unboundedPreceding(): Column = Column(UnboundedPreceding)

  /**
   * This function has been deprecated in Spark 2.4. See SPARK-25842 for more information.
   *
   * @group window_funcs
   * @since 2.3.0
   */
  @deprecated("Use Window.unboundedFollowing", "2.4.0")
  def unboundedFollowing(): Column = Column(UnboundedFollowing)

  /**
   * This function has been deprecated in Spark 2.4. See SPARK-25842 for more information.
   *
   * @group window_funcs
   * @since 2.3.0
   */
  @deprecated("Use Window.currentRow", "2.4.0")
  def currentRow(): Column = Column(CurrentRow)

  /**
   * Window function: returns the cumulative distribution of values within a window partition,
   * i.e. the fraction of rows that are below the current row.
   *
   * {{{
   *   N = total number of rows in the partition
   *   cumeDist(x) = number of values before (and including) x / N
   * }}}
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def cume_dist(): Column = withExpr { new CumeDist }

  /**
   * Window function: returns the rank of rows within a window partition, without any gaps.
   *
   * The difference between rank and dense_rank is that denseRank leaves no gaps in ranking
   * sequence when there are ties. That is, if you were ranking a competition using dense_rank
   * and had three people tie for second place, you would say that all three were in second
   * place and that the next person came in third. Rank would give me sequential numbers, making
   * the person that came in third place (after the ties) would register as coming in fifth.
   *
   * This is equivalent to the DENSE_RANK function in SQL.
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def dense_rank(): Column = withExpr { new DenseRank }

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `null` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(e: Column, offset: Int): Column = lag(e, offset, null)

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `null` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(columnName: String, offset: Int): Column = lag(columnName, offset, null)

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `defaultValue` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(columnName: String, offset: Int, defaultValue: Any): Column = {
    lag(Column(columnName), offset, defaultValue)
  }

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `defaultValue` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(e: Column, offset: Int, defaultValue: Any): Column = withExpr {
    Lag(e.expr, Literal(offset), Literal(defaultValue))
  }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `null` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(columnName: String, offset: Int): Column = { lead(columnName, offset, null) }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `null` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(e: Column, offset: Int): Column = { lead(e, offset, null) }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `defaultValue` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(columnName: String, offset: Int, defaultValue: Any): Column = {
    lead(Column(columnName), offset, defaultValue)
  }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `defaultValue` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(e: Column, offset: Int, defaultValue: Any): Column = withExpr {
    Lead(e.expr, Literal(offset), Literal(defaultValue))
  }

  /**
   * Window function: returns the ntile group id (from 1 to `n` inclusive) in an ordered window
   * partition. For example, if `n` is 4, the first quarter of the rows will get value 1, the second
   * quarter will get 2, the third quarter will get 3, and the last quarter will get 4.
   *
   * This is equivalent to the NTILE function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def ntile(n: Int): Column = withExpr { new NTile(Literal(n)) }

  /**
   * Window function: returns the relative rank (i.e. percentile) of rows within a window partition.
   *
   * This is computed by:
   * {{{
   *   (rank of row in its partition - 1) / (number of rows in the partition - 1)
   * }}}
   *
   * This is equivalent to the PERCENT_RANK function in SQL.
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def percent_rank(): Column = withExpr { new PercentRank }

  /**
   * Window function: returns the rank of rows within a window partition.
   *
   * The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking
   * sequence when there are ties. That is, if you were ranking a competition using dense_rank
   * and had three people tie for second place, you would say that all three were in second
   * place and that the next person came in third. Rank would give me sequential numbers, making
   * the person that came in third place (after the ties) would register as coming in fifth.
   *
   * This is equivalent to the RANK function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def rank(): Column = withExpr { new Rank }

  /**
   * Window function: returns a sequential number starting at 1 within a window partition.
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def row_number(): Column = withExpr { RowNumber() }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Non-aggregate functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Creates a new array column. The input columns must all have the same data type.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def array(cols: Column*): Column = withExpr { CreateArray(cols.map(_.expr)) }

  /**
   * Creates a new array column. The input columns must all have the same data type.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def array(colName: String, colNames: String*): Column = {
    array((colName +: colNames).map(col) : _*)
  }

  /**
   * Creates a new map column. The input columns must be grouped as key-value pairs, e.g.
   * (key1, value1, key2, value2, ...). The key columns must all have the same data type, and can't
   * be null. The value columns must all have the same data type.
   *
   * @group normal_funcs
   * @since 2.0
   */
  @scala.annotation.varargs
  def map(cols: Column*): Column = withExpr { CreateMap(cols.map(_.expr)) }

  /**
   * Creates a new map column. The array in the first column is used for keys. The array in the
   * second column is used for values. All elements in the array for key should not be null.
   *
   * @group normal_funcs
   * @since 2.4
   */
  def map_from_arrays(keys: Column, values: Column): Column = withExpr {
    MapFromArrays(keys.expr, values.expr)
  }

  /**
   * Marks a DataFrame as small enough for use in broadcast joins.
   *
   * The following example marks the right DataFrame for broadcast hash join using `joinKey`.
   * {{{
   *   // left and right are DataFrames
   *   left.join(broadcast(right), "joinKey")
   * }}}
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  def broadcast[T](df: Dataset[T]): Dataset[T] = {
    Dataset[T](df.sparkSession,
      ResolvedHint(df.logicalPlan, HintInfo(broadcast = true)))(df.exprEnc)
  }

  /**
   * Returns the first column that is not null, or null if all inputs are null.
   *
   * For example, `coalesce(a, b, c)` will return a if a is not null,
   * or b if a is null and b is not null, or c if both a and b are null but c is not null.
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  @scala.annotation.varargs
  def coalesce(e: Column*): Column = withExpr { Coalesce(e.map(_.expr)) }

  /**
   * Creates a string column for the file name of the current Spark task.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def input_file_name(): Column = withExpr { InputFileName() }

  /**
   * Return true iff the column is NaN.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def isnan(e: Column): Column = withExpr { IsNaN(e.expr) }

  /**
   * Return true iff the column is null.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def isnull(e: Column): Column = withExpr { IsNull(e.expr) }

  /**
   * A column expression that generates monotonically increasing 64-bit integers.
   *
   * The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.
   * The current implementation puts the partition ID in the upper 31 bits, and the record number
   * within each partition in the lower 33 bits. The assumption is that the data frame has
   * less than 1 billion partitions, and each partition has less than 8 billion records.
   *
   * As an example, consider a `DataFrame` with two partitions, each with 3 records.
   * This expression would return the following IDs:
   *
   * {{{
   * 0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.
   * }}}
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @deprecated("Use monotonically_increasing_id()", "2.0.0")
  def monotonicallyIncreasingId(): Column = monotonically_increasing_id()

  /**
   * A column expression that generates monotonically increasing 64-bit integers.
   *
   * The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.
   * The current implementation puts the partition ID in the upper 31 bits, and the record number
   * within each partition in the lower 33 bits. The assumption is that the data frame has
   * less than 1 billion partitions, and each partition has less than 8 billion records.
   *
   * As an example, consider a `DataFrame` with two partitions, each with 3 records.
   * This expression would return the following IDs:
   *
   * {{{
   * 0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.
   * }}}
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def monotonically_increasing_id(): Column = withExpr { MonotonicallyIncreasingID() }

  /**
   * Returns col1 if it is not NaN, or col2 if col1 is NaN.
   *
   * Both inputs should be floating point columns (DoubleType or FloatType).
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  def nanvl(col1: Column, col2: Column): Column = withExpr { NaNvl(col1.expr, col2.expr) }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Select the amount column and negates all values.
   *   // Scala:
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   df.select( negate(df.col("amount")) );
   * }}}
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def negate(e: Column): Column = -e

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def not(e: Column): Column = !e

  /**
   * Generate a random column with independent and identically distributed (i.i.d.) samples
   * uniformly distributed in [0.0, 1.0).
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def rand(seed: Long): Column = withExpr { Rand(seed) }

  /**
   * Generate a random column with independent and identically distributed (i.i.d.) samples
   * uniformly distributed in [0.0, 1.0).
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def rand(): Column = rand(Utils.random.nextLong)

  /**
   * Generate a column with independent and identically distributed (i.i.d.) samples from
   * the standard normal distribution.
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def randn(seed: Long): Column = withExpr { Randn(seed) }

  /**
   * Generate a column with independent and identically distributed (i.i.d.) samples from
   * the standard normal distribution.
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def randn(): Column = randn(Utils.random.nextLong)

  /**
   * Partition ID.
   *
   * @note This is non-deterministic because it depends on data partitioning and task scheduling.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def spark_partition_id(): Column = withExpr { SparkPartitionID() }

  /**
   * Computes the square root of the specified float value.
   *
   * @group math_funcs
   * @since 1.3.0
   */
  def sqrt(e: Column): Column = withExpr { Sqrt(e.expr) }

  /**
   * Computes the square root of the specified float value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def sqrt(colName: String): Column = sqrt(Column(colName))

  /**
   * Creates a new struct column.
   * If the input column is a column in a `DataFrame`, or a derived column expression
   * that is named (i.e. aliased), its name would be retained as the StructField's name,
   * otherwise, the newly generated StructField's name would be auto generated as
   * `col` with a suffix `index + 1`, i.e. col1, col2, col3, ...
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def struct(cols: Column*): Column = withExpr { CreateStruct(cols.map(_.expr)) }

  /**
   * Creates a new struct column that composes multiple input columns.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def struct(colName: String, colNames: String*): Column = {
    struct((colName +: colNames).map(col) : _*)
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = withExpr {
    CaseWhen(Seq((condition.expr, lit(value).expr)))
  }

  /**
   * Computes bitwise NOT (~) of a number.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def bitwiseNOT(e: Column): Column = withExpr { BitwiseNot(e.expr) }

  /**
   * Parses the expression string into the column that it represents, similar to
   * [[Dataset#selectExpr]].
   * {{{
   *   // get the number of words of each length
   *   df.groupBy(expr("length(word)")).count()
   * }}}
   *
   * @group normal_funcs
   */
  def expr(expr: String): Column = {
    val parser = SparkSession.getActiveSession.map(_.sessionState.sqlParser).getOrElse {
      new SparkSqlParser(new SQLConf)
    }
    Column(parser.parseExpression(expr))
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Math Functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Computes the absolute value of a numeric value.
   *
   * @group math_funcs
   * @since 1.3.0
   */
  def abs(e: Column): Column = withExpr { Abs(e.expr) }

  /**
   * @return inverse cosine of `e` in radians, as if computed by `java.lang.Math.acos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def acos(e: Column): Column = withExpr { Acos(e.expr) }

  /**
   * @return inverse cosine of `columnName`, as if computed by `java.lang.Math.acos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def acos(columnName: String): Column = acos(Column(columnName))

  /**
   * @return inverse sine of `e` in radians, as if computed by `java.lang.Math.asin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def asin(e: Column): Column = withExpr { Asin(e.expr) }

  /**
   * @return inverse sine of `columnName`, as if computed by `java.lang.Math.asin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def asin(columnName: String): Column = asin(Column(columnName))

  /**
   * @return inverse tangent of `e`, as if computed by `java.lang.Math.atan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan(e: Column): Column = withExpr { Atan(e.expr) }

  /**
   * @return inverse tangent of `columnName`, as if computed by `java.lang.Math.atan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan(columnName: String): Column = atan(Column(columnName))

  /**
   * @param y coordinate on y-axis
   * @param x coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(y: Column, x: Column): Column = withExpr { Atan2(y.expr, x.expr) }

  /**
   * @param y coordinate on y-axis
   * @param xName coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(y: Column, xName: String): Column = atan2(y, Column(xName))

  /**
   * @param yName coordinate on y-axis
   * @param x coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yName: String, x: Column): Column = atan2(Column(yName), x)

  /**
   * @param yName coordinate on y-axis
   * @param xName coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yName: String, xName: String): Column =
    atan2(Column(yName), Column(xName))

  /**
   * @param y coordinate on y-axis
   * @param xValue coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(y: Column, xValue: Double): Column = atan2(y, lit(xValue))

  /**
   * @param yName coordinate on y-axis
   * @param xValue coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yName: String, xValue: Double): Column = atan2(Column(yName), xValue)

  /**
   * @param yValue coordinate on y-axis
   * @param x coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yValue: Double, x: Column): Column = atan2(lit(yValue), x)

  /**
   * @param yValue coordinate on y-axis
   * @param xName coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yValue: Double, xName: String): Column = atan2(yValue, Column(xName))

  /**
   * An expression that returns the string representation of the binary value of the given long
   * column. For example, bin("12") returns "1100".
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def bin(e: Column): Column = withExpr { Bin(e.expr) }

  /**
   * An expression that returns the string representation of the binary value of the given long
   * column. For example, bin("12") returns "1100".
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def bin(columnName: String): Column = bin(Column(columnName))

  /**
   * Computes the cube-root of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cbrt(e: Column): Column = withExpr { Cbrt(e.expr) }

  /**
   * Computes the cube-root of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cbrt(columnName: String): Column = cbrt(Column(columnName))

  /**
   * Computes the ceiling of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def ceil(e: Column): Column = withExpr { Ceil(e.expr) }

  /**
   * Computes the ceiling of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def ceil(columnName: String): Column = ceil(Column(columnName))

  /**
   * Convert a number in a string column from one base to another.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def conv(num: Column, fromBase: Int, toBase: Int): Column = withExpr {
    Conv(num.expr, lit(fromBase).expr, lit(toBase).expr)
  }

  /**
   * @param e angle in radians
   * @return cosine of the angle, as if computed by `java.lang.Math.cos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cos(e: Column): Column = withExpr { Cos(e.expr) }

  /**
   * @param columnName angle in radians
   * @return cosine of the angle, as if computed by `java.lang.Math.cos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cos(columnName: String): Column = cos(Column(columnName))

  /**
   * @param e hyperbolic angle
   * @return hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cosh(e: Column): Column = withExpr { Cosh(e.expr) }

  /**
   * @param columnName hyperbolic angle
   * @return hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cosh(columnName: String): Column = cosh(Column(columnName))

  /**
   * Computes the exponential of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def exp(e: Column): Column = withExpr { Exp(e.expr) }

  /**
   * Computes the exponential of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def exp(columnName: String): Column = exp(Column(columnName))

  /**
   * Computes the exponential of the given value minus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def expm1(e: Column): Column = withExpr { Expm1(e.expr) }

  /**
   * Computes the exponential of the given column minus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def expm1(columnName: String): Column = expm1(Column(columnName))

  /**
   * Computes the factorial of the given value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def factorial(e: Column): Column = withExpr { Factorial(e.expr) }

  /**
   * Computes the floor of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def floor(e: Column): Column = withExpr { Floor(e.expr) }

  /**
   * Computes the floor of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def floor(columnName: String): Column = floor(Column(columnName))

  /**
   * Returns the greatest value of the list of values, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def greatest(exprs: Column*): Column = withExpr { Greatest(exprs.map(_.expr)) }

  /**
   * Returns the greatest value of the list of column names, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def greatest(columnName: String, columnNames: String*): Column = {
    greatest((columnName +: columnNames).map(Column.apply): _*)
  }

  /**
   * Computes hex value of the given column.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def hex(column: Column): Column = withExpr { Hex(column.expr) }

  /**
   * Inverse of hex. Interprets each pair of characters as a hexadecimal number
   * and converts to the byte representation of number.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def unhex(column: Column): Column = withExpr { Unhex(column.expr) }

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Column, r: Column): Column = withExpr { Hypot(l.expr, r.expr) }

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Column, rightName: String): Column = hypot(l, Column(rightName))

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(leftName: String, r: Column): Column = hypot(Column(leftName), r)

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(leftName: String, rightName: String): Column =
    hypot(Column(leftName), Column(rightName))

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Column, r: Double): Column = hypot(l, lit(r))

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(leftName: String, r: Double): Column = hypot(Column(leftName), r)

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Double, r: Column): Column = hypot(lit(l), r)

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Double, rightName: String): Column = hypot(l, Column(rightName))

  /**
   * Returns the least value of the list of values, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def least(exprs: Column*): Column = withExpr { Least(exprs.map(_.expr)) }

  /**
   * Returns the least value of the list of column names, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def least(columnName: String, columnNames: String*): Column = {
    least((columnName +: columnNames).map(Column.apply): _*)
  }

  /**
   * Computes the natural logarithm of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(e: Column): Column = withExpr { Log(e.expr) }

  /**
   * Computes the natural logarithm of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(columnName: String): Column = log(Column(columnName))

  /**
   * Returns the first argument-base logarithm of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(base: Double, a: Column): Column = withExpr { Logarithm(lit(base).expr, a.expr) }

  /**
   * Returns the first argument-base logarithm of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(base: Double, columnName: String): Column = log(base, Column(columnName))

  /**
   * Computes the logarithm of the given value in base 10.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log10(e: Column): Column = withExpr { Log10(e.expr) }

  /**
   * Computes the logarithm of the given value in base 10.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log10(columnName: String): Column = log10(Column(columnName))

  /**
   * Computes the natural logarithm of the given value plus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log1p(e: Column): Column = withExpr { Log1p(e.expr) }

  /**
   * Computes the natural logarithm of the given column plus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log1p(columnName: String): Column = log1p(Column(columnName))

  /**
   * Computes the logarithm of the given column in base 2.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def log2(expr: Column): Column = withExpr { Log2(expr.expr) }

  /**
   * Computes the logarithm of the given value in base 2.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def log2(columnName: String): Column = log2(Column(columnName))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Column, r: Column): Column = withExpr { Pow(l.expr, r.expr) }

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Column, rightName: String): Column = pow(l, Column(rightName))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(leftName: String, r: Column): Column = pow(Column(leftName), r)

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(leftName: String, rightName: String): Column = pow(Column(leftName), Column(rightName))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Column, r: Double): Column = pow(l, lit(r))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(leftName: String, r: Double): Column = pow(Column(leftName), r)

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Double, r: Column): Column = pow(lit(l), r)

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Double, rightName: String): Column = pow(l, Column(rightName))

  /**
   * Returns the positive value of dividend mod divisor.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def pmod(dividend: Column, divisor: Column): Column = withExpr {
    Pmod(dividend.expr, divisor.expr)
  }

  /**
   * Returns the double value that is closest in value to the argument and
   * is equal to a mathematical integer.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def rint(e: Column): Column = withExpr { Rint(e.expr) }

  /**
   * Returns the double value that is closest in value to the argument and
   * is equal to a mathematical integer.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def rint(columnName: String): Column = rint(Column(columnName))

  /**
   * Returns the value of the column `e` rounded to 0 decimal places with HALF_UP round mode.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def round(e: Column): Column = round(e, 0)

  /**
   * Round the value of `e` to `scale` decimal places with HALF_UP round mode
   * if `scale` is greater than or equal to 0 or at integral part when `scale` is less than 0.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def round(e: Column, scale: Int): Column = withExpr { Round(e.expr, Literal(scale)) }

  /**
   * Returns the value of the column `e` rounded to 0 decimal places with HALF_EVEN round mode.
   *
   * @group math_funcs
   * @since 2.0.0
   */
  def bround(e: Column): Column = bround(e, 0)

  /**
   * Round the value of `e` to `scale` decimal places with HALF_EVEN round mode
   * if `scale` is greater than or equal to 0 or at integral part when `scale` is less than 0.
   *
   * @group math_funcs
   * @since 2.0.0
   */
  def bround(e: Column, scale: Int): Column = withExpr { BRound(e.expr, Literal(scale)) }

  /**
   * Shift the given value numBits left. If the given value is a long value, this function
   * will return a long value else it will return an integer value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def shiftLeft(e: Column, numBits: Int): Column = withExpr { ShiftLeft(e.expr, lit(numBits).expr) }

  /**
   * (Signed) shift the given value numBits right. If the given value is a long value, it will
   * return a long value else it will return an integer value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def shiftRight(e: Column, numBits: Int): Column = withExpr {
    ShiftRight(e.expr, lit(numBits).expr)
  }

  /**
   * Unsigned shift the given value numBits right. If the given value is a long value,
   * it will return a long value else it will return an integer value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def shiftRightUnsigned(e: Column, numBits: Int): Column = withExpr {
    ShiftRightUnsigned(e.expr, lit(numBits).expr)
  }

  /**
   * Computes the signum of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def signum(e: Column): Column = withExpr { Signum(e.expr) }

  /**
   * Computes the signum of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def signum(columnName: String): Column = signum(Column(columnName))

  /**
   * @param e angle in radians
   * @return sine of the angle, as if computed by `java.lang.Math.sin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sin(e: Column): Column = withExpr { Sin(e.expr) }

  /**
   * @param columnName angle in radians
   * @return sine of the angle, as if computed by `java.lang.Math.sin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sin(columnName: String): Column = sin(Column(columnName))

  /**
   * @param e hyperbolic angle
   * @return hyperbolic sine of the given value, as if computed by `java.lang.Math.sinh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sinh(e: Column): Column = withExpr { Sinh(e.expr) }

  /**
   * @param columnName hyperbolic angle
   * @return hyperbolic sine of the given value, as if computed by `java.lang.Math.sinh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sinh(columnName: String): Column = sinh(Column(columnName))

  /**
   * @param e angle in radians
   * @return tangent of the given value, as if computed by `java.lang.Math.tan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tan(e: Column): Column = withExpr { Tan(e.expr) }

  /**
   * @param columnName angle in radians
   * @return tangent of the given value, as if computed by `java.lang.Math.tan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tan(columnName: String): Column = tan(Column(columnName))

  /**
   * @param e hyperbolic angle
   * @return hyperbolic tangent of the given value, as if computed by `java.lang.Math.tanh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tanh(e: Column): Column = withExpr { Tanh(e.expr) }

  /**
   * @param columnName hyperbolic angle
   * @return hyperbolic tangent of the given value, as if computed by `java.lang.Math.tanh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tanh(columnName: String): Column = tanh(Column(columnName))

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use degrees", "2.1.0")
  def toDegrees(e: Column): Column = degrees(e)

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use degrees", "2.1.0")
  def toDegrees(columnName: String): Column = degrees(Column(columnName))

  /**
   * Converts an angle measured in radians to an approximately equivalent angle measured in degrees.
   *
   * @param e angle in radians
   * @return angle in degrees, as if computed by `java.lang.Math.toDegrees`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def degrees(e: Column): Column = withExpr { ToDegrees(e.expr) }

  /**
   * Converts an angle measured in radians to an approximately equivalent angle measured in degrees.
   *
   * @param columnName angle in radians
   * @return angle in degrees, as if computed by `java.lang.Math.toDegrees`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def degrees(columnName: String): Column = degrees(Column(columnName))

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use radians", "2.1.0")
  def toRadians(e: Column): Column = radians(e)

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use radians", "2.1.0")
  def toRadians(columnName: String): Column = radians(Column(columnName))

  /**
   * Converts an angle measured in degrees to an approximately equivalent angle measured in radians.
   *
   * @param e angle in degrees
   * @return angle in radians, as if computed by `java.lang.Math.toRadians`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def radians(e: Column): Column = withExpr { ToRadians(e.expr) }

  /**
   * Converts an angle measured in degrees to an approximately equivalent angle measured in radians.
   *
   * @param columnName angle in degrees
   * @return angle in radians, as if computed by `java.lang.Math.toRadians`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def radians(columnName: String): Column = radians(Column(columnName))

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Misc functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Calculates the MD5 digest of a binary column and returns the value
   * as a 32 character hex string.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def md5(e: Column): Column = withExpr { Md5(e.expr) }

  /**
   * Calculates the SHA-1 digest of a binary column and returns the value
   * as a 40 character hex string.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def sha1(e: Column): Column = withExpr { Sha1(e.expr) }

  /**
   * Calculates the SHA-2 family of hash functions of a binary column and
   * returns the value as a hex string.
   *
   * @param e column to compute SHA-2 on.
   * @param numBits one of 224, 256, 384, or 512.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def sha2(e: Column, numBits: Int): Column = {
    require(Seq(0, 224, 256, 384, 512).contains(numBits),
      s"numBits $numBits is not in the permitted values (0, 224, 256, 384, 512)")
    withExpr { Sha2(e.expr, lit(numBits).expr) }
  }

  /**
   * Calculates the cyclic redundancy check value  (CRC32) of a binary column and
   * returns the value as a bigint.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def crc32(e: Column): Column = withExpr { Crc32(e.expr) }

  /**
   * Calculates the hash code of given columns, and returns the result as an int column.
   *
   * @group misc_funcs
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def hash(cols: Column*): Column = withExpr {
    new Murmur3Hash(cols.map(_.expr))
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // String functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Computes the numeric value of the first character of the string column, and returns the
   * result as an int column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def ascii(e: Column): Column = withExpr { Ascii(e.expr) }

  /**
   * Computes the BASE64 encoding of a binary column and returns it as a string column.
   * This is the reverse of unbase64.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def base64(e: Column): Column = withExpr { Base64(e.expr) }

  /**
   * Concatenates multiple input string columns together into a single string column,
   * using the given separator.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def concat_ws(sep: String, exprs: Column*): Column = withExpr {
    ConcatWs(Literal.create(sep, StringType) +: exprs.map(_.expr))
  }

  /**
   * Computes the first argument into a string from a binary using the provided character set
   * (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').
   * If either argument is null, the result will also be null.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def decode(value: Column, charset: String): Column = withExpr {
    Decode(value.expr, lit(charset).expr)
  }

  /**
   * Computes the first argument into a binary from a string using the provided character set
   * (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').
   * If either argument is null, the result will also be null.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def encode(value: Column, charset: String): Column = withExpr {
    Encode(value.expr, lit(charset).expr)
  }

  /**
   * Formats numeric column x to a format like '#,###,###.##', rounded to d decimal places
   * with HALF_EVEN round mode, and returns the result as a string column.
   *
   * If d is 0, the result has no decimal point or fractional part.
   * If d is less than 0, the result will be null.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def format_number(x: Column, d: Int): Column = withExpr {
    FormatNumber(x.expr, lit(d).expr)
  }

  /**
   * Formats the arguments in printf-style and returns the result as a string column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def format_string(format: String, arguments: Column*): Column = withExpr {
    FormatString((lit(format) +: arguments).map(_.expr): _*)
  }

  /**
   * Returns a new string column by converting the first letter of each word to uppercase.
   * Words are delimited by whitespace.
   *
   * For example, "hello world" will become "Hello World".
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def initcap(e: Column): Column = withExpr { InitCap(e.expr) }

  /**
   * Locate the position of the first occurrence of substr column in the given string.
   * Returns null if either of the arguments are null.
   *
   * @note The position is not zero based, but 1 based index. Returns 0 if substr
   * could not be found in str.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def instr(str: Column, substring: String): Column = withExpr {
    StringInstr(str.expr, lit(substring).expr)
  }

  /**
   * Computes the character length of a given string or number of bytes of a binary string.
   * The length of character strings include the trailing spaces. The length of binary strings
   * includes binary zeros.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def length(e: Column): Column = withExpr { Length(e.expr) }

  /**
   * Converts a string column to lower case.
   *
   * @group string_funcs
   * @since 1.3.0
   */
  def lower(e: Column): Column = withExpr { Lower(e.expr) }

  /**
   * Computes the Levenshtein distance of the two given string columns.
   * @group string_funcs
   * @since 1.5.0
   */
  def levenshtein(l: Column, r: Column): Column = withExpr { Levenshtein(l.expr, r.expr) }

  /**
   * Locate the position of the first occurrence of substr.
   *
   * @note The position is not zero based, but 1 based index. Returns 0 if substr
   * could not be found in str.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def locate(substr: String, str: Column): Column = withExpr {
    new StringLocate(lit(substr).expr, str.expr)
  }

  /**
   * Locate the position of the first occurrence of substr in a string column, after position pos.
   *
   * @note The position is not zero based, but 1 based index. returns 0 if substr
   * could not be found in str.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def locate(substr: String, str: Column, pos: Int): Column = withExpr {
    StringLocate(lit(substr).expr, str.expr, lit(pos).expr)
  }

  /**
   * Left-pad the string column with pad to a length of len. If the string column is longer
   * than len, the return value is shortened to len characters.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def lpad(str: Column, len: Int, pad: String): Column = withExpr {
    StringLPad(str.expr, lit(len).expr, lit(pad).expr)
  }

  /**
   * Trim the spaces from left end for the specified string value.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def ltrim(e: Column): Column = withExpr {StringTrimLeft(e.expr) }

  /**
   * Trim the specified character string from left end for the specified string column.
   * @group string_funcs
   * @since 2.3.0
   */
  def ltrim(e: Column, trimString: String): Column = withExpr {
    StringTrimLeft(e.expr, Literal(trimString))
  }

  /**
   * Extract a specific group matched by a Java regex, from the specified string column.
   * If the regex did not match, or the specified group did not match, an empty string is returned.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def regexp_extract(e: Column, exp: String, groupIdx: Int): Column = withExpr {
    RegExpExtract(e.expr, lit(exp).expr, lit(groupIdx).expr)
  }

  /**
   * Replace all substrings of the specified string value that match regexp with rep.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def regexp_replace(e: Column, pattern: String, replacement: String): Column = withExpr {
    RegExpReplace(e.expr, lit(pattern).expr, lit(replacement).expr)
  }

  /**
   * Replace all substrings of the specified string value that match regexp with rep.
   *
   * @group string_funcs
   * @since 2.1.0
   */
  def regexp_replace(e: Column, pattern: Column, replacement: Column): Column = withExpr {
    RegExpReplace(e.expr, pattern.expr, replacement.expr)
  }

  /**
   * Decodes a BASE64 encoded string column and returns it as a binary column.
   * This is the reverse of base64.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def unbase64(e: Column): Column = withExpr { UnBase64(e.expr) }

  /**
   * Right-pad the string column with pad to a length of len. If the string column is longer
   * than len, the return value is shortened to len characters.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def rpad(str: Column, len: Int, pad: String): Column = withExpr {
    StringRPad(str.expr, lit(len).expr, lit(pad).expr)
  }

  /**
   * Repeats a string column n times, and returns it as a new string column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def repeat(str: Column, n: Int): Column = withExpr {
    StringRepeat(str.expr, lit(n).expr)
  }

  /**
   * Trim the spaces from right end for the specified string value.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def rtrim(e: Column): Column = withExpr { StringTrimRight(e.expr) }

  /**
   * Trim the specified character string from right end for the specified string column.
   * @group string_funcs
   * @since 2.3.0
   */
  def rtrim(e: Column, trimString: String): Column = withExpr {
    StringTrimRight(e.expr, Literal(trimString))
  }

  /**
   * Returns the soundex code for the specified expression.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def soundex(e: Column): Column = withExpr { SoundEx(e.expr) }

  /**
   * Splits str around pattern (pattern is a regular expression).
   *
   * @note Pattern is a string representation of the regular expression.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def split(str: Column, pattern: String): Column = withExpr {
    StringSplit(str.expr, lit(pattern).expr)
  }

  /**
   * Substring starts at `pos` and is of length `len` when str is String type or
   * returns the slice of byte array that starts at `pos` in byte and is of length `len`
   * when str is Binary type
   *
   * @note The position is not zero based, but 1 based index.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def substring(str: Column, pos: Int, len: Int): Column = withExpr {
    Substring(str.expr, lit(pos).expr, lit(len).expr)
  }

  /**
   * Returns the substring from string str before count occurrences of the delimiter delim.
   * If count is positive, everything the left of the final delimiter (counting from left) is
   * returned. If count is negative, every to the right of the final delimiter (counting from the
   * right) is returned. substring_index performs a case-sensitive match when searching for delim.
   *
   * @group string_funcs
   */
  def substring_index(str: Column, delim: String, count: Int): Column = withExpr {
    SubstringIndex(str.expr, lit(delim).expr, lit(count).expr)
  }

  /**
   * Translate any character in the src by a character in replaceString.
   * The characters in replaceString correspond to the characters in matchingString.
   * The translate will happen when any character in the string matches the character
   * in the `matchingString`.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def translate(src: Column, matchingString: String, replaceString: String): Column = withExpr {
    StringTranslate(src.expr, lit(matchingString).expr, lit(replaceString).expr)
  }

  /**
   * Trim the spaces from both ends for the specified string column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def trim(e: Column): Column = withExpr { StringTrim(e.expr) }

  /**
   * Trim the specified character from both ends for the specified string column.
   * @group string_funcs
   * @since 2.3.0
   */
  def trim(e: Column, trimString: String): Column = withExpr {
    StringTrim(e.expr, Literal(trimString))
  }

  /**
   * Converts a string column to upper case.
   *
   * @group string_funcs
   * @since 1.3.0
   */
  def upper(e: Column): Column = withExpr { Upper(e.expr) }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // DateTime functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Returns the date that is `numMonths` after `startDate`.
   *
   * @param startDate A date, timestamp or string. If a string, the data must be in a format that
   *                  can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param numMonths The number of months to add to `startDate`, can be negative to subtract months
   * @return A date, or null if `startDate` was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def add_months(startDate: Column, numMonths: Int): Column = withExpr {
    AddMonths(startDate.expr, Literal(numMonths))
  }

  /**
   * Returns the current date as a date column.
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def current_date(): Column = withExpr { CurrentDate() }

  /**
   * Returns the current timestamp as a timestamp column.
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def current_timestamp(): Column = withExpr { CurrentTimestamp() }

  /**
   * Converts a date/timestamp/string to a value of string in the format specified by the date
   * format given by the second argument.
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param dateExpr A date, timestamp or string. If a string, the data must be in a format that
   *                 can be cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param format A pattern `dd.MM.yyyy` would return a string like `18.03.1993`
   * @return A string, or null if `dateExpr` was a string that could not be cast to a timestamp
   * @note Use specialized functions like [[year]] whenever possible as they benefit from a
   * specialized implementation.
   * @throws IllegalArgumentException if the `format` pattern is invalid
   * @group datetime_funcs
   * @since 1.5.0
   */
  def date_format(dateExpr: Column, format: String): Column = withExpr {
    DateFormatClass(dateExpr.expr, Literal(format))
  }

  /**
   * Returns the date that is `days` days after `start`
   *
   * @param start A date, timestamp or string. If a string, the data must be in a format that
   *              can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param days  The number of days to add to `start`, can be negative to subtract days
   * @return A date, or null if `start` was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def date_add(start: Column, days: Int): Column = withExpr { DateAdd(start.expr, Literal(days)) }

  /**
   * Returns the date that is `days` days before `start`
   *
   * @param start A date, timestamp or string. If a string, the data must be in a format that
   *              can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param days  The number of days to subtract from `start`, can be negative to add days
   * @return A date, or null if `start` was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def date_sub(start: Column, days: Int): Column = withExpr { DateSub(start.expr, Literal(days)) }

  /**
   * Returns the number of days from `start` to `end`.
   *
   * Only considers the date part of the input. For example:
   * {{{
   * dateddiff("2018-01-10 00:00:00", "2018-01-09 23:59:59")
   * // returns 1
   * }}}
   *
   * @param end A date, timestamp or string. If a string, the data must be in a format that
   *            can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param start A date, timestamp or string. If a string, the data must be in a format that
   *              can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return An integer, or null if either `end` or `start` were strings that could not be cast to
   *         a date. Negative if `end` is before `start`
   * @group datetime_funcs
   * @since 1.5.0
   */
  def datediff(end: Column, start: Column): Column = withExpr { DateDiff(end.expr, start.expr) }

  /**
   * Extracts the year as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def year(e: Column): Column = withExpr { Year(e.expr) }

  /**
   * Extracts the quarter as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def quarter(e: Column): Column = withExpr { Quarter(e.expr) }

  /**
   * Extracts the month as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def month(e: Column): Column = withExpr { Month(e.expr) }

  /**
   * Extracts the day of the week as an integer from a given date/timestamp/string.
   * Ranges from 1 for a Sunday through to 7 for a Saturday
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 2.3.0
   */
  def dayofweek(e: Column): Column = withExpr { DayOfWeek(e.expr) }

  /**
   * Extracts the day of the month as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def dayofmonth(e: Column): Column = withExpr { DayOfMonth(e.expr) }

  /**
   * Extracts the day of the year as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def dayofyear(e: Column): Column = withExpr { DayOfYear(e.expr) }

  /**
   * Extracts the hours as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def hour(e: Column): Column = withExpr { Hour(e.expr) }

  /**
   * Returns the last day of the month which the given date belongs to.
   * For example, input "2015-07-27" returns "2015-07-31" since July 31 is the last day of the
   * month in July 2015.
   *
   * @param e A date, timestamp or string. If a string, the data must be in a format that can be
   *          cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A date, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def last_day(e: Column): Column = withExpr { LastDay(e.expr) }

  /**
   * Extracts the minutes as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def minute(e: Column): Column = withExpr { Minute(e.expr) }

  /**
   * Returns number of months between dates `start` and `end`.
   *
   * A whole number is returned if both inputs have the same day of month or both are the last day
   * of their respective months. Otherwise, the difference is calculated assuming 31 days per month.
   *
   * For example:
   * {{{
   * months_between("2017-11-14", "2017-07-14")  // returns 4.0
   * months_between("2017-01-01", "2017-01-10")  // returns 0.29032258
   * months_between("2017-06-01", "2017-06-16 12:00:00")  // returns -0.5
   * }}}
   *
   * @param end   A date, timestamp or string. If a string, the data must be in a format that can
   *              be cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param start A date, timestamp or string. If a string, the data must be in a format that can
   *              cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A double, or null if either `end` or `start` were strings that could not be cast to a
   *         timestamp. Negative if `end` is before `start`
   * @group datetime_funcs
   * @since 1.5.0
   */
  def months_between(end: Column, start: Column): Column = withExpr {
    new MonthsBetween(end.expr, start.expr)
  }

  /**
   * Returns number of months between dates `end` and `start`. If `roundOff` is set to true, the
   * result is rounded off to 8 digits; it is not rounded otherwise.
   * @group datetime_funcs
   * @since 2.4.0
   */
  def months_between(end: Column, start: Column, roundOff: Boolean): Column = withExpr {
    MonthsBetween(end.expr, start.expr, lit(roundOff).expr)
  }

  /**
   * Returns the first date which is later than the value of the `date` column that is on the
   * specified day of the week.
   *
   * For example, `next_day('2015-07-27', "Sunday")` returns 2015-08-02 because that is the first
   * Sunday after 2015-07-27.
   *
   * @param date      A date, timestamp or string. If a string, the data must be in a format that
   *                  can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param dayOfWeek Case insensitive, and accepts: "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"
   * @return A date, or null if `date` was a string that could not be cast to a date or if
   *         `dayOfWeek` was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def next_day(date: Column, dayOfWeek: String): Column = withExpr {
    NextDay(date.expr, lit(dayOfWeek).expr)
  }

  /**
   * Extracts the seconds as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a timestamp
   * @group datetime_funcs
   * @since 1.5.0
   */
  def second(e: Column): Column = withExpr { Second(e.expr) }

  /**
   * Extracts the week number as an integer from a given date/timestamp/string.
   *
   * A week is considered to start on a Monday and week 1 is the first week with more than 3 days,
   * as defined by ISO 8601
   *
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def weekofyear(e: Column): Column = withExpr { WeekOfYear(e.expr) }

  /**
   * Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string
   * representing the timestamp of that moment in the current system time zone in the
   * yyyy-MM-dd HH:mm:ss format.
   *
   * @param ut A number of a type that is castable to a long, such as string or integer. Can be
   *           negative for timestamps before the unix epoch
   * @return A string, or null if the input was a string that could not be cast to a long
   * @group datetime_funcs
   * @since 1.5.0
   */
  def from_unixtime(ut: Column): Column = withExpr {
    FromUnixTime(ut.expr, Literal("yyyy-MM-dd HH:mm:ss"))
  }

  /**
   * Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string
   * representing the timestamp of that moment in the current system time zone in the given
   * format.
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param ut A number of a type that is castable to a long, such as string or integer. Can be
   *           negative for timestamps before the unix epoch
   * @param f  A date time pattern that the input will be formatted to
   * @return A string, or null if `ut` was a string that could not be cast to a long or `f` was
   *         an invalid date time pattern
   * @group datetime_funcs
   * @since 1.5.0
   */
  def from_unixtime(ut: Column, f: String): Column = withExpr {
    FromUnixTime(ut.expr, Literal(f))
  }

  /**
   * Returns the current Unix timestamp (in seconds) as a long.
   *
   * @note All calls of `unix_timestamp` within the same query return the same value
   * (i.e. the current timestamp is calculated at the start of query evaluation).
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def unix_timestamp(): Column = withExpr {
    UnixTimestamp(CurrentTimestamp(), Literal("yyyy-MM-dd HH:mm:ss"))
  }

  /**
   * Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds),
   * using the default timezone and the default locale.
   *
   * @param s A date, timestamp or string. If a string, the data must be in the
   *          `yyyy-MM-dd HH:mm:ss` format
   * @return A long, or null if the input was a string not of the correct format
   * @group datetime_funcs
   * @since 1.5.0
   */
  def unix_timestamp(s: Column): Column = withExpr {
    UnixTimestamp(s.expr, Literal("yyyy-MM-dd HH:mm:ss"))
  }

  /**
   * Converts time string with given pattern to Unix timestamp (in seconds).
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param s A date, timestamp or string. If a string, the data must be in a format that can be
   *          cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param p A date time pattern detailing the format of `s` when `s` is a string
   * @return A long, or null if `s` was a string that could not be cast to a date or `p` was
   *         an invalid format
   * @group datetime_funcs
   * @since 1.5.0
   */
  def unix_timestamp(s: Column, p: String): Column = withExpr { UnixTimestamp(s.expr, Literal(p)) }

  /**
   * Converts to a timestamp by casting rules to `TimestampType`.
   *
   * @param s A date, timestamp or string. If a string, the data must be in a format that can be
   *          cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A timestamp, or null if the input was a string that could not be cast to a timestamp
   * @group datetime_funcs
   * @since 2.2.0
   */
  def to_timestamp(s: Column): Column = withExpr {
    new ParseToTimestamp(s.expr)
  }

  /**
   * Converts time string with the given pattern to timestamp.
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param s   A date, timestamp or string. If a string, the data must be in a format that can be
   *            cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss`
   * @param fmt A date time pattern detailing the format of `s` when `s` is a string
   * @return A timestamp, or null if `s` was a string that could not be cast to a timestamp or
   *         `fmt` was an invalid format
   * @group datetime_funcs
   * @since 2.2.0
   */
  def to_timestamp(s: Column, fmt: String): Column = withExpr {
    new ParseToTimestamp(s.expr, Literal(fmt))
  }

  /**
   * Converts the column into `DateType` by casting rules to `DateType`.
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def to_date(e: Column): Column = withExpr { new ParseToDate(e.expr) }

  /**
   * Converts the column into a `DateType` with a specified format
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param e   A date, timestamp or string. If a string, the data must be in a format that can be
   *            cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param fmt A date time pattern detailing the format of `e` when `e`is a string
   * @return A date, or null if `e` was a string that could not be cast to a date or `fmt` was an
   *         invalid format
   * @group datetime_funcs
   * @since 2.2.0
   */
  def to_date(e: Column, fmt: String): Column = withExpr {
    new ParseToDate(e.expr, Literal(fmt))
  }

  /**
   * Returns date truncated to the unit specified by the format.
   *
   * For example, `trunc("2018-11-19 12:01:19", "year")` returns 2018-01-01
   *
   * @param date A date, timestamp or string. If a string, the data must be in a format that can be
   *             cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param format: 'year', 'yyyy', 'yy' for truncate by year,
   *               or 'month', 'mon', 'mm' for truncate by month
   *
   * @return A date, or null if `date` was a string that could not be cast to a date or `format`
   *         was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def trunc(date: Column, format: String): Column = withExpr {
    TruncDate(date.expr, Literal(format))
  }

  /**
   * Returns timestamp truncated to the unit specified by the format.
   *
   * For example, `date_tunc("2018-11-19 12:01:19", "year")` returns 2018-01-01 00:00:00
   *
   * @param format: 'year', 'yyyy', 'yy' for truncate by year,
   *                'month', 'mon', 'mm' for truncate by month,
   *                'day', 'dd' for truncate by day,
   *                Other options are: 'second', 'minute', 'hour', 'week', 'month', 'quarter'
   * @param timestamp A date, timestamp or string. If a string, the data must be in a format that
   *                  can be cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A timestamp, or null if `timestamp` was a string that could not be cast to a timestamp
   *         or `format` was an invalid value
   * @group datetime_funcs
   * @since 2.3.0
   */
  def date_trunc(format: String, timestamp: Column): Column = withExpr {
    TruncTimestamp(Literal(format), timestamp.expr)
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders
   * that time as a timestamp in the given time zone. For example, 'GMT+1' would yield
   * '2017-07-14 03:40:00.0'.
   *
   * @param ts A date, timestamp or string. If a string, the data must be in a format that can be
   *           cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param tz A string detailing the time zone that the input should be adjusted to, such as
   *           `Europe/London`, `PST` or `GMT+5`
   * @return A timestamp, or null if `ts` was a string that could not be cast to a timestamp or
   *         `tz` was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def from_utc_timestamp(ts: Column, tz: String): Column = withExpr {
    FromUTCTimestamp(ts.expr, Literal(tz))
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders
   * that time as a timestamp in the given time zone. For example, 'GMT+1' would yield
   * '2017-07-14 03:40:00.0'.
   * @group datetime_funcs
   * @since 2.4.0
   */
  def from_utc_timestamp(ts: Column, tz: Column): Column = withExpr {
    FromUTCTimestamp(ts.expr, tz.expr)
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time
   * zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield
   * '2017-07-14 01:40:00.0'.
   *
   * @param ts A date, timestamp or string. If a string, the data must be in a format that can be
   *           cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param tz A string detailing the time zone that the input belongs to, such as `Europe/London`,
   *           `PST` or `GMT+5`
   * @return A timestamp, or null if `ts` was a string that could not be cast to a timestamp or
   *         `tz` was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def to_utc_timestamp(ts: Column, tz: String): Column = withExpr {
    ToUTCTimestamp(ts.expr, Literal(tz))
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time
   * zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield
   * '2017-07-14 01:40:00.0'.
   * @group datetime_funcs
   * @since 2.4.0
   */
  def to_utc_timestamp(ts: Column, tz: Column): Column = withExpr {
    ToUTCTimestamp(ts.expr, tz.expr)
  }

  /**
   * Bucketize rows into one or more time windows given a timestamp specifying column. Window
   * starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
   * [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
   * the order of months are not supported. The following example takes the average stock price for
   * a one minute window every 10 seconds starting 5 seconds after the hour:
   *
   * {{{
   *   val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType
   *   df.groupBy(window($"time", "1 minute", "10 seconds", "5 seconds"), $"stockId")
   *     .agg(mean("price"))
   * }}}
   *
   * The windows will look like:
   *
   * {{{
   *   09:00:05-09:01:05
   *   09:00:15-09:01:15
   *   09:00:25-09:01:25 ...
   * }}}
   *
   * For a streaming query, you may use the function `current_timestamp` to generate windows on
   * processing time.
   *
   * @param timeColumn The column or the expression to use as the timestamp for windowing by time.
   *                   The time column must be of TimestampType.
   * @param windowDuration A string specifying the width of the window, e.g. `10 minutes`,
   *                       `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for
   *                       valid duration identifiers. Note that the duration is a fixed length of
   *                       time, and does not vary over time according to a calendar. For example,
   *                       `1 day` always means 86,400,000 milliseconds, not a calendar day.
   * @param slideDuration A string specifying the sliding interval of the window, e.g. `1 minute`.
   *                      A new window will be generated every `slideDuration`. Must be less than
   *                      or equal to the `windowDuration`. Check
   *                      `org.apache.spark.unsafe.types.CalendarInterval` for valid duration
   *                      identifiers. This duration is likewise absolute, and does not vary
   *                      according to a calendar.
   * @param startTime The offset with respect to 1970-01-01 00:00:00 UTC with which to start
   *                  window intervals. For example, in order to have hourly tumbling windows that
   *                  start 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15... provide
   *                  `startTime` as `15 minutes`.
   *
   * @group datetime_funcs
   * @since 2.0.0
   */
  def window(
      timeColumn: Column,
      windowDuration: String,
      slideDuration: String,
      startTime: String): Column = {
    withExpr {
      TimeWindow(timeColumn.expr, windowDuration, slideDuration, startTime)
    }.as("window")
  }


  /**
   * Bucketize rows into one or more time windows given a timestamp specifying column. Window
   * starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
   * [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
   * the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC.
   * The following example takes the average stock price for a one minute window every 10 seconds:
   *
   * {{{
   *   val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType
   *   df.groupBy(window($"time", "1 minute", "10 seconds"), $"stockId")
   *     .agg(mean("price"))
   * }}}
   *
   * The windows will look like:
   *
   * {{{
   *   09:00:00-09:01:00
   *   09:00:10-09:01:10
   *   09:00:20-09:01:20 ...
   * }}}
   *
   * For a streaming query, you may use the function `current_timestamp` to generate windows on
   * processing time.
   *
   * @param timeColumn The column or the expression to use as the timestamp for windowing by time.
   *                   The time column must be of TimestampType.
   * @param windowDuration A string specifying the width of the window, e.g. `10 minutes`,
   *                       `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for
   *                       valid duration identifiers. Note that the duration is a fixed length of
   *                       time, and does not vary over time according to a calendar. For example,
   *                       `1 day` always means 86,400,000 milliseconds, not a calendar day.
   * @param slideDuration A string specifying the sliding interval of the window, e.g. `1 minute`.
   *                      A new window will be generated every `slideDuration`. Must be less than
   *                      or equal to the `windowDuration`. Check
   *                      `org.apache.spark.unsafe.types.CalendarInterval` for valid duration
   *                      identifiers. This duration is likewise absolute, and does not vary
   *                      according to a calendar.
   *
   * @group datetime_funcs
   * @since 2.0.0
   */
  def window(timeColumn: Column, windowDuration: String, slideDuration: String): Column = {
    window(timeColumn, windowDuration, slideDuration, "0 second")
  }

  /**
   * Generates tumbling time windows given a timestamp specifying column. Window
   * starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
   * [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
   * the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC.
   * The following example takes the average stock price for a one minute tumbling window:
   *
   * {{{
   *   val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType
   *   df.groupBy(window($"time", "1 minute"), $"stockId")
   *     .agg(mean("price"))
   * }}}
   *
   * The windows will look like:
   *
   * {{{
   *   09:00:00-09:01:00
   *   09:01:00-09:02:00
   *   09:02:00-09:03:00 ...
   * }}}
   *
   * For a streaming query, you may use the function `current_timestamp` to generate windows on
   * processing time.
   *
   * @param timeColumn The column or the expression to use as the timestamp for windowing by time.
   *                   The time column must be of TimestampType.
   * @param windowDuration A string specifying the width of the window, e.g. `10 minutes`,
   *                       `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for
   *                       valid duration identifiers.
   *
   * @group datetime_funcs
   * @since 2.0.0
   */
  def window(timeColumn: Column, windowDuration: String): Column = {
    window(timeColumn, windowDuration, windowDuration, "0 second")
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Collection functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Returns null if the array is null, true if the array contains `value`, and false otherwise.
   * @group collection_funcs
   * @since 1.5.0
   */
  def array_contains(column: Column, value: Any): Column = withExpr {
    ArrayContains(column.expr, lit(value).expr)
  }

  /**
   * Returns `true` if `a1` and `a2` have at least one non-null element in common. If not and both
   * the arrays are non-empty and any of them contains a `null`, it returns `null`. It returns
   * `false` otherwise.
   * @group collection_funcs
   * @since 2.4.0
   */
  def arrays_overlap(a1: Column, a2: Column): Column = withExpr {
    ArraysOverlap(a1.expr, a2.expr)
  }

  /**
   * Returns an array containing all the elements in `x` from index `start` (or starting from the
   * end if `start` is negative) with the specified `length`.
   *
   * @param x the array column to be sliced
   * @param start the starting index
   * @param length the length of the slice
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def slice(x: Column, start: Int, length: Int): Column = withExpr {
    Slice(x.expr, Literal(start), Literal(length))
  }

  /**
   * Concatenates the elements of `column` using the `delimiter`. Null values are replaced with
   * `nullReplacement`.
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_join(column: Column, delimiter: String, nullReplacement: String): Column = withExpr {
    ArrayJoin(column.expr, Literal(delimiter), Some(Literal(nullReplacement)))
  }

  /**
   * Concatenates the elements of `column` using the `delimiter`.
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_join(column: Column, delimiter: String): Column = withExpr {
    ArrayJoin(column.expr, Literal(delimiter), None)
  }

  /**
   * Concatenates multiple input columns together into a single column.
   * The function works with strings, binary and compatible array columns.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def concat(exprs: Column*): Column = withExpr { Concat(exprs.map(_.expr)) }

  /**
   * Locates the position of the first occurrence of the value in the given array as long.
   * Returns null if either of the arguments are null.
   *
   * @note The position is not zero based, but 1 based index. Returns 0 if value
   * could not be found in array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_position(column: Column, value: Any): Column = withExpr {
    ArrayPosition(column.expr, lit(value).expr)
  }

  /**
   * Returns element of array at given index in value if column is array. Returns value for
   * the given key in value if column is map.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def element_at(column: Column, value: Any): Column = withExpr {
    ElementAt(column.expr, lit(value).expr)
  }

  /**
   * Sorts the input array in ascending order. The elements of the input array must be orderable.
   * Null elements will be placed at the end of the returned array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_sort(e: Column): Column = withExpr { ArraySort(e.expr) }

  /**
   * Remove all elements that equal to element from the given array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_remove(column: Column, element: Any): Column = withExpr {
    ArrayRemove(column.expr, lit(element).expr)
  }

  /**
   * Removes duplicate values from the array.
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_distinct(e: Column): Column = withExpr { ArrayDistinct(e.expr) }

  /**
   * Returns an array of the elements in the intersection of the given two arrays,
   * without duplicates.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_intersect(col1: Column, col2: Column): Column = withExpr {
    ArrayIntersect(col1.expr, col2.expr)
  }

  /**
   * Returns an array of the elements in the union of the given two arrays, without duplicates.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_union(col1: Column, col2: Column): Column = withExpr {
    ArrayUnion(col1.expr, col2.expr)
  }

  /**
   * Returns an array of the elements in the first array but not in the second array,
   * without duplicates. The order of elements in the result is not determined
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_except(col1: Column, col2: Column): Column = withExpr {
    ArrayExcept(col1.expr, col2.expr)
  }

  /**
   * Creates a new row for each element in the given array or map column.
   *
   * @group collection_funcs
   * @since 1.3.0
   */
  def explode(e: Column): Column = withExpr { Explode(e.expr) }

  /**
   * Creates a new row for each element in the given array or map column.
   * Unlike explode, if the array/map is null or empty then null is produced.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def explode_outer(e: Column): Column = withExpr { GeneratorOuter(Explode(e.expr)) }

  /**
   * Creates a new row for each element with position in the given array or map column.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def posexplode(e: Column): Column = withExpr { PosExplode(e.expr) }

  /**
   * Creates a new row for each element with position in the given array or map column.
   * Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def posexplode_outer(e: Column): Column = withExpr { GeneratorOuter(PosExplode(e.expr)) }

  /**
   * Extracts json object from a json string based on json path specified, and returns json string
   * of the extracted json object. It will return null if the input json string is invalid.
   *
   * @group collection_funcs
   * @since 1.6.0
   */
  def get_json_object(e: Column, path: String): Column = withExpr {
    GetJsonObject(e.expr, lit(path).expr)
  }

  /**
   * Creates a new row for a json column according to the given field names.
   *
   * @group collection_funcs
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def json_tuple(json: Column, fields: String*): Column = withExpr {
    require(fields.nonEmpty, "at least 1 field name should be given.")
    JsonTuple(json.expr +: fields.map(Literal.apply))
  }

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `StructType` with the
   * specified schema. Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. Accepts the same options as the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: StructType, options: Map[String, String]): Column =
    from_json(e, schema.asInstanceOf[DataType], options)

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def from_json(e: Column, schema: DataType, options: Map[String, String]): Column = withExpr {
    JsonToStructs(schema, options, e.expr)
  }

  /**
   * (Java-specific) Parses a column containing a JSON string into a `StructType` with the
   * specified schema. Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: StructType, options: java.util.Map[String, String]): Column =
    from_json(e, schema, options.asScala.toMap)

  /**
   * (Java-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def from_json(e: Column, schema: DataType, options: java.util.Map[String, String]): Column =
    from_json(e, schema, options.asScala.toMap)

  /**
   * Parses a column containing a JSON string into a `StructType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: StructType): Column =
    from_json(e, schema, Map.empty[String, String])

  /**
   * Parses a column containing a JSON string into a `MapType` with `StringType` as keys type,
   * `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def from_json(e: Column, schema: DataType): Column =
    from_json(e, schema, Map.empty[String, String])

  /**
   * (Java-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string as a json string. In Spark 2.1,
   *               the user-provided schema has to be in JSON format. Since Spark 2.2, the DDL
   *               format is also supported for the schema.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: String, options: java.util.Map[String, String]): Column = {
    from_json(e, schema, options.asScala.toMap)
  }

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string as a json string, it could be a
   *               JSON format string or a DDL-formatted string.
   *
   * @group collection_funcs
   * @since 2.3.0
   */
  def from_json(e: Column, schema: String, options: Map[String, String]): Column = {
    val dataType = try {
      DataType.fromJson(schema)
    } catch {
      case NonFatal(_) => DataType.fromDDL(schema)
    }
    from_json(e, dataType, options)
  }

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` of `StructType`s with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def from_json(e: Column, schema: Column): Column = {
    from_json(e, schema, Map.empty[String, String].asJava)
  }

  /**
   * (Java-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` of `StructType`s with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def from_json(e: Column, schema: Column, options: java.util.Map[String, String]): Column = {
    withExpr(new JsonToStructs(e.expr, schema.expr, options.asScala.toMap))
  }

  /**
   * Parses a JSON string and infers its schema in DDL format.
   *
   * @param json a JSON string.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def schema_of_json(json: String): Column = schema_of_json(lit(json))

  /**
   * Parses a JSON string and infers its schema in DDL format.
   *
   * @param json a string literal containing a JSON string.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def schema_of_json(json: Column): Column = withExpr(new SchemaOfJson(json.expr))

  /**
   * (Scala-specific) Converts a column containing a `StructType`, `ArrayType` or
   * a `MapType` into a JSON string with the specified schema.
   * Throws an exception, in the case of an unsupported type.
   *
   * @param e a column containing a struct, an array or a map.
   * @param options options to control how the struct column is converted into a json string.
   *                accepts the same options and the json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def to_json(e: Column, options: Map[String, String]): Column = withExpr {
    StructsToJson(options, e.expr)
  }

  /**
   * (Java-specific) Converts a column containing a `StructType`, `ArrayType` or
   * a `MapType` into a JSON string with the specified schema.
   * Throws an exception, in the case of an unsupported type.
   *
   * @param e a column containing a struct, an array or a map.
   * @param options options to control how the struct column is converted into a json string.
   *                accepts the same options and the json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def to_json(e: Column, options: java.util.Map[String, String]): Column =
    to_json(e, options.asScala.toMap)

  /**
   * Converts a column containing a `StructType`, `ArrayType` or
   * a `MapType` into a JSON string with the specified schema.
   * Throws an exception, in the case of an unsupported type.
   *
   * @param e a column containing a struct, an array or a map.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def to_json(e: Column): Column =
    to_json(e, Map.empty[String, String])

  /**
   * Returns length of array or map.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  def size(e: Column): Column = withExpr { Size(e.expr) }

  /**
   * Sorts the input array for the given column in ascending order,
   * according to the natural ordering of the array elements.
   * Null elements will be placed at the beginning of the returned array.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  def sort_array(e: Column): Column = sort_array(e, asc = true)

  /**
   * Sorts the input array for the given column in ascending or descending order,
   * according to the natural ordering of the array elements.
   * Null elements will be placed at the beginning of the returned array in ascending order or
   * at the end of the returned array in descending order.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  def sort_array(e: Column, asc: Boolean): Column = withExpr { SortArray(e.expr, lit(asc).expr) }

  /**
   * Returns the minimum value in the array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_min(e: Column): Column = withExpr { ArrayMin(e.expr) }

  /**
   * Returns the maximum value in the array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_max(e: Column): Column = withExpr { ArrayMax(e.expr) }

  /**
   * Returns a random permutation of the given array.
   *
   * @note The function is non-deterministic.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def shuffle(e: Column): Column = withExpr { Shuffle(e.expr) }

  /**
   * Returns a reversed string or an array with reverse order of elements.
   * @group collection_funcs
   * @since 1.5.0
   */
  def reverse(e: Column): Column = withExpr { Reverse(e.expr) }

  /**
   * Creates a single array from an array of arrays. If a structure of nested arrays is deeper than
   * two levels, only one level of nesting is removed.
   * @group collection_funcs
   * @since 2.4.0
   */
  def flatten(e: Column): Column = withExpr { Flatten(e.expr) }

  /**
   * Generate a sequence of integers from start to stop, incrementing by step.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def sequence(start: Column, stop: Column, step: Column): Column = withExpr {
    new Sequence(start.expr, stop.expr, step.expr)
  }

  /**
   * Generate a sequence of integers from start to stop,
   * incrementing by 1 if start is less than or equal to stop, otherwise -1.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def sequence(start: Column, stop: Column): Column = withExpr {
    new Sequence(start.expr, stop.expr)
  }

  /**
   * Creates an array containing the left argument repeated the number of times given by the
   * right argument.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_repeat(left: Column, right: Column): Column = withExpr {
    ArrayRepeat(left.expr, right.expr)
  }

  /**
   * Creates an array containing the left argument repeated the number of times given by the
   * right argument.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_repeat(e: Column, count: Int): Column = array_repeat(e, lit(count))

  /**
   * Returns an unordered array containing the keys of the map.
   * @group collection_funcs
   * @since 2.3.0
   */
  def map_keys(e: Column): Column = withExpr { MapKeys(e.expr) }

  /**
   * Returns an unordered array containing the values of the map.
   * @group collection_funcs
   * @since 2.3.0
   */
  def map_values(e: Column): Column = withExpr { MapValues(e.expr) }

  /**
   * Returns a map created from the given array of entries.
   * @group collection_funcs
   * @since 2.4.0
   */
  def map_from_entries(e: Column): Column = withExpr { MapFromEntries(e.expr) }

  /**
   * Returns a merged array of structs in which the N-th struct contains all N-th values of input
   * arrays.
   * @group collection_funcs
   * @since 2.4.0
   */
  @scala.annotation.varargs
  def arrays_zip(e: Column*): Column = withExpr { ArraysZip(e.map(_.expr)) }

  /**
   * Returns the union of all the given maps.
   * @group collection_funcs
   * @since 2.4.0
   */
  @scala.annotation.varargs
  def map_concat(cols: Column*): Column = withExpr { MapConcat(cols.map(_.expr)) }

  // scalastyle:off line.size.limit
  // scalastyle:off parameter.number

  /* Use the following code to generate:

  (0 to 10).foreach { x =>
    val types = (1 to x).foldRight("RT")((i, s) => {s"A$i, $s"})
    val typeTags = (1 to x).map(i => s"A$i: TypeTag").foldLeft("RT: TypeTag")(_ + ", " + _)
    val inputSchemas = (1 to x).foldRight("Nil")((i, s) => {s"Try(ScalaReflection.schemaFor(typeTag[A$i])).toOption :: $s"})
    println(s"""
      |/**
      | * Defines a Scala closure of $x arguments as user-defined function (UDF).
      | * The data types are automatically inferred based on the Scala closure's
      | * signature. By default the returned UDF is deterministic. To change it to
      | * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
      | *
      | * @group udf_funcs
      | * @since 1.3.0
      | */
      |def udf[$typeTags](f: Function$x[$types]): UserDefinedFunction = {
      |  val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
      |  val inputSchemas = $inputSchemas
      |  val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
      |  if (nullable) udf else udf.asNonNullable()
      |}""".stripMargin)
  }

  (0 to 10).foreach { i =>
    val extTypeArgs = (0 to i).map(_ => "_").mkString(", ")
    val anyTypeArgs = (0 to i).map(_ => "Any").mkString(", ")
    val anyCast = s".asInstanceOf[UDF$i[$anyTypeArgs]]"
    val anyParams = (1 to i).map(_ => "_: Any").mkString(", ")
    val funcCall = if (i == 0) "() => func" else "func"
    println(s"""
      |/**
      | * Defines a Java UDF$i instance as user-defined function (UDF).
      | * The caller must specify the output data type, and there is no automatic input type coercion.
      | * By default the returned UDF is deterministic. To change it to nondeterministic, call the
      | * API `UserDefinedFunction.asNondeterministic()`.
      | *
      | * @group udf_funcs
      | * @since 2.3.0
      | */
      |def udf(f: UDF$i[$extTypeArgs], returnType: DataType): UserDefinedFunction = {
      |  val func = f$anyCast.call($anyParams)
      |  SparkUserDefinedFunction.create($funcCall, returnType, inputSchemas = Seq.fill($i)(None))
      |}""".stripMargin)
  }

  */

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Scala UDF functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Defines a Scala closure of 0 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag](f: Function0[RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 1 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag](f: Function1[A1, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 2 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag](f: Function2[A1, A2, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 3 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag](f: Function3[A1, A2, A3, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 4 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag](f: Function4[A1, A2, A3, A4, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 5 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag](f: Function5[A1, A2, A3, A4, A5, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 6 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag](f: Function6[A1, A2, A3, A4, A5, A6, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 7 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag](f: Function7[A1, A2, A3, A4, A5, A6, A7, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 8 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag, A8: TypeTag](f: Function8[A1, A2, A3, A4, A5, A6, A7, A8, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A8])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 9 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag, A8: TypeTag, A9: TypeTag](f: Function9[A1, A2, A3, A4, A5, A6, A7, A8, A9, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A8])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A9])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 10 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag, A8: TypeTag, A9: TypeTag, A10: TypeTag](f: Function10[A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A8])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A9])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A10])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Java UDF functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Defines a Java UDF0 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF0[_], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF0[Any]].call()
    SparkUserDefinedFunction.create(() => func, returnType, inputSchemas = Seq.fill(0)(None))
  }

  /**
   * Defines a Java UDF1 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF1[_, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF1[Any, Any]].call(_: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(1)(None))
  }

  /**
   * Defines a Java UDF2 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF2[_, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF2[Any, Any, Any]].call(_: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(2)(None))
  }

  /**
   * Defines a Java UDF3 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF3[_, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF3[Any, Any, Any, Any]].call(_: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(3)(None))
  }

  /**
   * Defines a Java UDF4 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF4[_, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF4[Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(4)(None))
  }

  /**
   * Defines a Java UDF5 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF5[_, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF5[Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(5)(None))
  }

  /**
   * Defines a Java UDF6 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF6[_, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF6[Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(6)(None))
  }

  /**
   * Defines a Java UDF7 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF7[_, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF7[Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(7)(None))
  }

  /**
   * Defines a Java UDF8 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF8[_, _, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF8[Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(8)(None))
  }

  /**
   * Defines a Java UDF9 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF9[_, _, _, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF9[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(9)(None))
  }

  /**
   * Defines a Java UDF10 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF10[_, _, _, _, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF10[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(10)(None))
  }

  // scalastyle:on parameter.number
  // scalastyle:on line.size.limit

  /**
   * Defines a deterministic user-defined function (UDF) using a Scala closure. For this variant,
   * the caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @param f  A closure in Scala
   * @param dataType  The output data type of the UDF
   *
   * @group udf_funcs
   * @since 2.0.0
   */
  def udf(f: AnyRef, dataType: DataType): UserDefinedFunction = {
    // TODO: should call SparkUserDefinedFunction.create() instead but inputSchemas is currently
    // unavailable. We may need to create type-safe overloaded versions of udf() methods.
    new UserDefinedFunction(f, dataType, inputTypes = None)
  }

  /**
   * Call an user-defined function.
   * Example:
   * {{{
   *  import org.apache.spark.sql._
   *
   *  val df = Seq(("id1", 1), ("id2", 4), ("id3", 5)).toDF("id", "value")
   *  val spark = df.sparkSession
   *  spark.udf.register("simpleUDF", (v: Int) => v * v)
   *  df.select($"id", callUDF("simpleUDF", $"value"))
   * }}}
   *
   * @group udf_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def callUDF(udfName: String, cols: Column*): Column = withExpr {
    UnresolvedFunction(udfName, cols.map(_.expr), isDistinct = false)
  }
}

[0m2021.02.26 14:44:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:44:02 INFO  time: compiled root in 0.73s[0m
[0m2021.02.26 14:45:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:45:40 INFO  time: compiled root in 0.94s[0m
[0m2021.02.26 14:48:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:48:31 INFO  time: compiled root in 0.58s[0m
[0m2021.02.26 14:48:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:48:47 INFO  time: compiled root in 0.58s[0m
[0m2021.02.26 14:51:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:51:14 INFO  time: compiled root in 0.62s[0m
[0m2021.02.26 14:56:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:56:36 INFO  time: compiled root in 0.7s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.Map
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder

/**
 * A collection of implicit methods for converting common Scala objects into [[Dataset]]s.
 *
 * @since 1.6.0
 */
@InterfaceStability.Evolving
abstract class SQLImplicits extends LowPrioritySQLImplicits {

  protected def _sqlContext: SQLContext

  /**
   * Converts $"col name" into a [[Column]].
   *
   * @since 2.0.0
   */
  implicit class StringToColumn(val sc: StringContext) {
    def $(args: Any*): ColumnName = {
      new ColumnName(sc.s(args: _*))
    }
  }

  // Primitives

  /** @since 1.6.0 */
  implicit def newIntEncoder: Encoder[Int] = Encoders.scalaInt

  /** @since 1.6.0 */
  implicit def newLongEncoder: Encoder[Long] = Encoders.scalaLong

  /** @since 1.6.0 */
  implicit def newDoubleEncoder: Encoder[Double] = Encoders.scalaDouble

  /** @since 1.6.0 */
  implicit def newFloatEncoder: Encoder[Float] = Encoders.scalaFloat

  /** @since 1.6.0 */
  implicit def newByteEncoder: Encoder[Byte] = Encoders.scalaByte

  /** @since 1.6.0 */
  implicit def newShortEncoder: Encoder[Short] = Encoders.scalaShort

  /** @since 1.6.0 */
  implicit def newBooleanEncoder: Encoder[Boolean] = Encoders.scalaBoolean

  /** @since 1.6.0 */
  implicit def newStringEncoder: Encoder[String] = Encoders.STRING

  /** @since 2.2.0 */
  implicit def newJavaDecimalEncoder: Encoder[java.math.BigDecimal] = Encoders.DECIMAL

  /** @since 2.2.0 */
  implicit def newScalaDecimalEncoder: Encoder[scala.math.BigDecimal] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newDateEncoder: Encoder[java.sql.Date] = Encoders.DATE

  /** @since 2.2.0 */
  implicit def newTimeStampEncoder: Encoder[java.sql.Timestamp] = Encoders.TIMESTAMP


  // Boxed primitives

  /** @since 2.0.0 */
  implicit def newBoxedIntEncoder: Encoder[java.lang.Integer] = Encoders.INT

  /** @since 2.0.0 */
  implicit def newBoxedLongEncoder: Encoder[java.lang.Long] = Encoders.LONG

  /** @since 2.0.0 */
  implicit def newBoxedDoubleEncoder: Encoder[java.lang.Double] = Encoders.DOUBLE

  /** @since 2.0.0 */
  implicit def newBoxedFloatEncoder: Encoder[java.lang.Float] = Encoders.FLOAT

  /** @since 2.0.0 */
  implicit def newBoxedByteEncoder: Encoder[java.lang.Byte] = Encoders.BYTE

  /** @since 2.0.0 */
  implicit def newBoxedShortEncoder: Encoder[java.lang.Short] = Encoders.SHORT

  /** @since 2.0.0 */
  implicit def newBoxedBooleanEncoder: Encoder[java.lang.Boolean] = Encoders.BOOLEAN

  // Seqs

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newIntSeqEncoder: Encoder[Seq[Int]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newLongSeqEncoder: Encoder[Seq[Long]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newDoubleSeqEncoder: Encoder[Seq[Double]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newFloatSeqEncoder: Encoder[Seq[Float]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newByteSeqEncoder: Encoder[Seq[Byte]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newShortSeqEncoder: Encoder[Seq[Short]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newBooleanSeqEncoder: Encoder[Seq[Boolean]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newStringSeqEncoder: Encoder[Seq[String]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newProductSeqEncoder[A <: Product : TypeTag]: Encoder[Seq[A]] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newSequenceEncoder[T <: Seq[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Maps
  /** @since 2.3.0 */
  implicit def newMapEncoder[T <: Map[_, _] : TypeTag]: Encoder[T] = ExpressionEncoder()

  /**
   * Notice that we serialize `Set` to Catalyst array. The set property is only kept when
   * manipulating the domain objects. The serialization format doesn't keep the set property.
   * When we have a Catalyst array which contains duplicated elements and convert it to
   * `Dataset[Set[T]]` by using the encoder, the elements will be de-duplicated.
   *
   * @since 2.3.0
   */
  implicit def newSetEncoder[T <: Set[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Arrays

  /** @since 1.6.1 */
  implicit def newIntArrayEncoder: Encoder[Array[Int]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newLongArrayEncoder: Encoder[Array[Long]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newDoubleArrayEncoder: Encoder[Array[Double]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newFloatArrayEncoder: Encoder[Array[Float]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newByteArrayEncoder: Encoder[Array[Byte]] = Encoders.BINARY

  /** @since 1.6.1 */
  implicit def newShortArrayEncoder: Encoder[Array[Short]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newBooleanArrayEncoder: Encoder[Array[Boolean]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newStringArrayEncoder: Encoder[Array[String]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newProductArrayEncoder[A <: Product : TypeTag]: Encoder[Array[A]] =
    ExpressionEncoder()

  /**
   * Creates a [[Dataset]] from an RDD.
   *
   * @since 1.6.0
   */
  implicit def rddToDatasetHolder[T : Encoder](rdd: RDD[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(rdd))
  }

  /**
   * Creates a [[Dataset]] from a local Seq.
   * @since 1.6.0
   */
  implicit def localSeqToDatasetHolder[T : Encoder](s: Seq[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(s))
  }

  /**
   * An implicit conversion that turns a Scala `Symbol` into a [[Column]].
   * @since 1.3.0
   */
  implicit def symbolToColumn(s: Symbol): ColumnName = new ColumnName(s.name)

}

/**
 * Lower priority implicit methods for converting Scala objects into [[Dataset]]s.
 * Conflicting implicits are placed here to disambiguate resolution.
 *
 * Reasons for including specific implicits:
 * newProductEncoder - to disambiguate for `List`s which are both `Seq` and `Product`
 */
trait LowPrioritySQLImplicits {
  /** @since 1.6.0 */
  implicit def newProductEncoder[T <: Product : TypeTag]: Encoder[T] = Encoders.product[T]

}

[0m2021.02.26 14:56:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:56:52 INFO  time: compiled root in 0.17s[0m
[0m2021.02.26 14:57:18 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:65:15: stale bloop error: value contains is not a member of org.apache.spark.sql.Row
      .filter(_.contains("tech"))
              ^^^^^^^^^^[0m
[0m2021.02.26 14:57:18 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:65:15: stale bloop error: value contains is not a member of org.apache.spark.sql.Row
      .filter(_.contains("tech"))
              ^^^^^^^^^^[0m
[0m2021.02.26 14:57:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:57:23 INFO  time: compiled root in 0.63s[0m
[0m2021.02.26 14:57:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:57:30 INFO  time: compiled root in 0.65s[0m
[0m2021.02.26 14:58:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:58:15 INFO  time: compiled root in 0.73s[0m
[0m2021.02.26 14:58:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:58:18 INFO  time: compiled root in 0.69s[0m
[0m2021.02.26 15:00:08 INFO  compiling root (1 scala source)[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.encoders.{encoderFor, ExpressionEncoder}
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
import org.apache.spark.sql.catalyst.util.toPrettySQL
import org.apache.spark.sql.execution.aggregate.TypedAggregateExpression
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.types._

private[sql] object Column {

  def apply(colName: String): Column = new Column(colName)

  def apply(expr: Expression): Column = new Column(expr)

  def unapply(col: Column): Option[Expression] = Some(col.expr)

  private[sql] def generateAlias(e: Expression): String = {
    e match {
      case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
        a.aggregateFunction.toString
      case expr => toPrettySQL(expr)
    }
  }
}

/**
 * A [[Column]] where an [[Encoder]] has been given for the expected input and return type.
 * To create a [[TypedColumn]], use the `as` function on a [[Column]].
 *
 * @tparam T The input type expected for this expression.  Can be `Any` if the expression is type
 *           checked by the analyzer instead of the compiler (i.e. `expr("sum(...)")`).
 * @tparam U The output type of this column.
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class TypedColumn[-T, U](
    expr: Expression,
    private[sql] val encoder: ExpressionEncoder[U])
  extends Column(expr) {

  /**
   * Inserts the specific input type and schema into any expressions that are expected to operate
   * on a decoded object.
   */
  private[sql] def withInputType(
      inputEncoder: ExpressionEncoder[_],
      inputAttributes: Seq[Attribute]): TypedColumn[T, U] = {
    val unresolvedDeserializer = UnresolvedDeserializer(inputEncoder.deserializer, inputAttributes)
    val newExpr = expr transform {
      case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty =>
        ta.withInputInfo(
          deser = unresolvedDeserializer,
          cls = inputEncoder.clsTag.runtimeClass,
          schema = inputEncoder.schema)
    }
    new TypedColumn[T, U](newExpr, encoder)
  }

  /**
   * Gives the [[TypedColumn]] a name (alias).
   * If the current `TypedColumn` has metadata associated with it, this metadata will be propagated
   * to the new column.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  override def name(alias: String): TypedColumn[T, U] =
    new TypedColumn[T, U](super.name(alias).expr, encoder)

}

/**
 * A column that will be computed based on the data in a `DataFrame`.
 *
 * A new column can be constructed based on the input columns present in a DataFrame:
 *
 * {{{
 *   df("columnName")            // On a specific `df` DataFrame.
 *   col("columnName")           // A generic column not yet associated with a DataFrame.
 *   col("columnName.field")     // Extracting a struct field
 *   col("`a.column.with.dots`") // Escape `.` in column names.
 *   $"columnName"               // Scala short hand for a named column.
 * }}}
 *
 * [[Column]] objects can be composed to form complex expressions:
 *
 * {{{
 *   $"a" + 1
 *   $"a" === $"b"
 * }}}
 *
 * @note The internal Catalyst expression can be accessed via [[expr]], but this method is for
 * debugging purposes only and can change in any future Spark releases.
 *
 * @groupname java_expr_ops Java-specific expression operators
 * @groupname expr_ops Expression operators
 * @groupname df_ops DataFrame functions
 * @groupname Ungrouped Support functions for DataFrames
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class Column(val expr: Expression) extends Logging {

  def this(name: String) = this(name match {
    case "*" => UnresolvedStar(None)
    case _ if name.endsWith(".*") =>
      val parts = UnresolvedAttribute.parseAttributeName(name.substring(0, name.length - 2))
      UnresolvedStar(Some(parts))
    case _ => UnresolvedAttribute.quotedString(name)
  })

  override def toString: String = toPrettySQL(expr)

  override def equals(that: Any): Boolean = that match {
    case that: Column => that.expr.equals(this.expr)
    case _ => false
  }

  override def hashCode: Int = this.expr.hashCode()

  /** Creates a column based on the given expression. */
  private def withExpr(newExpr: Expression): Column = new Column(newExpr)

  /**
   * Returns the expression for this column either with an existing or auto assigned name.
   */
  private[sql] def named: NamedExpression = expr match {
    // Wrap UnresolvedAttribute with UnresolvedAlias, as when we resolve UnresolvedAttribute, we
    // will remove intermediate Alias for ExtractValue chain, and we need to alias it again to
    // make it a NamedExpression.
    case u: UnresolvedAttribute => UnresolvedAlias(u)

    case u: UnresolvedExtractValue => UnresolvedAlias(u)

    case expr: NamedExpression => expr

    // Leave an unaliased generator with an empty list of names since the analyzer will generate
    // the correct defaults after the nested expression's type has been resolved.
    case g: Generator => MultiAlias(g, Nil)

    case func: UnresolvedFunction => UnresolvedAlias(func, Some(Column.generateAlias))

    // If we have a top level Cast, there is a chance to give it a better alias, if there is a
    // NamedExpression under this Cast.
    case c: Cast =>
      c.transformUp {
        case c @ Cast(_: NamedExpression, _, _) => UnresolvedAlias(c)
      } match {
        case ne: NamedExpression => ne
        case _ => Alias(expr, toPrettySQL(expr))()
      }

    case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
      UnresolvedAlias(a, Some(Column.generateAlias))

    // Wait until the struct is resolved. This will generate a nicer looking alias.
    case struct: CreateNamedStructLike => UnresolvedAlias(struct)

    case expr: Expression => Alias(expr, toPrettySQL(expr))()
  }

  /**
   * Provides a type hint about the expected return value of this column.  This information can
   * be used by operations such as `select` on a [[Dataset]] to automatically convert the
   * results into the correct JVM types.
   * @since 1.6.0
   */
  def as[U : Encoder]: TypedColumn[Any, U] = new TypedColumn[Any, U](expr, encoderFor[U])

  /**
   * Extracts a value or values from a complex type.
   * The following types of extraction are supported:
   * <ul>
   * <li>Given an Array, an integer ordinal can be used to retrieve a single value.</li>
   * <li>Given a Map, a key of the correct type can be used to retrieve an individual value.</li>
   * <li>Given a Struct, a string fieldName can be used to extract that field.</li>
   * <li>Given an Array of Structs, a string fieldName can be used to extract filed
   *    of every struct in that array, and return an Array of fields.</li>
   * </ul>
   * @group expr_ops
   * @since 1.4.0
   */
  def apply(extraction: Any): Column = withExpr {
    UnresolvedExtractValue(expr, lit(extraction).expr)
  }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Scala: select the amount column and negates all values.
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.select( negate(col("amount") );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_- : Column = withExpr { UnaryMinus(expr) }

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_! : Column = withExpr { Not(expr) }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def === (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} = $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualTo(expr, right)
  }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def equalTo(other: Any): Column = this === other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") =!= df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
    */
  def =!= (other: Any): Column = withExpr{ Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
    */
  @deprecated("!== does not have the same precedence as ===, use =!= instead", "2.0.0")
  def !== (other: Any): Column = this =!= other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def notEqual(other: Any): Column = withExpr { Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > 21 )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def > (other: Any): Column = withExpr { GreaterThan(expr, lit(other).expr) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > lit(21) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def gt(other: Any): Column = this > other

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def < (other: Any): Column = withExpr { LessThan(expr, lit(other).expr) }

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def lt(other: Any): Column = this < other

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <= (other: Any): Column = withExpr { LessThanOrEqual(expr, lit(other).expr) }

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def leq(other: Any): Column = this <= other

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def >= (other: Any): Column = withExpr { GreaterThanOrEqual(expr, lit(other).expr) }

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def geq(other: Any): Column = this >= other

  /**
   * Equality test that is safe for null values.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <=> (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} <=> $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualNullSafe(expr, right)
  }

  /**
   * Equality test that is safe for null values.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def eqNullSafe(other: Any): Column = this <=> other

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches :+ ((condition.expr, lit(value).expr))) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "when() cannot be applied once otherwise() is applied")
    case _ =>
      throw new IllegalArgumentException(
        "when() can only be applied on a Column previously generated by when() function")
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def otherwise(value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches, Option(lit(value).expr)) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied once on a Column previously generated by when()")
    case _ =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied on a Column previously generated by when()")
  }

  /**
   * True if the current column is between the lower bound and upper bound, inclusive.
   *
   * @group java_expr_ops
   * @since 1.4.0
   */
  def between(lowerBound: Any, upperBound: Any): Column = {
    (this >= lowerBound) && (this <= upperBound)
  }

  /**
   * True if the current expression is NaN.
   *
   * @group expr_ops
   * @since 1.5.0
   */
  def isNaN: Column = withExpr { IsNaN(expr) }

  /**
   * True if the current expression is null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNull: Column = withExpr { IsNull(expr) }

  /**
   * True if the current expression is NOT null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNotNull: Column = withExpr { IsNotNull(expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def || (other: Any): Column = withExpr { Or(expr, lit(other).expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def or(other: Column): Column = this || other

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def && (other: Any): Column = withExpr { And(expr, lit(other).expr) }

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def and(other: Column): Column = this && other

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def + (other: Any): Column = withExpr { Add(expr, lit(other).expr) }

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def plus(other: Any): Column = this + other

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def - (other: Any): Column = withExpr { Subtract(expr, lit(other).expr) }

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def minus(other: Any): Column = this - other

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def * (other: Any): Column = withExpr { Multiply(expr, lit(other).expr) }

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def multiply(other: Any): Column = this * other

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def / (other: Any): Column = withExpr { Divide(expr, lit(other).expr) }

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def divide(other: Any): Column = this / other

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def % (other: Any): Column = withExpr { Remainder(expr, lit(other).expr) }

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def mod(other: Any): Column = this % other

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the evaluated values of the arguments.
   *
   * Note: Since the type of the elements in the list are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def isin(list: Any*): Column = withExpr { In(expr, list.map(lit(_).expr)) }

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: scala.collection.Iterable[_]): Column = isin(values.toSeq: _*)

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group java_expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: java.lang.Iterable[_]): Column = isInCollection(values.asScala)

  /**
   * SQL like expression. Returns a boolean column based on a SQL LIKE match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def like(literal: String): Column = withExpr { Like(expr, lit(literal).expr) }

  /**
   * SQL RLIKE expression (LIKE with Regex). Returns a boolean column based on a regex
   * match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def rlike(literal: String): Column = withExpr { RLike(expr, lit(literal).expr) }

  /**
   * An expression that gets an item at position `ordinal` out of an array,
   * or gets a value by key `key` in a `MapType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getItem(key: Any): Column = withExpr { UnresolvedExtractValue(expr, Literal(key)) }

  /**
   * An expression that gets a field by name in a `StructType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getField(fieldName: String): Column = withExpr {
    UnresolvedExtractValue(expr, Literal(fieldName))
  }

  /**
   * An expression that returns a substring.
   * @param startPos expression for the starting position.
   * @param len expression for the length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Column, len: Column): Column = withExpr {
    Substring(expr, startPos.expr, len.expr)
  }

  /**
   * An expression that returns a substring.
   * @param startPos starting position.
   * @param len length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Int, len: Int): Column = withExpr {
    Substring(expr, lit(startPos).expr, lit(len).expr)
  }

  /**
   * Contains the other element. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def contains(other: Any): Column = withExpr { Contains(expr, lit(other).expr) }

  /**
   * String starts with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(other: Column): Column = withExpr { StartsWith(expr, lit(other).expr) }

  /**
   * String starts with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(literal: String): Column = this.startsWith(lit(literal))

  /**
   * String ends with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(other: Column): Column = withExpr { EndsWith(expr, lit(other).expr) }

  /**
   * String ends with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(literal: String): Column = this.endsWith(lit(literal))

  /**
   * Gives the column an alias. Same as `as`.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".alias("colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def alias(alias: String): Column = name(alias)

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String): Column = name(alias)

  /**
   * (Scala-specific) Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Seq[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Array[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as('colB))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: Symbol): Column = name(alias.name)

  /**
   * Gives the column an alias with metadata.
   * {{{
   *   val metadata: Metadata = ...
   *   df.select($"colA".as("colB", metadata))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String, metadata: Metadata): Column = withExpr {
    Alias(expr, alias)(explicitMetadata = Some(metadata))
  }

  /**
   * Gives the column a name (alias).
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".name("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def name(alias: String): Column = withExpr {
    expr match {
      case ne: NamedExpression => Alias(expr, alias)(explicitMetadata = Some(ne.metadata))
      case other => Alias(other, alias)()
    }
  }

  /**
   * Casts the column to a different data type.
   * {{{
   *   // Casts colA to IntegerType.
   *   import org.apache.spark.sql.types.IntegerType
   *   df.select(df("colA").cast(IntegerType))
   *
   *   // equivalent to
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: DataType): Column = withExpr { Cast(expr, to) }

  /**
   * Casts the column to a different data type, using the canonical string representation
   * of the type. The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
   * `float`, `double`, `decimal`, `date`, `timestamp`.
   * {{{
   *   // Casts colA to integer.
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: String): Column = cast(CatalystSqlParser.parseDataType(to))

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   // Scala
   *   df.sort(df("age").desc)
   *
   *   // Java
   *   df.sort(df.col("age").desc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def desc: Column = withExpr { SortOrder(expr, Descending) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing first.
   *   df.sort(df("age").desc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_first: Column = withExpr { SortOrder(expr, Descending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing last.
   *   df.sort(df("age").desc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_last: Column = withExpr { SortOrder(expr, Descending, NullsLast, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order.
   *   df.sort(df("age").asc)
   *
   *   // Java
   *   df.sort(df.col("age").asc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def asc: Column = withExpr { SortOrder(expr, Ascending) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing first.
   *   df.sort(df("age").asc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_first: Column = withExpr { SortOrder(expr, Ascending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing last.
   *   df.sort(df("age").asc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_last: Column = withExpr { SortOrder(expr, Ascending, NullsLast, Set.empty) }

  /**
   * Prints the expression to the console for debugging purposes.
   *
   * @group df_ops
   * @since 1.3.0
   */
  def explain(extended: Boolean): Unit = {
    // scalastyle:off println
    if (extended) {
      println(expr)
    } else {
      println(expr.sql)
    }
    // scalastyle:on println
  }

  /**
   * Compute bitwise OR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseOR(other: Any): Column = withExpr { BitwiseOr(expr, lit(other).expr) }

  /**
   * Compute bitwise AND of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseAND($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseAND(other: Any): Column = withExpr { BitwiseAnd(expr, lit(other).expr) }

  /**
   * Compute bitwise XOR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseXOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseXOR(other: Any): Column = withExpr { BitwiseXor(expr, lit(other).expr) }

  /**
   * Defines a windowing column.
   *
   * {{{
   *   val w = Window.partitionBy("name").orderBy("id")
   *   df.select(
   *     sum("price").over(w.rangeBetween(Window.unboundedPreceding, 2)),
   *     avg("price").over(w.rowsBetween(Window.currentRow, 4))
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def over(window: expressions.WindowSpec): Column = window.withAggregate(this)

  /**
   * Defines an empty analytic clause. In this case the analytic function is applied
   * and presented for all rows in the result set.
   *
   * {{{
   *   df.select(
   *     sum("price").over(),
   *     avg("price").over()
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def over(): Column = over(Window.spec)

}


/**
 * A convenient class used for constructing schema.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class ColumnName(name: String) extends Column(name) {

  /**
   * Creates a new `StructField` of type boolean.
   * @since 1.3.0
   */
  def boolean: StructField = StructField(name, BooleanType)

  /**
   * Creates a new `StructField` of type byte.
   * @since 1.3.0
   */
  def byte: StructField = StructField(name, ByteType)

  /**
   * Creates a new `StructField` of type short.
   * @since 1.3.0
   */
  def short: StructField = StructField(name, ShortType)

  /**
   * Creates a new `StructField` of type int.
   * @since 1.3.0
   */
  def int: StructField = StructField(name, IntegerType)

  /**
   * Creates a new `StructField` of type long.
   * @since 1.3.0
   */
  def long: StructField = StructField(name, LongType)

  /**
   * Creates a new `StructField` of type float.
   * @since 1.3.0
   */
  def float: StructField = StructField(name, FloatType)

  /**
   * Creates a new `StructField` of type double.
   * @since 1.3.0
   */
  def double: StructField = StructField(name, DoubleType)

  /**
   * Creates a new `StructField` of type string.
   * @since 1.3.0
   */
  def string: StructField = StructField(name, StringType)

  /**
   * Creates a new `StructField` of type date.
   * @since 1.3.0
   */
  def date: StructField = StructField(name, DateType)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal: StructField = StructField(name, DecimalType.USER_DEFAULT)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal(precision: Int, scale: Int): StructField =
    StructField(name, DecimalType(precision, scale))

  /**
   * Creates a new `StructField` of type timestamp.
   * @since 1.3.0
   */
  def timestamp: StructField = StructField(name, TimestampType)

  /**
   * Creates a new `StructField` of type binary.
   * @since 1.3.0
   */
  def binary: StructField = StructField(name, BinaryType)

  /**
   * Creates a new `StructField` of type array.
   * @since 1.3.0
   */
  def array(dataType: DataType): StructField = StructField(name, ArrayType(dataType))

  /**
   * Creates a new `StructField` of type map.
   * @since 1.3.0
   */
  def map(keyType: DataType, valueType: DataType): StructField =
    map(MapType(keyType, valueType))

  def map(mapType: MapType): StructField = StructField(name, mapType)

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(fields: StructField*): StructField = struct(StructType(fields))

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(structType: StructType): StructField = StructField(name, structType)
}

[0m2021.02.26 15:00:08 INFO  time: compiled root in 0.17s[0m
[0m2021.02.26 15:00:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:00:23 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 15:00:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:00:34 INFO  time: compiled root in 0.69s[0m
[0m2021.02.26 15:01:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:01:18 INFO  time: compiled root in 0.67s[0m
[0m2021.02.26 15:02:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:02:00 INFO  time: compiled root in 0.66s[0m
[0m2021.02.26 15:04:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:04:32 INFO  time: compiled root in 0.68s[0m
[0m2021.02.26 15:07:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:07:07 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 15:08:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:08:39 INFO  time: compiled root in 0.67s[0m
[0m2021.02.26 15:10:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:10:47 INFO  time: compiled root in 0.64s[0m
[0m2021.02.26 15:12:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:12:34 INFO  time: compiled root in 0.69s[0m
[0m2021.02.26 15:12:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:12:45 INFO  time: compiled root in 0.71s[0m
[0m2021.02.26 15:14:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:14:51 INFO  time: compiled root in 0.65s[0m
[0m2021.02.26 15:16:48 INFO  shutting down Metals[0m
[0m2021.02.26 15:16:48 INFO  Shut down connection with build server.[0m
[0m2021.02.26 15:16:48 INFO  Shut down connection with build server.[0m
[0m2021.02.26 15:16:48 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.01 14:19:47 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.53.2.[0m
[0m2021.03.01 14:19:48 INFO  time: initialize in 0.4s[0m
[0m2021.03.01 14:19:47 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3279832539601474146/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.01 14:19:48 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
[0m2021.03.01 14:19:48 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.01 14:19:50 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    //filter urls that have job(s) in the url, print the host name and url path
    // val df = spark.read.load("s3a://commoncrawl/cc-index/table/cc-main/warc/")

    // df
    //   .select("url_host_name", "url_path")
    //   .filter($"crawl" === "CC-MAIN-2020-16")
    //   .filter($"subset" === "warc")
    //   .filter($"url_path".contains("job"))
    //   .show(200, false)

    //filter text to show jobs that require "year experience"
    // val commonCrawl = spark.read.textFile(
    //   "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    // )
    // commonCrawl
    //   .filter(_.contains("year experience"))
    //   .show(false)

    /** My take/method:
      */

    val rddFromFile = spark.read
      .json(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz"
      )

    //"Container", "Envelope", "_corrupt_record"
    // rddFromFile.show(false)
    // rddFromFile.printSchema()

    val newRdd = rddFromFile
    .filter(!functions.isnull($"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Head.Scripts.url"))
      .select(
        $"Envelope.Payload-Metadata.HTTP-Response-Metadata.HTML-Metadata.Head.Scripts.url"
      )
    newRdd.show()

  }
}

Waiting for the bsp connection to come up...
[0m2021.03.01 14:19:52 INFO  time: code lens generation in 3.25s[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3279832539601474146/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3279832539601474146/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.01 14:19:52 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:19:52 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2968558786409873069/bsp.socket'...[0m
2021.03.01 14:19:52 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher9176717073534025112/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher9176717073534025112/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher9176717073534025112/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.01 14:19:53 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2968558786409873069/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2968558786409873069/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.01 14:19:53 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:19:53 INFO  time: Connected to build server in 4.19s[0m
[0m2021.03.01 14:19:53 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.01 14:19:53 INFO  time: Imported build in 0.21s[0m
[0m2021.03.01 14:19:53 ERROR code navigation does not work for the file '/home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala' because the SemanticDB file '/home/amburkee/scala_s3_read/.bloop/root/bloop-bsp-clients-classes/classes-Metals-U23V2LQLRhiux1GMI0uFkQ==/META-INF/semanticdb/src/main/scala/com/revature/Runner.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.03.01 14:19:55 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.01 14:19:55 INFO  time: indexed workspace in 2.48s[0m
[0m2021.03.01 14:20:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:20:11 INFO  time: compiled root in 1.46s[0m
[0m2021.03.01 14:20:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:20:24 INFO  time: compiled root in 0.32s[0m
[0m2021.03.01 14:20:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:20:27 INFO  time: compiled root in 0.25s[0m
[0m2021.03.01 14:20:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:20:54 INFO  time: compiled root in 2.06s[0m
[0m2021.03.01 14:21:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:21:14 INFO  time: compiled root in 1.16s[0m
[0m2021.03.01 14:21:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:21:31 INFO  time: compiled root in 1.17s[0m
[0m2021.03.01 14:24:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:24:35 INFO  time: compiled root in 0.89s[0m
[0m2021.03.01 14:26:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:26:44 INFO  time: compiled root in 0.86s[0m
[0m2021.03.01 14:27:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:27:06 INFO  time: compiled root in 0.76s[0m
[0m2021.03.01 14:27:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:27:10 INFO  time: compiled root in 0.81s[0m
[0m2021.03.01 14:27:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:27:48 INFO  time: compiled root in 0.73s[0m
[0m2021.03.01 14:31:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:31:33 INFO  time: compiled root in 0.72s[0m
[0m2021.03.01 14:32:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:32:43 INFO  time: compiled root in 0.8s[0m
[0m2021.03.01 14:33:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:33:11 INFO  time: compiled root in 0.71s[0m
[0m2021.03.01 14:33:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:33:39 INFO  time: compiled root in 0.72s[0m
[0m2021.03.01 14:34:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:34:29 INFO  time: compiled root in 0.87s[0m
[0m2021.03.01 14:34:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:34:55 INFO  time: compiled root in 0.66s[0m
[0m2021.03.01 14:35:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:35:18 INFO  time: compiled root in 0.68s[0m
[0m2021.03.01 14:38:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:38:05 INFO  time: compiled root in 0.71s[0m
[0m2021.03.01 14:43:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:43:47 INFO  time: compiled root in 0.17s[0m
[0m2021.03.01 14:44:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:44:04 INFO  time: compiled root in 0.62s[0m
[0m2021.03.01 14:44:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:44:12 INFO  time: compiled root in 0.17s[0m
[0m2021.03.01 14:44:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:44:27 INFO  time: compiled root in 0.61s[0m
Mar 01, 2021 2:45:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:45:33 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52: error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
Mar 01, 2021 2:45:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:45:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:45:46 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50: error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
Mar 01, 2021 2:45:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:45:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:45:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:45:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:45:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:45:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:45:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:45:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:45:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:45:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:45:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:45:48 INFO  time: compiled root in 0.13s[0m
Mar 01, 2021 2:45:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:45:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:45:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:45:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:45:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:45:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:45:54 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:45:54 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:46:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:46:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:46:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:46:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:32: stale bloop error: unclosed character literal
WHERE crawl LIKE 'CC-MAIN-2020%'
                               ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:21: stale bloop error: unclosed character literal
  AND subset = 'warc'
                    ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:26: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                         ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:55: stale bloop error: unclosed character literal
  AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                      ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:27: stale bloop error: unclosed character literal
  AND url_path like '%job%'
                          ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:10: stale bloop error: unclosed string literal
limit 200"
         ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:05 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:23 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:23 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:23 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:23 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:23 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:23 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:23 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:23 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:23 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:24 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:46:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:46:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:25 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:46:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:46:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:27 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:46:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:28 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:46:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:29 INFO  time: compiled root in 0.12s[0m
Mar 01, 2021 2:46:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 60 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:36 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:13: stale bloop error: unclosed string literal
      FROM \"ccindex\".\"ccindex\"
            ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:46:41 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:46:41 INFO  time: compiled root in 0.19s[0m
Mar 01, 2021 2:46:54 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:46:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:47:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:47:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:47:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:47:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:03 INFO  time: compiled root in 0.12s[0m
Mar 01, 2021 2:47:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:47:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:54 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:47:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:47:59 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:48:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:48:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:48:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:48:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:62:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:05 INFO  time: compiled root in 0.11s[0m
Mar 01, 2021 2:48:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:48:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:48:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 59 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 59 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 59 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:48:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 59 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 59 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 59 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:48:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:48:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:48:53 INFO  time: compiled root in 0.11s[0m
Mar 01, 2021 2:48:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:49:15 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:49:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:49:16 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:49:16 INFO  time: compiled root in 0.14s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.Closeable
import java.util.concurrent.atomic.AtomicReference

import scala.collection.JavaConverters._
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.spark.{SPARK_VERSION, SparkConf, SparkContext, TaskContext}
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd}
import org.apache.spark.sql.catalog.Catalog
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions.AttributeReference
import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, Range}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.internal._
import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
import org.apache.spark.sql.sources.BaseRelation
import org.apache.spark.sql.streaming._
import org.apache.spark.sql.types.{DataType, StructType}
import org.apache.spark.sql.util.ExecutionListenerManager
import org.apache.spark.util.{CallSite, Utils}


/**
 * The entry point to programming Spark with the Dataset and DataFrame API.
 *
 * In environments that this has been created upfront (e.g. REPL, notebooks), use the builder
 * to get an existing session:
 *
 * {{{
 *   SparkSession.builder().getOrCreate()
 * }}}
 *
 * The builder can also be used to create a new session:
 *
 * {{{
 *   SparkSession.builder
 *     .master("local")
 *     .appName("Word Count")
 *     .config("spark.some.config.option", "some-value")
 *     .getOrCreate()
 * }}}
 *
 * @param sparkContext The Spark context associated with this Spark session.
 * @param existingSharedState If supplied, use the existing shared state
 *                            instead of creating a new one.
 * @param parentSessionState If supplied, inherit all session state (i.e. temporary
 *                            views, SQL config, UDFs etc) from parent.
 */
@InterfaceStability.Stable
class SparkSession private(
    @transient val sparkContext: SparkContext,
    @transient private val existingSharedState: Option[SharedState],
    @transient private val parentSessionState: Option[SessionState],
    @transient private[sql] val extensions: SparkSessionExtensions)
  extends Serializable with Closeable with Logging { self =>

  // The call site where this SparkSession was constructed.
  private val creationSite: CallSite = Utils.getCallSite()

  private[sql] def this(sc: SparkContext) {
    this(sc, None, None, new SparkSessionExtensions)
  }

  sparkContext.assertNotStopped()

  // If there is no active SparkSession, uses the default SQL conf. Otherwise, use the session's.
  SQLConf.setSQLConfGetter(() => {
    SparkSession.getActiveSession.filterNot(_.sparkContext.isStopped).map(_.sessionState.conf)
      .getOrElse(SQLConf.getFallbackConf)
  })

  /**
   * The version of Spark on which this application is running.
   *
   * @since 2.0.0
   */
  def version: String = SPARK_VERSION

  /* ----------------------- *
   |  Session-related state  |
   * ----------------------- */

  /**
   * State shared across sessions, including the `SparkContext`, cached data, listener,
   * and a catalog that interacts with external systems.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sharedState: SharedState = {
    existingSharedState.getOrElse(new SharedState(sparkContext))
  }

  /**
   * Initial options for session. This options are applied once when sessionState is created.
   */
  @transient
  private[sql] val initialSessionOptions = new scala.collection.mutable.HashMap[String, String]

  /**
   * State isolated across sessions, including SQL configurations, temporary tables, registered
   * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]].
   * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sessionState: SessionState = {
    parentSessionState
      .map(_.clone(this))
      .getOrElse {
        val state = SparkSession.instantiateSessionState(
          SparkSession.sessionStateClassName(sparkContext.conf),
          self)
        initialSessionOptions.foreach { case (k, v) => state.conf.setConfString(k, v) }
        state
      }
  }

  /**
   * A wrapped version of this session in the form of a [[SQLContext]], for backward compatibility.
   *
   * @since 2.0.0
   */
  @transient
  val sqlContext: SQLContext = new SQLContext(this)

  /**
   * Runtime configuration interface for Spark.
   *
   * This is the interface through which the user can get and set all Spark and Hadoop
   * configurations that are relevant to Spark SQL. When getting the value of a config,
   * this defaults to the value set in the underlying `SparkContext`, if any.
   *
   * @since 2.0.0
   */
  @transient lazy val conf: RuntimeConfig = new RuntimeConfig(sessionState.conf)

  /**
   * :: Experimental ::
   * An interface to register custom [[org.apache.spark.sql.util.QueryExecutionListener]]s
   * that listen for execution metrics.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def listenerManager: ExecutionListenerManager = sessionState.listenerManager

  /**
   * :: Experimental ::
   * A collection of methods that are considered experimental, but can be used to hook into
   * the query planner for advanced functionality.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Unstable
  def experimental: ExperimentalMethods = sessionState.experimentalMethods

  /**
   * A collection of methods for registering user-defined functions (UDF).
   *
   * The following example registers a Scala closure as UDF:
   * {{{
   *   sparkSession.udf.register("myUDF", (arg1: Int, arg2: String) => arg2 + arg1)
   * }}}
   *
   * The following example registers a UDF in Java:
   * {{{
   *   sparkSession.udf().register("myUDF",
   *       (Integer arg1, String arg2) -> arg2 + arg1,
   *       DataTypes.StringType);
   * }}}
   *
   * @note The user-defined functions must be deterministic. Due to optimization,
   * duplicate invocations may be eliminated or the function may even be invoked more times than
   * it is present in the query.
   *
   * @since 2.0.0
   */
  def udf: UDFRegistration = sessionState.udfRegistration

  /**
   * :: Experimental ::
   * Returns a `StreamingQueryManager` that allows managing all the
   * `StreamingQuery`s active on `this`.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Unstable
  def streams: StreamingQueryManager = sessionState.streamingQueryManager

  /**
   * Start a new session with isolated SQL configurations, temporary tables, registered
   * functions are isolated, but sharing the underlying `SparkContext` and cached data.
   *
   * @note Other than the `SparkContext`, all shared state is initialized lazily.
   * This method will force the initialization of the shared state to ensure that parent
   * and child sessions are set up with the same shared state. If the underlying catalog
   * implementation is Hive, this will initialize the metastore, which may take some time.
   *
   * @since 2.0.0
   */
  def newSession(): SparkSession = {
    new SparkSession(sparkContext, Some(sharedState), parentSessionState = None, extensions)
  }

  /**
   * Create an identical copy of this `SparkSession`, sharing the underlying `SparkContext`
   * and shared state. All the state of this session (i.e. SQL configurations, temporary tables,
   * registered functions) is copied over, and the cloned session is set up with the same shared
   * state as this session. The cloned session is independent of this session, that is, any
   * non-global change in either session is not reflected in the other.
   *
   * @note Other than the `SparkContext`, all shared state is initialized lazily.
   * This method will force the initialization of the shared state to ensure that parent
   * and child sessions are set up with the same shared state. If the underlying catalog
   * implementation is Hive, this will initialize the metastore, which may take some time.
   */
  private[sql] def cloneSession(): SparkSession = {
    val result = new SparkSession(sparkContext, Some(sharedState), Some(sessionState), extensions)
    result.sessionState // force copy of SessionState
    result
  }


  /* --------------------------------- *
   |  Methods for creating DataFrames  |
   * --------------------------------- */

  /**
   * Returns a `DataFrame` with no rows or columns.
   *
   * @since 2.0.0
   */
  @transient
  lazy val emptyDataFrame: DataFrame = {
    createDataFrame(sparkContext.emptyRDD[Row].setName("empty"), StructType(Nil))
  }

  /**
   * :: Experimental ::
   * Creates a new [[Dataset]] of type T containing zero elements.
   *
   * @return 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def emptyDataset[T: Encoder]: Dataset[T] = {
    val encoder = implicitly[Encoder[T]]
    new Dataset(self, LocalRelation(encoder.schema.toAttributes), encoder)
  }

  /**
   * :: Experimental ::
   * Creates a `DataFrame` from an RDD of Product (e.g. case classes, tuples).
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = {
    SparkSession.setActiveSession(this)
    val encoder = Encoders.product[A]
    Dataset.ofRows(self, ExternalRDD(rdd, self)(encoder))
  }

  /**
   * :: Experimental ::
   * Creates a `DataFrame` from a local Seq of Product.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataFrame[A <: Product : TypeTag](data: Seq[A]): DataFrame = {
    SparkSession.setActiveSession(this)
    val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]
    val attributeSeq = schema.toAttributes
    Dataset.ofRows(self, LocalRelation.fromProduct(attributeSeq, data))
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from an `RDD` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided RDD matches
   * the provided schema. Otherwise, there will be runtime exception.
   * Example:
   * {{{
   *  import org.apache.spark.sql._
   *  import org.apache.spark.sql.types._
   *  val sparkSession = new org.apache.spark.sql.SparkSession(sc)
   *
   *  val schema =
   *    StructType(
   *      StructField("name", StringType, false) ::
   *      StructField("age", IntegerType, true) :: Nil)
   *
   *  val people =
   *    sc.textFile("examples/src/main/resources/people.txt").map(
   *      _.split(",")).map(p => Row(p(0), p(1).trim.toInt))
   *  val dataFrame = sparkSession.createDataFrame(people, schema)
   *  dataFrame.printSchema
   *  // root
   *  // |-- name: string (nullable = false)
   *  // |-- age: integer (nullable = true)
   *
   *  dataFrame.createOrReplaceTempView("people")
   *  sparkSession.sql("select name from people").collect.foreach(println)
   * }}}
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame = {
    createDataFrame(rowRDD, schema, needsConversion = true)
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from a `JavaRDD` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided RDD matches
   * the provided schema. Otherwise, there will be runtime exception.
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rowRDD: JavaRDD[Row], schema: StructType): DataFrame = {
    createDataFrame(rowRDD.rdd, schema)
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from a `java.util.List` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided List matches
   * the provided schema. Otherwise, there will be runtime exception.
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rows: java.util.List[Row], schema: StructType): DataFrame = {
    Dataset.ofRows(self, LocalRelation.fromExternalRows(schema.toAttributes, rows.asScala))
  }

  /**
   * Applies a schema to an RDD of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   * SELECT * queries will return the columns in an undefined order.
   *
   * @since 2.0.0
   */
  def createDataFrame(rdd: RDD[_], beanClass: Class[_]): DataFrame = {
    val attributeSeq: Seq[AttributeReference] = getSchema(beanClass)
    val className = beanClass.getName
    val rowRdd = rdd.mapPartitions { iter =>
    // BeanInfo is not serializable so we must rediscover it remotely for each partition.
      SQLContext.beansToRows(iter, Utils.classForName(className), attributeSeq)
    }
    Dataset.ofRows(self, LogicalRDD(attributeSeq, rowRdd.setName(rdd.name))(self))
  }

  /**
   * Applies a schema to an RDD of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   * SELECT * queries will return the columns in an undefined order.
   *
   * @since 2.0.0
   */
  def createDataFrame(rdd: JavaRDD[_], beanClass: Class[_]): DataFrame = {
    createDataFrame(rdd.rdd, beanClass)
  }

  /**
   * Applies a schema to a List of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   *          SELECT * queries will return the columns in an undefined order.
   * @since 1.6.0
   */
  def createDataFrame(data: java.util.List[_], beanClass: Class[_]): DataFrame = {
    val attrSeq = getSchema(beanClass)
    val rows = SQLContext.beansToRows(data.asScala.iterator, beanClass, attrSeq)
    Dataset.ofRows(self, LocalRelation(attrSeq, rows.toSeq))
  }

  /**
   * Convert a `BaseRelation` created for external data sources into a `DataFrame`.
   *
   * @since 2.0.0
   */
  def baseRelationToDataFrame(baseRelation: BaseRelation): DataFrame = {
    Dataset.ofRows(self, LogicalRelation(baseRelation))
  }

  /* ------------------------------- *
   |  Methods for creating DataSets  |
   * ------------------------------- */

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from a local Seq of data of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * == Example ==
   *
   * {{{
   *
   *   import spark.implicits._
   *   case class Person(name: String, age: Long)
   *   val data = Seq(Person("Michael", 29), Person("Andy", 30), Person("Justin", 19))
   *   val ds = spark.createDataset(data)
   *
   *   ds.show()
   *   // +-------+---+
   *   // |   name|age|
   *   // +-------+---+
   *   // |Michael| 29|
   *   // |   Andy| 30|
   *   // | Justin| 19|
   *   // +-------+---+
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: Seq[T]): Dataset[T] = {
    // `ExpressionEncoder` is not thread-safe, here we create a new encoder.
    val enc = encoderFor[T].copy()
    val attributes = enc.schema.toAttributes
    val encoded = data.map(d => enc.toRow(d).copy())
    val plan = new LocalRelation(attributes, encoded)
    Dataset[T](self, plan)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from an RDD of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: RDD[T]): Dataset[T] = {
    Dataset[T](self, ExternalRDD(data, self))
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from a `java.util.List` of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * == Java Example ==
   *
   * {{{
   *     List<String> data = Arrays.asList("hello", "world");
   *     Dataset<String> ds = spark.createDataset(data, Encoders.STRING());
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: java.util.List[T]): Dataset[T] = {
    createDataset(data.asScala)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from 0 to `end` (exclusive) with step value 1.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(end: Long): Dataset[java.lang.Long] = range(0, end)

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with step value 1.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long): Dataset[java.lang.Long] = {
    range(start, end, step = 1, numPartitions = sparkContext.defaultParallelism)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with a step value.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long, step: Long): Dataset[java.lang.Long] = {
    range(start, end, step, numPartitions = sparkContext.defaultParallelism)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with a step value, with partition number
   * specified.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long] = {
    new Dataset(self, Range(start, end, step, numPartitions), Encoders.LONG)
  }

  /**
   * Creates a `DataFrame` from an `RDD[InternalRow]`.
   */
  private[sql] def internalCreateDataFrame(
      catalystRows: RDD[InternalRow],
      schema: StructType,
      isStreaming: Boolean = false): DataFrame = {
    // TODO: use MutableProjection when rowRDD is another DataFrame and the applied
    // schema differs from the existing schema on any field data type.
    val logicalPlan = LogicalRDD(
      schema.toAttributes,
      catalystRows,
      isStreaming = isStreaming)(self)
    Dataset.ofRows(self, logicalPlan)
  }

  /**
   * Creates a `DataFrame` from an `RDD[Row]`.
   * User can specify whether the input rows should be converted to Catalyst rows.
   */
  private[sql] def createDataFrame(
      rowRDD: RDD[Row],
      schema: StructType,
      needsConversion: Boolean) = {
    // TODO: use MutableProjection when rowRDD is another DataFrame and the applied
    // schema differs from the existing schema on any field data type.
    val catalystRows = if (needsConversion) {
      val encoder = RowEncoder(schema)
      rowRDD.map(encoder.toRow)
    } else {
      rowRDD.map { r: Row => InternalRow.fromSeq(r.toSeq) }
    }
    internalCreateDataFrame(catalystRows.setName(rowRDD.name), schema)
  }


  /* ------------------------- *
   |  Catalog-related methods  |
   * ------------------------- */

  /**
   * Interface through which the user may create, drop, alter or query underlying
   * databases, tables, functions etc.
   *
   * @since 2.0.0
   */
  @transient lazy val catalog: Catalog = new CatalogImpl(self)

  /**
   * Returns the specified table/view as a `DataFrame`.
   *
   * @param tableName is either a qualified or unqualified name that designates a table or view.
   *                  If a database is specified, it identifies the table/view from the database.
   *                  Otherwise, it first attempts to find a temporary view with the given name
   *                  and then match the table/view from the current database.
   *                  Note that, the global temporary view database is also valid here.
   * @since 2.0.0
   */
  def table(tableName: String): DataFrame = {
    table(sessionState.sqlParser.parseTableIdentifier(tableName))
  }

  private[sql] def table(tableIdent: TableIdentifier): DataFrame = {
    Dataset.ofRows(self, UnresolvedRelation(tableIdent))
  }

  /* ----------------- *
   |  Everything else  |
   * ----------------- */

  /**
   * Executes a SQL query using Spark, returning the result as a `DataFrame`.
   * The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'.
   *
   * @since 2.0.0
   */
  def sql(sqlText: String): DataFrame = {
    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))
  }

  /**
   * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a
   * `DataFrame`.
   * {{{
   *   sparkSession.read.parquet("/path/to/file.parquet")
   *   sparkSession.read.schema(schema).json("/path/to/file.json")
   * }}}
   *
   * @since 2.0.0
   */
  def read: DataFrameReader = new DataFrameReader(self)

  /**
   * Returns a `DataStreamReader` that can be used to read streaming data in as a `DataFrame`.
   * {{{
   *   sparkSession.readStream.parquet("/path/to/directory/of/parquet/files")
   *   sparkSession.readStream.schema(schema).json("/path/to/directory/of/json/files")
   * }}}
   *
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def readStream: DataStreamReader = new DataStreamReader(self)

  /**
   * Executes some code block and prints to stdout the time taken to execute the block. This is
   * available in Scala only and is used primarily for interactive testing and debugging.
   *
   * @since 2.1.0
   */
  def time[T](f: => T): T = {
    val start = System.nanoTime()
    val ret = f
    val end = System.nanoTime()
    // scalastyle:off println
    println(s"Time taken: ${(end - start) / 1000 / 1000} ms")
    // scalastyle:on println
    ret
  }

  // scalastyle:off
  // Disable style checker so "implicits" object can start with lowercase i
  /**
   * :: Experimental ::
   * (Scala-specific) Implicit methods available in Scala for converting
   * common Scala objects into `DataFrame`s.
   *
   * {{{
   *   val sparkSession = SparkSession.builder.getOrCreate()
   *   import sparkSession.implicits._
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  object implicits extends SQLImplicits with Serializable {
    protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext
  }
  // scalastyle:on

  /**
   * Stop the underlying `SparkContext`.
   *
   * @since 2.0.0
   */
  def stop(): Unit = {
    sparkContext.stop()
  }

  /**
   * Synonym for `stop()`.
   *
   * @since 2.1.0
   */
  override def close(): Unit = stop()

  /**
   * Parses the data type in our internal string representation. The data type string should
   * have the same format as the one generated by `toString` in scala.
   * It is only used by PySpark.
   */
  protected[sql] def parseDataType(dataTypeString: String): DataType = {
    DataType.fromJson(dataTypeString)
  }

  /**
   * Apply a schema defined by the schemaString to an RDD. It is only used by PySpark.
   */
  private[sql] def applySchemaToPythonRDD(
      rdd: RDD[Array[Any]],
      schemaString: String): DataFrame = {
    val schema = DataType.fromJson(schemaString).asInstanceOf[StructType]
    applySchemaToPythonRDD(rdd, schema)
  }

  /**
   * Apply `schema` to an RDD.
   *
   * @note Used by PySpark only
   */
  private[sql] def applySchemaToPythonRDD(
      rdd: RDD[Array[Any]],
      schema: StructType): DataFrame = {
    val rowRdd = rdd.mapPartitions { iter =>
      val fromJava = python.EvaluatePython.makeFromJava(schema)
      iter.map(r => fromJava(r).asInstanceOf[InternalRow])
    }
    internalCreateDataFrame(rowRdd, schema)
  }

  /**
   * Returns a Catalyst Schema for the given java bean class.
   */
  private def getSchema(beanClass: Class[_]): Seq[AttributeReference] = {
    val (dataType, _) = JavaTypeInference.inferDataType(beanClass)
    dataType.asInstanceOf[StructType].fields.map { f =>
      AttributeReference(f.name, f.dataType, f.nullable)()
    }
  }

}


@InterfaceStability.Stable
object SparkSession extends Logging {

  /**
   * Builder for [[SparkSession]].
   */
  @InterfaceStability.Stable
  class Builder extends Logging {

    private[this] val options = new scala.collection.mutable.HashMap[String, String]

    private[this] val extensions = new SparkSessionExtensions

    private[this] var userSuppliedContext: Option[SparkContext] = None

    private[spark] def sparkContext(sparkContext: SparkContext): Builder = synchronized {
      userSuppliedContext = Option(sparkContext)
      this
    }

    /**
     * Sets a name for the application, which will be shown in the Spark web UI.
     * If no application name is set, a randomly generated name will be used.
     *
     * @since 2.0.0
     */
    def appName(name: String): Builder = config("spark.app.name", name)

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: String): Builder = synchronized {
      options += key -> value
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Long): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Double): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Boolean): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a list of config options based on the given `SparkConf`.
     *
     * @since 2.0.0
     */
    def config(conf: SparkConf): Builder = synchronized {
      conf.getAll.foreach { case (k, v) => options += k -> v }
      this
    }

    /**
     * Sets the Spark master URL to connect to, such as "local" to run locally, "local[4]" to
     * run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone cluster.
     *
     * @since 2.0.0
     */
    def master(master: String): Builder = config("spark.master", master)

    /**
     * Enables Hive support, including connectivity to a persistent Hive metastore, support for
     * Hive serdes, and Hive user-defined functions.
     *
     * @since 2.0.0
     */
    def enableHiveSupport(): Builder = synchronized {
      if (hiveClassesArePresent) {
        config(CATALOG_IMPLEMENTATION.key, "hive")
      } else {
        throw new IllegalArgumentException(
          "Unable to instantiate SparkSession with Hive support because " +
            "Hive classes are not found.")
      }
    }

    /**
     * Inject extensions into the [[SparkSession]]. This allows a user to add Analyzer rules,
     * Optimizer rules, Planning Strategies or a customized parser.
     *
     * @since 2.2.0
     */
    def withExtensions(f: SparkSessionExtensions => Unit): Builder = synchronized {
      f(extensions)
      this
    }

    /**
     * Gets an existing [[SparkSession]] or, if there is no existing one, creates a new
     * one based on the options set in this builder.
     *
     * This method first checks whether there is a valid thread-local SparkSession,
     * and if yes, return that one. It then checks whether there is a valid global
     * default SparkSession, and if yes, return that one. If no valid global default
     * SparkSession exists, the method creates a new SparkSession and assigns the
     * newly created SparkSession as the global default.
     *
     * In case an existing SparkSession is returned, the non-static config options specified in
     * this builder will be applied to the existing SparkSession.
     *
     * @since 2.0.0
     */
    def getOrCreate(): SparkSession = synchronized {
      assertOnDriver()
      // Get the session from current thread's active session.
      var session = activeThreadSession.get()
      if ((session ne null) && !session.sparkContext.isStopped) {
        applyModifiableSettings(session)
        return session
      }

      // Global synchronization so we will only set the default session once.
      SparkSession.synchronized {
        // If the current thread does not have an active session, get it from the global session.
        session = defaultSession.get()
        if ((session ne null) && !session.sparkContext.isStopped) {
          applyModifiableSettings(session)
          return session
        }

        // No active nor global default session. Create a new one.
        val sparkContext = userSuppliedContext.getOrElse {
          val sparkConf = new SparkConf()
          options.foreach { case (k, v) => sparkConf.set(k, v) }

          // set a random app name if not given.
          if (!sparkConf.contains("spark.app.name")) {
            sparkConf.setAppName(java.util.UUID.randomUUID().toString)
          }

          SparkContext.getOrCreate(sparkConf)
          // Do not update `SparkConf` for existing `SparkContext`, as it's shared by all sessions.
        }

        // Initialize extensions if the user has defined a configurator class.
        val extensionConfOption = sparkContext.conf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS)
        if (extensionConfOption.isDefined) {
          val extensionConfClassName = extensionConfOption.get
          try {
            val extensionConfClass = Utils.classForName(extensionConfClassName)
            val extensionConf = extensionConfClass.newInstance()
              .asInstanceOf[SparkSessionExtensions => Unit]
            extensionConf(extensions)
          } catch {
            // Ignore the error if we cannot find the class or when the class has the wrong type.
            case e @ (_: ClassCastException |
                      _: ClassNotFoundException |
                      _: NoClassDefFoundError) =>
              logWarning(s"Cannot use $extensionConfClassName to configure session extensions.", e)
          }
        }

        session = new SparkSession(sparkContext, None, None, extensions)
        options.foreach { case (k, v) => session.initialSessionOptions.put(k, v) }
        setDefaultSession(session)
        setActiveSession(session)

        // Register a successfully instantiated context to the singleton. This should be at the
        // end of the class definition so that the singleton is updated only if there is no
        // exception in the construction of the instance.
        sparkContext.addSparkListener(new SparkListener {
          override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {
            defaultSession.set(null)
          }
        })
      }

      return session
    }

    private def applyModifiableSettings(session: SparkSession): Unit = {
      val (staticConfs, otherConfs) =
        options.partition(kv => SQLConf.staticConfKeys.contains(kv._1))

      otherConfs.foreach { case (k, v) => session.sessionState.conf.setConfString(k, v) }

      if (staticConfs.nonEmpty) {
        logWarning("Using an existing SparkSession; the static sql configurations will not take" +
          " effect.")
      }
      if (otherConfs.nonEmpty) {
        logWarning("Using an existing SparkSession; some spark core configurations may not take" +
          " effect.")
      }
    }
  }

  /**
   * Creates a [[SparkSession.Builder]] for constructing a [[SparkSession]].
   *
   * @since 2.0.0
   */
  def builder(): Builder = new Builder

  /**
   * Changes the SparkSession that will be returned in this thread and its children when
   * SparkSession.getOrCreate() is called. This can be used to ensure that a given thread receives
   * a SparkSession with an isolated session, instead of the global (first created) context.
   *
   * @since 2.0.0
   */
  def setActiveSession(session: SparkSession): Unit = {
    activeThreadSession.set(session)
  }

  /**
   * Clears the active SparkSession for current thread. Subsequent calls to getOrCreate will
   * return the first created context instead of a thread-local override.
   *
   * @since 2.0.0
   */
  def clearActiveSession(): Unit = {
    activeThreadSession.remove()
  }

  /**
   * Sets the default SparkSession that is returned by the builder.
   *
   * @since 2.0.0
   */
  def setDefaultSession(session: SparkSession): Unit = {
    defaultSession.set(session)
  }

  /**
   * Clears the default SparkSession that is returned by the builder.
   *
   * @since 2.0.0
   */
  def clearDefaultSession(): Unit = {
    defaultSession.set(null)
  }

  /**
   * Returns the active SparkSession for the current thread, returned by the builder.
   *
   * @note Return None, when calling this function on executors
   *
   * @since 2.2.0
   */
  def getActiveSession: Option[SparkSession] = {
    if (TaskContext.get != null) {
      // Return None when running on executors.
      None
    } else {
      Option(activeThreadSession.get)
    }
  }

  /**
   * Returns the default SparkSession that is returned by the builder.
   *
   * @note Return None, when calling this function on executors
   *
   * @since 2.2.0
   */
  def getDefaultSession: Option[SparkSession] = {
    if (TaskContext.get != null) {
      // Return None when running on executors.
      None
    } else {
      Option(defaultSession.get)
    }
  }

  /**
   * Returns the currently active SparkSession, otherwise the default one. If there is no default
   * SparkSession, throws an exception.
   *
   * @since 2.4.0
   */
  def active: SparkSession = {
    getActiveSession.getOrElse(getDefaultSession.getOrElse(
      throw new IllegalStateException("No active or default Spark session found")))
  }

  ////////////////////////////////////////////////////////////////////////////////////////
  // Private methods from now on
  ////////////////////////////////////////////////////////////////////////////////////////

  /** The active SparkSession for the current thread. */
  private val activeThreadSession = new InheritableThreadLocal[SparkSession]

  /** Reference to the root SparkSession. */
  private val defaultSession = new AtomicReference[SparkSession]

  private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME =
    "org.apache.spark.sql.hive.HiveSessionStateBuilder"

  private def sessionStateClassName(conf: SparkConf): String = {
    conf.get(CATALOG_IMPLEMENTATION) match {
      case "hive" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME
      case "in-memory" => classOf[SessionStateBuilder].getCanonicalName
    }
  }

  private def assertOnDriver(): Unit = {
    if (Utils.isTesting && TaskContext.get != null) {
      // we're accessing it during task execution, fail.
      throw new IllegalStateException(
        "SparkSession should only be created and accessed on the driver.")
    }
  }

  /**
   * Helper method to create an instance of `SessionState` based on `className` from conf.
   * The result is either `SessionState` or a Hive based `SessionState`.
   */
  private def instantiateSessionState(
      className: String,
      sparkSession: SparkSession): SessionState = {
    try {
      // invoke `new [Hive]SessionStateBuilder(SparkSession, Option[SessionState])`
      val clazz = Utils.classForName(className)
      val ctor = clazz.getConstructors.head
      ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build()
    } catch {
      case NonFatal(e) =>
        throw new IllegalArgumentException(s"Error while instantiating '$className':", e)
    }
  }

  /**
   * @return true if Hive classes can be loaded, otherwise false.
   */
  private[spark] def hiveClassesArePresent: Boolean = {
    try {
      Utils.classForName(HIVE_SESSION_STATE_BUILDER_CLASS_NAME)
      Utils.classForName("org.apache.hadoop.hive.conf.HiveConf")
      true
    } catch {
      case _: ClassNotFoundException | _: NoClassDefFoundError => false
    }
  }

  private[spark] def cleanupAnyExistingSession(): Unit = {
    val session = getActiveSession.orElse(getDefaultSession)
    if (session.isDefined) {
      logWarning(
        s"""An existing Spark session exists as the active or default session.
           |This probably means another suite leaked it. Attempting to stop it before continuing.
           |This existing Spark session was created at:
           |
           |${session.get.creationSite.longForm}
           |
         """.stripMargin)
      session.get.stop()
      SparkSession.clearActiveSession()
      SparkSession.clearDefaultSession()
    }
  }
}

Mar 01, 2021 2:49:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:26 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:29 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:33: stale bloop error: unclosed character literal
        AND url_path like '%job%'
                                ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:30 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:30 INFO  time: compiled root in 0.11s[0m
Mar 01, 2021 2:50:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:50:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:37 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:50:37 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:50:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:45 INFO  time: compiled root in 0.12s[0m
Mar 01, 2021 2:50:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:50:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 55 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 55 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 55 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:50:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 55 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 55 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 55 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:50:54 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:55 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:50:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:56 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:50:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:50:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 56 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:50:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:50:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:00 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:51:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      limit 200"
               ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      LIMIT 200"
               ^[0m
[0m2021.03.01 14:51:01 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:01 INFO  time: compiled root in 0.11s[0m
Mar 01, 2021 2:51:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:51:12 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50: error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
Mar 01, 2021 2:51:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 58 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:51:13 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50: error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
Mar 01, 2021 2:51:37 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      LIMIT 200"
               ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      LIMIT 200"
               ^[0m
[0m2021.03.01 14:51:39 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:38: stale bloop error: unclosed character literal
      WHERE crawl LIKE 'CC-MAIN-2020%'
                                     ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:27: stale bloop error: unclosed character literal
        AND subset = 'warc'
                          ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:32: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                               ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:61: stale bloop error: unclosed character literal
        AND (url_host_tld = 'us' or url_host_name like 'us.%')
                                                            ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:38: stale bloop error: unclosed character literal
        AND url_path like '%tech%job%'
                                     ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:16: stale bloop error: unclosed string literal
      LIMIT 200"
               ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:60:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:40 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:40 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, count(*) as n, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:51:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:49 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:57 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:58 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:58 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.01 14:51:58 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:51:58 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:58 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:51:58 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:51:58 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:51:58 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:52:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:52:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:52:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:52:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:52:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:52:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:52:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Mar 01, 2021 2:52:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:52:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:52:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Mar 01, 2021 2:52:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 57 is not a valid line number, allowed [0..54]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.01 14:52:07 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:52:07 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:52:07 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:52:07 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:7: stale bloop error: unclosed string literal
      "SELECT url_host_name, arbitrary(url_path) as sample_path
      ^[0m
[0m2021.03.01 14:52:07 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:16: stale bloop error: unclosed string literal
      FROM rdd"
               ^[0m
[0m2021.03.01 14:52:07 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 14:52:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:52:08 INFO  time: compiled root in 0.62s[0m
[0m2021.03.01 14:52:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:52:18 INFO  time: compiled root in 0.56s[0m
[0m2021.03.01 14:52:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:52:42 INFO  time: compiled root in 0.58s[0m
[0m2021.03.01 14:53:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:53:27 INFO  time: compiled root in 0.58s[0m
[0m2021.03.01 14:54:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:54:12 INFO  time: compiled root in 0.63s[0m
[0m2021.03.01 14:55:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:55:03 INFO  time: compiled root in 0.61s[0m
[0m2021.03.01 14:58:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:58:44 INFO  time: compiled root in 0.66s[0m
[0m2021.03.01 20:19:29 INFO  shutting down Metals[0m
[0m2021.03.01 20:19:29 INFO  Shut down connection with build server.[0m
[0m2021.03.01 20:19:29 INFO  Shut down connection with build server.[0m
[0m2021.03.01 20:19:29 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.02 08:15:26 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.53.2.[0m
[0m2021.03.02 08:15:27 INFO  time: initialize in 0.38s[0m
[0m2021.03.02 08:15:26 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.02 08:15:27 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2803100368090441336/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.02 08:15:27 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.02 08:15:29 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
// import com.amazonaws.services.s3.AmazonS3Client
// import com.amazonaws.auth.BasicAWSCredentials
// import org.apache.spark.SparkContext
// import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?

    // df
    //   .select("url_host_name", "url_path")
    //   .filter($"crawl" === "CC-MAIN-2020-16")
    //   .filter($"subset" === "warc")
    //   .filter($"url_path".contains("job"))
    //   .show(200, false)

    val rddFromFile = spark.read.json(
      "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    )

    //"Container", "Envelope", "_corrupt_record"
    // rddFromFile.show(false)
    // rddFromFile.printSchema()

    rddFromFile.createOrReplaceGlobalTempView("rdd")

    spark
      .sql(
        "SELECT url_host_name " +
          "FROM rdd " +
          "WHERE crawl LIKE 'CC-MAIN-2020% " +
          "AND subset = 'warc" +
          "AND url_path LIKE '%tech%job%" +
          "LIMIT 200"
      )
      .show()
  }
}

Waiting for the bsp connection to come up...
[0m2021.03.02 08:15:31 INFO  time: code lens generation in 3.34s[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2803100368090441336/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2803100368090441336/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.02 08:15:31 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 08:15:31 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.02 08:15:31 Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7540335949043866379/bsp.socket'...INFO 
 Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher145908222744500001/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7540335949043866379/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7540335949043866379/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher145908222744500001/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher145908222744500001/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.02 08:15:32 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 08:15:32 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 08:15:32 INFO  time: Connected to build server in 4.27s[0m
[0m2021.03.02 08:15:32 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.02 08:15:32 INFO  time: Imported build in 0.22s[0m
[0m2021.03.02 08:15:33 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.02 08:15:33 INFO  time: indexed workspace in 1.98s[0m
[0m2021.03.02 08:37:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:37:28 INFO  time: compiled root in 3.16s[0m
[0m2021.03.02 08:50:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:50:05 INFO  time: compiled root in 1.19s[0m
[0m2021.03.02 08:51:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:51:09 INFO  time: compiled root in 0.96s[0m
[0m2021.03.02 08:51:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:51:43 INFO  time: compiled root in 1.06s[0m
[0m2021.03.02 08:52:07 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:63: error: } expected but end of file found

^[0m
[0m2021.03.02 08:52:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:52:43 INFO  time: compiled root in 0.26s[0m
[0m2021.03.02 08:53:27 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:65: error: } expected but end of file found

^[0m
[0m2021.03.02 08:53:31 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:65: error: identifier expected but } found
}
^[0m
[0m2021.03.02 08:53:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:53:33 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 08:54:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:54:11 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 08:54:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:54:24 INFO  time: compiled root in 0.13s[0m
Mar 02, 2021 8:55:01 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 65 is not a valid line number, allowed [0..62]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 65 is not a valid line number, allowed [0..62]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 65 is not a valid line number, allowed [0..62]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.02 08:55:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:55:05 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 08:55:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:55:09 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 08:55:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:55:19 INFO  time: compiled root in 1.1s[0m
[0m2021.03.02 08:55:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:55:26 INFO  time: compiled root in 1s[0m
[0m2021.03.02 08:56:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:56:21 INFO  time: compiled root in 0.91s[0m
[0m2021.03.02 08:58:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:58:10 INFO  time: compiled root in 0.73s[0m
[0m2021.03.02 08:58:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 08:58:52 INFO  time: compiled root in 0.74s[0m
[0m2021.03.02 09:00:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:00:18 INFO  time: compiled root in 0.71s[0m
[0m2021.03.02 09:00:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:00:31 INFO  time: compiled root in 0.73s[0m
[0m2021.03.02 09:01:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:01:04 INFO  time: compiled root in 0.94s[0m
[0m2021.03.02 09:01:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:01:37 INFO  time: compiled root in 0.61s[0m
[0m2021.03.02 09:01:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:01:43 INFO  time: compiled root in 0.6s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.03.02 09:06:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:06:53 INFO  time: compiled root in 0.84s[0m
[0m2021.03.02 09:07:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:07:35 INFO  time: compiled root in 0.72s[0m
[0m2021.03.02 09:08:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:08:08 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 09:09:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:09:59 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 09:10:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:10:46 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 09:12:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:12:19 INFO  time: compiled root in 0.63s[0m
[0m2021.03.02 09:13:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:13:09 INFO  time: compiled root in 0.6s[0m
[0m2021.03.02 09:14:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:14:32 INFO  time: compiled root in 0.74s[0m
[0m2021.03.02 09:15:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:15:10 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 09:16:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:16:02 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 09:16:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:16:31 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 09:16:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:16:37 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 09:16:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:16:46 INFO  time: compiled root in 0.93s[0m
[0m2021.03.02 09:17:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:17:42 INFO  time: compiled root in 0.71s[0m
[0m2021.03.02 09:17:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:17:50 INFO  time: compiled root in 0.67s[0m
[0m2021.03.02 09:19:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:19:04 INFO  time: compiled root in 0.64s[0m
[0m2021.03.02 09:19:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:19:11 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 09:19:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:19:16 INFO  time: compiled root in 0.74s[0m
[0m2021.03.02 09:19:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:19:53 INFO  time: compiled root in 0.72s[0m
[0m2021.03.02 09:22:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:22:32 INFO  time: compiled root in 0.71s[0m
[0m2021.03.02 09:26:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:26:02 INFO  time: compiled root in 0.1s[0m
[0m2021.03.02 09:27:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:27:23 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 09:27:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:27:40 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 09:28:14 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:5: stale bloop error: value toArray is not a member of org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
possible cause: maybe a semicolon is missing before `value toArray'?
> rddFromFile
>     .select()
>     .where($"value")
>     .toArray[0m
[0m2021.03.02 09:28:14 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:5: stale bloop error: value toArray is not a member of org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
possible cause: maybe a semicolon is missing before `value toArray'?
> rddFromFile
>     .select()
>     .where($"value")
>     .toArray[0m
[0m2021.03.02 09:28:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:28:16 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 09:28:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:28:45 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 09:28:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:28:48 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 09:28:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:28:57 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 09:29:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:29:05 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 09:29:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:29:11 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 09:30:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:30:21 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 09:30:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:30:38 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 09:30:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:30:42 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 09:31:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:31:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:31:50 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 09:32:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:32:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:32:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:32:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:32:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:32:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:32:06 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:32:06 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:32:10 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:32:10 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:29: stale bloop error: unclosed character literal
    .where(value LIKE '%job%')
                            ^[0m
[0m2021.03.02 09:32:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:32:12 INFO  time: compiled root in 0.64s[0m
[0m2021.03.02 09:35:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:35:24 INFO  time: compiled root in 0.63s[0m
[0m2021.03.02 09:35:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:35:51 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 09:36:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:36:01 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 09:36:06 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:13: stale bloop error: not found: value value
    .filter(value.where("value LIKE '%job%' OR '%tech% OR '%career%' OR '%jobs%'"))
            ^^^^^[0m
[0m2021.03.02 09:36:06 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:13: stale bloop error: not found: value value
    .filter(value.where("value LIKE '%job%' OR '%tech% OR '%career%' OR '%jobs%'"))
            ^^^^^[0m
[0m2021.03.02 09:36:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:36:10 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 09:36:31 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:13: stale bloop error: value where is not a member of org.apache.spark.sql.ColumnName
    .filter($"value".where("value LIKE '%job%' OR '%tech% OR '%career%' OR '%jobs%'"))
            ^^^^^^^^^^^^^^[0m
[0m2021.03.02 09:36:31 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:13: stale bloop error: value where is not a member of org.apache.spark.sql.ColumnName
    .filter($"value".where("value LIKE '%job%' OR '%tech% OR '%career%' OR '%jobs%'"))
            ^^^^^^^^^^^^^^[0m
[0m2021.03.02 09:37:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:37:06 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 09:37:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:37:12 INFO  time: compiled root in 0.67s[0m
[0m2021.03.02 09:39:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:39:10 INFO  time: compiled root in 0.76s[0m
[0m2021.03.02 09:43:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:43:49 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 09:43:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:43:53 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 09:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:13: stale bloop error: value LIKE is not a member of org.apache.spark.sql.ColumnName
    .filter($"value" LIKE reg)
            ^^^^^^^^^^^^^[0m
[0m2021.03.02 09:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:13: stale bloop error: value LIKE is not a member of org.apache.spark.sql.ColumnName
    .filter($"value" LIKE reg)
            ^^^^^^^^^^^^^[0m
[0m2021.03.02 09:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:13: stale bloop error: value LIKE is not a member of org.apache.spark.sql.ColumnName
    .filter($"value" LIKE reg)
            ^^^^^^^^^^^^^[0m
[0m2021.03.02 09:45:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:13: stale bloop error: value LIKE is not a member of org.apache.spark.sql.ColumnName
    .filter($"value" LIKE reg)
            ^^^^^^^^^^^^^[0m
[0m2021.03.02 09:46:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:46:40 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 09:46:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:46:53 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 09:47:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:47:11 INFO  time: compiled root in 0.7s[0m
[0m2021.03.02 09:48:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:48:43 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 09:48:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:5: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> rddFromFile
>     .select()
>     .withColumn(regexp_extract($"value", "%<a%href%job%", 1))[0m
[0m2021.03.02 09:48:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:5: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> rddFromFile
>     .select()
>     .withColumn(regexp_extract($"value", "%<a%href%job%", 1))[0m
[0m2021.03.02 09:48:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:5: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> rddFromFile
>     .select()
>     .withColumn(regexp_extract($"value", "%<a%href%job%", 1))[0m
[0m2021.03.02 09:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:5: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> rddFromFile
>     .select()
>     .withColumn(regexp_extract($"value", "%<a%href%job%", 1))[0m
[0m2021.03.02 09:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:5: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> rddFromFile
>     .select()
>     .withColumn(regexp_extract($"value", "%<a%href%job%", 1))[0m
[0m2021.03.02 09:48:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:51:5: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> rddFromFile
>     .select()
>     .withColumn(regexp_extract($"value", "%<a%href%job%", 1))[0m
[0m2021.03.02 09:49:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:49:02 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 09:49:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:49:08 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 09:49:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:49:28 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 09:49:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:49:51 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 09:50:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:50:06 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 09:50:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:50:35 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 09:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:26: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .withColumn("value", split(regexp_extract($"value", "%<a%href%job%", "$1"+sep)))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 09:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:26: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .withColumn("value", split(regexp_extract($"value", "%<a%href%job%", "$1"+sep)))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 09:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:26: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .withColumn("value", split(regexp_extract($"value", "%<a%href%job%", "$1"+sep)))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 09:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:26: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .withColumn("value", split(regexp_extract($"value", "%<a%href%job%", "$1"+sep)))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 09:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:26: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .withColumn("value", split(regexp_extract($"value", "%<a%href%job%", "$1"+sep)))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 09:50:38 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:53:26: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .withColumn("value", split(regexp_extract($"value", "%<a%href%job%", "$1"+sep)))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 09:50:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:50:40 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 09:51:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:51:00 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 09:51:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:51:10 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 09:52:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:52:02 INFO  time: compiled root in 99ms[0m
[0m2021.03.02 09:52:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:52:05 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 09:52:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:52:56 INFO  time: compiled root in 0.87s[0m
[0m2021.03.02 09:53:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:53:43 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 09:53:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:55: stale bloop error: type mismatch;
 found   : String("$1")
 required: Int
    .select(regexp_extract($"value", "%<a%href%job%", "$1"))
                                                      ^^^^[0m
[0m2021.03.02 09:53:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:55: stale bloop error: type mismatch;
 found   : String("$1")
 required: Int
    .select(regexp_extract($"value", "%<a%href%job%", "$1"))
                                                      ^^^^[0m
[0m2021.03.02 09:53:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:55: stale bloop error: type mismatch;
 found   : String("$1")
 required: Int
    .select(regexp_extract($"value", "%<a%href%job%", "$1"))
                                                      ^^^^[0m
[0m2021.03.02 09:53:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:55: stale bloop error: type mismatch;
 found   : String("$1")
 required: Int
    .select(regexp_extract($"value", "%<a%href%job%", "$1"))
                                                      ^^^^[0m
[0m2021.03.02 09:53:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:55: stale bloop error: type mismatch;
 found   : String("$1")
 required: Int
    .select(regexp_extract($"value", "%<a%href%job%", "$1"))
                                                      ^^^^[0m
[0m2021.03.02 09:53:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:52:55: stale bloop error: type mismatch;
 found   : String("$1")
 required: Int
    .select(regexp_extract($"value", "%<a%href%job%", "$1"))
                                                      ^^^^[0m
[0m2021.03.02 09:53:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:53:51 INFO  time: compiled root in 0.6s[0m
[0m2021.03.02 09:54:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:54:22 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 09:55:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:55:23 INFO  time: compiled root in 0.73s[0m
[0m2021.03.02 09:55:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:55:26 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 09:56:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:56:05 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 09:56:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:56:35 INFO  time: compiled root in 0.74s[0m
[0m2021.03.02 09:57:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:57:00 INFO  time: compiled root in 0.22s[0m
[0m2021.03.02 09:57:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 09:57:39 INFO  time: compiled root in 0.64s[0m
[0m2021.03.02 10:01:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:01:28 INFO  time: compiled root in 0.61s[0m
[0m2021.03.02 10:03:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:03:10 INFO  time: compiled root in 0.67s[0m
[0m2021.03.02 10:03:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:03:43 INFO  time: compiled root in 0.69s[0m
[0m2021.03.02 10:04:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:04:00 INFO  time: compiled root in 0.66s[0m
[0m2021.03.02 10:04:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:04:11 INFO  time: compiled root in 0.66s[0m
[0m2021.03.02 10:06:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:06:56 INFO  time: compiled root in 0.75s[0m
[0m2021.03.02 10:08:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:08:54 INFO  time: compiled root in 0.74s[0m
[0m2021.03.02 10:10:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:10:24 INFO  time: compiled root in 0.82s[0m
[0m2021.03.02 10:10:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:10:59 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 10:11:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:11:04 INFO  time: compiled root in 0.66s[0m
[0m2021.03.02 10:11:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:11:51 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 10:12:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:12:07 INFO  time: compiled root in 0.61s[0m
[0m2021.03.02 10:13:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:13:19 INFO  time: compiled root in 0.76s[0m
[0m2021.03.02 10:13:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:13:53 INFO  time: compiled root in 0.64s[0m
[0m2021.03.02 10:14:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:14:04 INFO  time: compiled root in 0.73s[0m
[0m2021.03.02 10:14:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:14:29 INFO  time: compiled root in 0.89s[0m
[0m2021.03.02 10:14:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:14:46 INFO  time: compiled root in 0.6s[0m
[0m2021.03.02 10:15:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:15:50 INFO  time: compiled root in 0.66s[0m
[0m2021.03.02 10:16:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:16:40 INFO  time: compiled root in 0.59s[0m
[0m2021.03.02 10:18:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:18:24 INFO  time: compiled root in 97ms[0m
[0m2021.03.02 10:18:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:18:27 INFO  time: compiled root in 0.58s[0m
[0m2021.03.02 10:19:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:19:23 INFO  time: compiled root in 93ms[0m
[0m2021.03.02 10:19:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:19:33 INFO  time: compiled root in 0.63s[0m
[0m2021.03.02 10:20:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:20:14 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 10:20:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:20:25 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 10:20:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:20:50 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 10:24:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:24:25 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 10:26:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:26:17 INFO  time: compiled root in 0.67s[0m
[0m2021.03.02 10:27:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:27:45 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 10:29:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:29:09 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 10:29:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:29:29 INFO  time: compiled root in 0.74s[0m
[0m2021.03.02 10:29:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:29:38 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 10:29:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:29:53 INFO  time: compiled root in 0.55s[0m
[0m2021.03.02 10:30:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:30:43 INFO  time: compiled root in 0.65s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.util.{Locale, Properties}

import scala.collection.JavaConverters._

import com.fasterxml.jackson.databind.ObjectMapper
import com.univocity.parsers.csv.CsvParser

import org.apache.spark.Partition
import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.json.{CreateJacksonParser, JacksonParser, JSONOptions}
import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap
import org.apache.spark.sql.execution.command.DDLUtils
import org.apache.spark.sql.execution.datasources.{DataSource, FailureSafeParser}
import org.apache.spark.sql.execution.datasources.csv._
import org.apache.spark.sql.execution.datasources.jdbc._
import org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource
import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation
import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils
import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport}
import org.apache.spark.sql.types.{StringType, StructType}
import org.apache.spark.unsafe.types.UTF8String

/**
 * Interface used to load a [[Dataset]] from external storage systems (e.g. file systems,
 * key-value stores, etc). Use `SparkSession.read` to access this.
 *
 * @since 1.4.0
 */
@InterfaceStability.Stable
class DataFrameReader private[sql](sparkSession: SparkSession) extends Logging {

  /**
   * Specifies the input data source format.
   *
   * @since 1.4.0
   */
  def format(source: String): DataFrameReader = {
    this.source = source
    this
  }

  /**
   * Specifies the input schema. Some data sources (e.g. JSON) can infer the input schema
   * automatically from data. By specifying the schema here, the underlying data source can
   * skip the schema inference step, and thus speed up data loading.
   *
   * @since 1.4.0
   */
  def schema(schema: StructType): DataFrameReader = {
    this.userSpecifiedSchema = Option(schema)
    this
  }

  /**
   * Specifies the schema by using the input DDL-formatted string. Some data sources (e.g. JSON) can
   * infer the input schema automatically from data. By specifying the schema here, the underlying
   * data source can skip the schema inference step, and thus speed up data loading.
   *
   * {{{
   *   spark.read.schema("a INT, b STRING, c DOUBLE").csv("test.csv")
   * }}}
   *
   * @since 2.3.0
   */
  def schema(schemaString: String): DataFrameReader = {
    this.userSpecifiedSchema = Option(StructType.fromDDL(schemaString))
    this
  }

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def option(key: String, value: String): DataFrameReader = {
    this.extraOptions += (key -> value)
    this
  }

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Boolean): DataFrameReader = option(key, value.toString)

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Long): DataFrameReader = option(key, value.toString)

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Double): DataFrameReader = option(key, value.toString)

  /**
   * (Scala-specific) Adds input options for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def options(options: scala.collection.Map[String, String]): DataFrameReader = {
    this.extraOptions ++= options
    this
  }

  /**
   * Adds input options for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def options(options: java.util.Map[String, String]): DataFrameReader = {
    this.options(options.asScala)
    this
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that don't require a path (e.g. external
   * key-value stores).
   *
   * @since 1.4.0
   */
  def load(): DataFrame = {
    load(Seq.empty: _*) // force invocation of `load(...varargs...)`
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that require a path (e.g. data backed by
   * a local or distributed file system).
   *
   * @since 1.4.0
   */
  def load(path: String): DataFrame = {
    // force invocation of `load(...varargs...)`
    option(DataSourceOptions.PATH_KEY, path).load(Seq.empty: _*)
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that support multiple paths.
   * Only works if the source is a HadoopFsRelationProvider.
   *
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def load(paths: String*): DataFrame = {
    if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) {
      throw new AnalysisException("Hive data source can only be used with tables, you can not " +
        "read files of Hive data source directly.")
    }

    val cls = DataSource.lookupDataSource(source, sparkSession.sessionState.conf)
    if (classOf[DataSourceV2].isAssignableFrom(cls)) {
      val ds = cls.newInstance().asInstanceOf[DataSourceV2]
      if (ds.isInstanceOf[ReadSupport]) {
        val sessionOptions = DataSourceV2Utils.extractSessionConfigs(
          ds = ds, conf = sparkSession.sessionState.conf)
        val pathsOption = {
          val objectMapper = new ObjectMapper()
          DataSourceOptions.PATHS_KEY -> objectMapper.writeValueAsString(paths.toArray)
        }
        Dataset.ofRows(sparkSession, DataSourceV2Relation.create(
          ds, sessionOptions ++ extraOptions.toMap + pathsOption,
          userSpecifiedSchema = userSpecifiedSchema))
      } else {
        loadV1Source(paths: _*)
      }
    } else {
      loadV1Source(paths: _*)
    }
  }

  private def loadV1Source(paths: String*) = {
    // Code path for data source v1.
    sparkSession.baseRelationToDataFrame(
      DataSource.apply(
        sparkSession,
        paths = paths,
        userSpecifiedSchema = userSpecifiedSchema,
        className = source,
        options = extraOptions.toMap).resolveRelation())
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table and connection properties.
   *
   * @since 1.4.0
   */
  def jdbc(url: String, table: String, properties: Properties): DataFrame = {
    assertNoSpecifiedSchema("jdbc")
    // properties should override settings in extraOptions.
    this.extraOptions ++= properties.asScala
    // explicit url and dbtable should override all
    this.extraOptions ++= Seq(JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table)
    format("jdbc").load()
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table. Partitions of the table will be retrieved in parallel based on the parameters
   * passed to this function.
   *
   * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
   * your external database systems.
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`.
   * @param table Name of the table in the external database.
   * @param columnName the name of a column of numeric, date, or timestamp type
   *                   that will be used for partitioning.
   * @param lowerBound the minimum value of `columnName` used to decide partition stride.
   * @param upperBound the maximum value of `columnName` used to decide partition stride.
   * @param numPartitions the number of partitions. This, along with `lowerBound` (inclusive),
   *                      `upperBound` (exclusive), form partition strides for generated WHERE
   *                      clause expressions used to split the column `columnName` evenly. When
   *                      the input is less than 1, the number is set to 1.
   * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
   *                             tag/value. Normally at least a "user" and "password" property
   *                             should be included. "fetchsize" can be used to control the
   *                             number of rows per fetch and "queryTimeout" can be used to wait
   *                             for a Statement object to execute to the given number of seconds.
   * @since 1.4.0
   */
  def jdbc(
      url: String,
      table: String,
      columnName: String,
      lowerBound: Long,
      upperBound: Long,
      numPartitions: Int,
      connectionProperties: Properties): DataFrame = {
    // columnName, lowerBound, upperBound and numPartitions override settings in extraOptions.
    this.extraOptions ++= Map(
      JDBCOptions.JDBC_PARTITION_COLUMN -> columnName,
      JDBCOptions.JDBC_LOWER_BOUND -> lowerBound.toString,
      JDBCOptions.JDBC_UPPER_BOUND -> upperBound.toString,
      JDBCOptions.JDBC_NUM_PARTITIONS -> numPartitions.toString)
    jdbc(url, table, connectionProperties)
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table using connection properties. The `predicates` parameter gives a list
   * expressions suitable for inclusion in WHERE clauses; each one defines one partition
   * of the `DataFrame`.
   *
   * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
   * your external database systems.
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`
   * @param table Name of the table in the external database.
   * @param predicates Condition in the where clause for each partition.
   * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
   *                             tag/value. Normally at least a "user" and "password" property
   *                             should be included. "fetchsize" can be used to control the
   *                             number of rows per fetch.
   * @since 1.4.0
   */
  def jdbc(
      url: String,
      table: String,
      predicates: Array[String],
      connectionProperties: Properties): DataFrame = {
    assertNoSpecifiedSchema("jdbc")
    // connectionProperties should override settings in extraOptions.
    val params = extraOptions ++ connectionProperties.asScala
    val options = new JDBCOptions(url, table, params)
    val parts: Array[Partition] = predicates.zipWithIndex.map { case (part, i) =>
      JDBCPartition(part, i) : Partition
    }
    val relation = JDBCRelation(parts, options)(sparkSession)
    sparkSession.baseRelationToDataFrame(relation)
  }

  /**
   * Loads a JSON file and returns the results as a `DataFrame`.
   *
   * See the documentation on the overloaded `json()` method with varargs for more details.
   *
   * @since 1.4.0
   */
  def json(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    json(Seq(path): _*)
  }

  /**
   * Loads JSON files and returns the results as a `DataFrame`.
   *
   * <a href="http://jsonlines.org/">JSON Lines</a> (newline-delimited JSON) is supported by
   * default. For JSON (one record per file), set the `multiLine` option to true.
   *
   * This function goes through the input once to determine the input schema. If you know the
   * schema in advance, use the version that specifies the schema to avoid the extra scan.
   *
   * You can set the following JSON-specific options to deal with non-standard JSON files:
   * <ul>
   * <li>`primitivesAsString` (default `false`): infers all primitive values as a string type</li>
   * <li>`prefersDecimal` (default `false`): infers all floating-point values as a decimal
   * type. If the values do not fit in decimal, then it infers them as doubles.</li>
   * <li>`allowComments` (default `false`): ignores Java/C++ style comment in JSON records</li>
   * <li>`allowUnquotedFieldNames` (default `false`): allows unquoted JSON field names</li>
   * <li>`allowSingleQuotes` (default `true`): allows single quotes in addition to double quotes
   * </li>
   * <li>`allowNumericLeadingZeros` (default `false`): allows leading zeros in numbers
   * (e.g. 00012)</li>
   * <li>`allowBackslashEscapingAnyCharacter` (default `false`): allows accepting quoting of all
   * character using backslash quoting mechanism</li>
   * <li>`allowUnquotedControlChars` (default `false`): allows JSON Strings to contain unquoted
   * control characters (ASCII characters with value less than 32, including tab and line feed
   * characters) or not.</li>
   * <li>`mode` (default `PERMISSIVE`): allows a mode for dealing with corrupt records
   * during parsing.
   *   <ul>
   *     <li>`PERMISSIVE` : when it meets a corrupted record, puts the malformed string into a
   *     field configured by `columnNameOfCorruptRecord`, and sets other fields to `null`. To
   *     keep corrupt records, an user can set a string type field named
   *     `columnNameOfCorruptRecord` in an user-defined schema. If a schema does not have the
   *     field, it drops corrupt records during parsing. When inferring a schema, it implicitly
   *     adds a `columnNameOfCorruptRecord` field in an output schema.</li>
   *     <li>`DROPMALFORMED` : ignores the whole corrupted records.</li>
   *     <li>`FAILFAST` : throws an exception when it meets corrupted records.</li>
   *   </ul>
   * </li>
   * <li>`columnNameOfCorruptRecord` (default is the value specified in
   * `spark.sql.columnNameOfCorruptRecord`): allows renaming the new field having malformed string
   * created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.</li>
   * <li>`dateFormat` (default `yyyy-MM-dd`): sets the string that indicates a date format.
   * Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to
   * date type.</li>
   * <li>`timestampFormat` (default `yyyy-MM-dd'T'HH:mm:ss.SSSXXX`): sets the string that
   * indicates a timestamp format. Custom date formats follow the formats at
   * `java.text.SimpleDateFormat`. This applies to timestamp type.</li>
   * <li>`multiLine` (default `false`): parse one record, which may span multiple lines,
   * per file</li>
   * <li>`encoding` (by default it is not set): allows to forcibly set one of standard basic
   * or extended encoding for the JSON files. For example UTF-16BE, UTF-32LE. If the encoding
   * is not specified and `multiLine` is set to `true`, it will be detected automatically.</li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * <li>`samplingRatio` (default is 1.0): defines fraction of input JSON objects used
   * for schema inferring.</li>
   * <li>`dropFieldIfAllNull` (default `false`): whether to ignore column of all null values or
   * empty array/struct during schema inference.</li>
   * </ul>
   *
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def json(paths: String*): DataFrame = format("json").load(paths : _*)

  /**
   * Loads a `JavaRDD[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON
   * Lines text format or newline-delimited JSON</a>) and returns the result as
   * a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonRDD input RDD with one JSON object per record
   * @since 1.4.0
   */
  @deprecated("Use json(Dataset[String]) instead.", "2.2.0")
  def json(jsonRDD: JavaRDD[String]): DataFrame = json(jsonRDD.rdd)

  /**
   * Loads an `RDD[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON Lines
   * text format or newline-delimited JSON</a>) and returns the result as a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonRDD input RDD with one JSON object per record
   * @since 1.4.0
   */
  @deprecated("Use json(Dataset[String]) instead.", "2.2.0")
  def json(jsonRDD: RDD[String]): DataFrame = {
    json(sparkSession.createDataset(jsonRDD)(Encoders.STRING))
  }

  /**
   * Loads a `Dataset[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON Lines
   * text format or newline-delimited JSON</a>) and returns the result as a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonDataset input Dataset with one JSON object per record
   * @since 2.2.0
   */
  def json(jsonDataset: Dataset[String]): DataFrame = {
    val parsedOptions = new JSONOptions(
      extraOptions.toMap,
      sparkSession.sessionState.conf.sessionLocalTimeZone,
      sparkSession.sessionState.conf.columnNameOfCorruptRecord)

    val schema = userSpecifiedSchema.getOrElse {
      TextInputJsonDataSource.inferFromDataset(jsonDataset, parsedOptions)
    }

    verifyColumnNameOfCorruptRecord(schema, parsedOptions.columnNameOfCorruptRecord)
    val actualSchema =
      StructType(schema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord))

    val createParser = CreateJacksonParser.string _
    val parsed = jsonDataset.rdd.mapPartitions { iter =>
      val rawParser = new JacksonParser(actualSchema, parsedOptions)
      val parser = new FailureSafeParser[String](
        input => rawParser.parse(input, createParser, UTF8String.fromString),
        parsedOptions.parseMode,
        schema,
        parsedOptions.columnNameOfCorruptRecord)
      iter.flatMap(parser.parse)
    }
    sparkSession.internalCreateDataFrame(parsed, schema, isStreaming = jsonDataset.isStreaming)
  }

  /**
   * Loads a CSV file and returns the result as a `DataFrame`. See the documentation on the
   * other overloaded `csv()` method for more details.
   *
   * @since 2.0.0
   */
  def csv(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    csv(Seq(path): _*)
  }

  /**
   * Loads an `Dataset[String]` storing CSV rows and returns the result as a `DataFrame`.
   *
   * If the schema is not specified using `schema` function and `inferSchema` option is enabled,
   * this function goes through the input once to determine the input schema.
   *
   * If the schema is not specified using `schema` function and `inferSchema` option is disabled,
   * it determines the columns as string types and it reads only the first line to determine the
   * names and the number of fields.
   *
   * If the enforceSchema is set to `false`, only the CSV header in the first line is checked
   * to conform specified or inferred schema.
   *
   * @param csvDataset input Dataset with one CSV row per record
   * @since 2.2.0
   */
  def csv(csvDataset: Dataset[String]): DataFrame = {
    val parsedOptions: CSVOptions = new CSVOptions(
      extraOptions.toMap,
      sparkSession.sessionState.conf.csvColumnPruning,
      sparkSession.sessionState.conf.sessionLocalTimeZone)
    val filteredLines: Dataset[String] =
      CSVUtils.filterCommentAndEmpty(csvDataset, parsedOptions)
    val maybeFirstLine: Option[String] = filteredLines.take(1).headOption

    val schema = userSpecifiedSchema.getOrElse {
      TextInputCSVDataSource.inferFromDataset(
        sparkSession,
        csvDataset,
        maybeFirstLine,
        parsedOptions)
    }

    verifyColumnNameOfCorruptRecord(schema, parsedOptions.columnNameOfCorruptRecord)
    val actualSchema =
      StructType(schema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord))

    val linesWithoutHeader = if (parsedOptions.headerFlag && maybeFirstLine.isDefined) {
      val firstLine = maybeFirstLine.get
      val parser = new CsvParser(parsedOptions.asParserSettings)
      val columnNames = parser.parseLine(firstLine)
      CSVDataSource.checkHeaderColumnNames(
        actualSchema,
        columnNames,
        csvDataset.getClass.getCanonicalName,
        parsedOptions.enforceSchema,
        sparkSession.sessionState.conf.caseSensitiveAnalysis)
      filteredLines.rdd.mapPartitions(CSVUtils.filterHeaderLine(_, firstLine, parsedOptions))
    } else {
      filteredLines.rdd
    }

    val parsed = linesWithoutHeader.mapPartitions { iter =>
      val rawParser = new UnivocityParser(actualSchema, parsedOptions)
      val parser = new FailureSafeParser[String](
        input => Seq(rawParser.parse(input)),
        parsedOptions.parseMode,
        schema,
        parsedOptions.columnNameOfCorruptRecord)
      iter.flatMap(parser.parse)
    }
    sparkSession.internalCreateDataFrame(parsed, schema, isStreaming = csvDataset.isStreaming)
  }

  /**
   * Loads CSV files and returns the result as a `DataFrame`.
   *
   * This function will go through the input once to determine the input schema if `inferSchema`
   * is enabled. To avoid going through the entire data once, disable `inferSchema` option or
   * specify the schema explicitly using `schema`.
   *
   * You can set the following CSV-specific options to deal with CSV files:
   * <ul>
   * <li>`sep` (default `,`): sets a single character as a separator for each
   * field and value.</li>
   * <li>`encoding` (default `UTF-8`): decodes the CSV files by the given encoding
   * type.</li>
   * <li>`quote` (default `"`): sets a single character used for escaping quoted values where
   * the separator can be part of the value. If you would like to turn off quotations, you need to
   * set not `null` but an empty string. This behaviour is different from
   * `com.databricks.spark.csv`.</li>
   * <li>`escape` (default `\`): sets a single character used for escaping quotes inside
   * an already quoted value.</li>
   * <li>`charToEscapeQuoteEscaping` (default `escape` or `\0`): sets a single character used for
   * escaping the escape for the quote character. The default value is escape character when escape
   * and quote characters are different, `\0` otherwise.</li>
   * <li>`comment` (default empty string): sets a single character used for skipping lines
   * beginning with this character. By default, it is disabled.</li>
   * <li>`header` (default `false`): uses the first line as names of columns.</li>
   * <li>`enforceSchema` (default `true`): If it is set to `true`, the specified or inferred schema
   * will be forcibly applied to datasource files, and headers in CSV files will be ignored.
   * If the option is set to `false`, the schema will be validated against all headers in CSV files
   * in the case when the `header` option is set to `true`. Field names in the schema
   * and column names in CSV headers are checked by their positions taking into account
   * `spark.sql.caseSensitive`. Though the default value is true, it is recommended to disable
   * the `enforceSchema` option to avoid incorrect results.</li>
   * <li>`inferSchema` (default `false`): infers the input schema automatically from data. It
   * requires one extra pass over the data.</li>
   * <li>`samplingRatio` (default is 1.0): defines fraction of rows used for schema inferring.</li>
   * <li>`ignoreLeadingWhiteSpace` (default `false`): a flag indicating whether or not leading
   * whitespaces from values being read should be skipped.</li>
   * <li>`ignoreTrailingWhiteSpace` (default `false`): a flag indicating whether or not trailing
   * whitespaces from values being read should be skipped.</li>
   * <li>`nullValue` (default empty string): sets the string representation of a null value. Since
   * 2.0.1, this applies to all supported types including the string type.</li>
   * <li>`emptyValue` (default empty string): sets the string representation of an empty value.</li>
   * <li>`nanValue` (default `NaN`): sets the string representation of a non-number" value.</li>
   * <li>`positiveInf` (default `Inf`): sets the string representation of a positive infinity
   * value.</li>
   * <li>`negativeInf` (default `-Inf`): sets the string representation of a negative infinity
   * value.</li>
   * <li>`dateFormat` (default `yyyy-MM-dd`): sets the string that indicates a date format.
   * Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to
   * date type.</li>
   * <li>`timestampFormat` (default `yyyy-MM-dd'T'HH:mm:ss.SSSXXX`): sets the string that
   * indicates a timestamp format. Custom date formats follow the formats at
   * `java.text.SimpleDateFormat`. This applies to timestamp type.</li>
   * <li>`maxColumns` (default `20480`): defines a hard limit of how many columns
   * a record can have.</li>
   * <li>`maxCharsPerColumn` (default `-1`): defines the maximum number of characters allowed
   * for any given value being read. By default, it is -1 meaning unlimited length</li>
   * <li>`mode` (default `PERMISSIVE`): allows a mode for dealing with corrupt records
   *    during parsing. It supports the following case-insensitive modes. Note that Spark tries
   *    to parse only required columns in CSV under column pruning. Therefore, corrupt records
   *    can be different based on required set of fields. This behavior can be controlled by
   *    `spark.sql.csv.parser.columnPruning.enabled` (enabled by default).
   *   <ul>
   *     <li>`PERMISSIVE` : when it meets a corrupted record, puts the malformed string into a
   *     field configured by `columnNameOfCorruptRecord`, and sets other fields to `null`. To keep
   *     corrupt records, an user can set a string type field named `columnNameOfCorruptRecord`
   *     in an user-defined schema. If a schema does not have the field, it drops corrupt records
   *     during parsing. A record with less/more tokens than schema is not a corrupted record to
   *     CSV. When it meets a record having fewer tokens than the length of the schema, sets
   *     `null` to extra fields. When the record has more tokens than the length of the schema,
   *     it drops extra tokens.</li>
   *     <li>`DROPMALFORMED` : ignores the whole corrupted records.</li>
   *     <li>`FAILFAST` : throws an exception when it meets corrupted records.</li>
   *   </ul>
   * </li>
   * <li>`columnNameOfCorruptRecord` (default is the value specified in
   * `spark.sql.columnNameOfCorruptRecord`): allows renaming the new field having malformed string
   * created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.</li>
   * <li>`multiLine` (default `false`): parse one record, which may span multiple lines.</li>
   * </ul>
   *
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def csv(paths: String*): DataFrame = format("csv").load(paths : _*)

  /**
   * Loads a Parquet file, returning the result as a `DataFrame`. See the documentation
   * on the other overloaded `parquet()` method for more details.
   *
   * @since 2.0.0
   */
  def parquet(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    parquet(Seq(path): _*)
  }

  /**
   * Loads a Parquet file, returning the result as a `DataFrame`.
   *
   * You can set the following Parquet-specific option(s) for reading Parquet files:
   * <ul>
   * <li>`mergeSchema` (default is the value specified in `spark.sql.parquet.mergeSchema`): sets
   * whether we should merge schemas collected from all Parquet part-files. This will override
   * `spark.sql.parquet.mergeSchema`.</li>
   * </ul>
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def parquet(paths: String*): DataFrame = {
    format("parquet").load(paths: _*)
  }

  /**
   * Loads an ORC file and returns the result as a `DataFrame`.
   *
   * @param path input path
   * @since 1.5.0
   */
  def orc(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    orc(Seq(path): _*)
  }

  /**
   * Loads ORC files and returns the result as a `DataFrame`.
   *
   * @param paths input paths
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orc(paths: String*): DataFrame = format("orc").load(paths: _*)

  /**
   * Returns the specified table as a `DataFrame`.
   *
   * @since 1.4.0
   */
  def table(tableName: String): DataFrame = {
    assertNoSpecifiedSchema("table")
    sparkSession.table(tableName)
  }

  /**
   * Loads text files and returns a `DataFrame` whose schema starts with a string column named
   * "value", and followed by partitioned columns if there are any. See the documentation on
   * the other overloaded `text()` method for more details.
   *
   * @since 2.0.0
   */
  def text(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    text(Seq(path): _*)
  }

  /**
   * Loads text files and returns a `DataFrame` whose schema starts with a string column named
   * "value", and followed by partitioned columns if there are any.
   *
   * By default, each line in the text files is a new row in the resulting DataFrame. For example:
   * {{{
   *   // Scala:
   *   spark.read.text("/path/to/spark/README.md")
   *
   *   // Java:
   *   spark.read().text("/path/to/spark/README.md")
   * }}}
   *
   * You can set the following text-specific option(s) for reading text files:
   * <ul>
   * <li>`wholetext` (default `false`): If true, read a file as a single row and not split by "\n".
   * </li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * </ul>
   *
   * @param paths input paths
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def text(paths: String*): DataFrame = format("text").load(paths : _*)

  /**
   * Loads text files and returns a [[Dataset]] of String. See the documentation on the
   * other overloaded `textFile()` method for more details.
   * @since 2.0.0
   */
  def textFile(path: String): Dataset[String] = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    textFile(Seq(path): _*)
  }

  /**
   * Loads text files and returns a [[Dataset]] of String. The underlying schema of the Dataset
   * contains a single string column named "value".
   *
   * If the directory structure of the text files contains partitioning information, those are
   * ignored in the resulting Dataset. To include partitioning information as columns, use `text`.
   *
   * By default, each line in the text files is a new row in the resulting DataFrame. For example:
   * {{{
   *   // Scala:
   *   spark.read.textFile("/path/to/spark/README.md")
   *
   *   // Java:
   *   spark.read().textFile("/path/to/spark/README.md")
   * }}}
   *
   * You can set the following textFile-specific option(s) for reading text files:
   * <ul>
   * <li>`wholetext` (default `false`): If true, read a file as a single row and not split by "\n".
   * </li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * </ul>
   *
   * @param paths input path
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def textFile(paths: String*): Dataset[String] = {
    assertNoSpecifiedSchema("textFile")
    text(paths : _*).select("value").as[String](sparkSession.implicits.newStringEncoder)
  }

  /**
   * A convenient function for schema validation in APIs.
   */
  private def assertNoSpecifiedSchema(operation: String): Unit = {
    if (userSpecifiedSchema.nonEmpty) {
      throw new AnalysisException(s"User specified schema not supported with `$operation`")
    }
  }

  /**
   * A convenient function for schema validation in datasources supporting
   * `columnNameOfCorruptRecord` as an option.
   */
  private def verifyColumnNameOfCorruptRecord(
      schema: StructType,
      columnNameOfCorruptRecord: String): Unit = {
    schema.getFieldIndex(columnNameOfCorruptRecord).foreach { corruptFieldIndex =>
      val f = schema(corruptFieldIndex)
      if (f.dataType != StringType || !f.nullable) {
        throw new AnalysisException(
          "The field for corrupt records must be string type and nullable")
      }
    }
  }

  ///////////////////////////////////////////////////////////////////////////////////////
  // Builder pattern config options
  ///////////////////////////////////////////////////////////////////////////////////////

  private var source: String = sparkSession.sessionState.conf.defaultDataSourceName

  private var userSpecifiedSchema: Option[StructType] = None

  private var extraOptions = CaseInsensitiveMap[String](Map.empty)

}

[0m2021.03.02 10:30:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:30:50 INFO  time: compiled root in 0.56s[0m
[0m2021.03.02 10:31:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:31:08 INFO  time: compiled root in 0.73s[0m
[0m2021.03.02 10:31:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:31:17 INFO  time: compiled root in 0.86s[0m
[0m2021.03.02 10:31:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:31:53 INFO  time: compiled root in 0.69s[0m
[0m2021.03.02 10:32:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:32:59 INFO  time: compiled root in 0.69s[0m
[0m2021.03.02 10:33:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:33:59 INFO  time: compiled root in 0.7s[0m
[0m2021.03.02 10:34:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:34:06 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 10:34:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:34:19 INFO  time: compiled root in 0.6s[0m
[0m2021.03.02 10:34:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:34:27 INFO  time: compiled root in 0.7s[0m
[0m2021.03.02 10:35:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:35:24 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 10:36:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:36:06 INFO  time: compiled root in 0.59s[0m
[0m2021.03.02 10:36:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:36:25 INFO  time: compiled root in 0.73s[0m
[0m2021.03.02 10:36:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:36:57 INFO  time: compiled root in 1.03s[0m
[0m2021.03.02 10:38:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:38:10 INFO  time: compiled root in 0.75s[0m
[0m2021.03.02 10:39:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:39:08 INFO  time: compiled root in 0.14s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.Map
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder

/**
 * A collection of implicit methods for converting common Scala objects into [[Dataset]]s.
 *
 * @since 1.6.0
 */
@InterfaceStability.Evolving
abstract class SQLImplicits extends LowPrioritySQLImplicits {

  protected def _sqlContext: SQLContext

  /**
   * Converts $"col name" into a [[Column]].
   *
   * @since 2.0.0
   */
  implicit class StringToColumn(val sc: StringContext) {
    def $(args: Any*): ColumnName = {
      new ColumnName(sc.s(args: _*))
    }
  }

  // Primitives

  /** @since 1.6.0 */
  implicit def newIntEncoder: Encoder[Int] = Encoders.scalaInt

  /** @since 1.6.0 */
  implicit def newLongEncoder: Encoder[Long] = Encoders.scalaLong

  /** @since 1.6.0 */
  implicit def newDoubleEncoder: Encoder[Double] = Encoders.scalaDouble

  /** @since 1.6.0 */
  implicit def newFloatEncoder: Encoder[Float] = Encoders.scalaFloat

  /** @since 1.6.0 */
  implicit def newByteEncoder: Encoder[Byte] = Encoders.scalaByte

  /** @since 1.6.0 */
  implicit def newShortEncoder: Encoder[Short] = Encoders.scalaShort

  /** @since 1.6.0 */
  implicit def newBooleanEncoder: Encoder[Boolean] = Encoders.scalaBoolean

  /** @since 1.6.0 */
  implicit def newStringEncoder: Encoder[String] = Encoders.STRING

  /** @since 2.2.0 */
  implicit def newJavaDecimalEncoder: Encoder[java.math.BigDecimal] = Encoders.DECIMAL

  /** @since 2.2.0 */
  implicit def newScalaDecimalEncoder: Encoder[scala.math.BigDecimal] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newDateEncoder: Encoder[java.sql.Date] = Encoders.DATE

  /** @since 2.2.0 */
  implicit def newTimeStampEncoder: Encoder[java.sql.Timestamp] = Encoders.TIMESTAMP


  // Boxed primitives

  /** @since 2.0.0 */
  implicit def newBoxedIntEncoder: Encoder[java.lang.Integer] = Encoders.INT

  /** @since 2.0.0 */
  implicit def newBoxedLongEncoder: Encoder[java.lang.Long] = Encoders.LONG

  /** @since 2.0.0 */
  implicit def newBoxedDoubleEncoder: Encoder[java.lang.Double] = Encoders.DOUBLE

  /** @since 2.0.0 */
  implicit def newBoxedFloatEncoder: Encoder[java.lang.Float] = Encoders.FLOAT

  /** @since 2.0.0 */
  implicit def newBoxedByteEncoder: Encoder[java.lang.Byte] = Encoders.BYTE

  /** @since 2.0.0 */
  implicit def newBoxedShortEncoder: Encoder[java.lang.Short] = Encoders.SHORT

  /** @since 2.0.0 */
  implicit def newBoxedBooleanEncoder: Encoder[java.lang.Boolean] = Encoders.BOOLEAN

  // Seqs

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newIntSeqEncoder: Encoder[Seq[Int]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newLongSeqEncoder: Encoder[Seq[Long]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newDoubleSeqEncoder: Encoder[Seq[Double]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newFloatSeqEncoder: Encoder[Seq[Float]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newByteSeqEncoder: Encoder[Seq[Byte]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newShortSeqEncoder: Encoder[Seq[Short]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newBooleanSeqEncoder: Encoder[Seq[Boolean]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newStringSeqEncoder: Encoder[Seq[String]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newProductSeqEncoder[A <: Product : TypeTag]: Encoder[Seq[A]] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newSequenceEncoder[T <: Seq[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Maps
  /** @since 2.3.0 */
  implicit def newMapEncoder[T <: Map[_, _] : TypeTag]: Encoder[T] = ExpressionEncoder()

  /**
   * Notice that we serialize `Set` to Catalyst array. The set property is only kept when
   * manipulating the domain objects. The serialization format doesn't keep the set property.
   * When we have a Catalyst array which contains duplicated elements and convert it to
   * `Dataset[Set[T]]` by using the encoder, the elements will be de-duplicated.
   *
   * @since 2.3.0
   */
  implicit def newSetEncoder[T <: Set[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Arrays

  /** @since 1.6.1 */
  implicit def newIntArrayEncoder: Encoder[Array[Int]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newLongArrayEncoder: Encoder[Array[Long]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newDoubleArrayEncoder: Encoder[Array[Double]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newFloatArrayEncoder: Encoder[Array[Float]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newByteArrayEncoder: Encoder[Array[Byte]] = Encoders.BINARY

  /** @since 1.6.1 */
  implicit def newShortArrayEncoder: Encoder[Array[Short]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newBooleanArrayEncoder: Encoder[Array[Boolean]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newStringArrayEncoder: Encoder[Array[String]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newProductArrayEncoder[A <: Product : TypeTag]: Encoder[Array[A]] =
    ExpressionEncoder()

  /**
   * Creates a [[Dataset]] from an RDD.
   *
   * @since 1.6.0
   */
  implicit def rddToDatasetHolder[T : Encoder](rdd: RDD[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(rdd))
  }

  /**
   * Creates a [[Dataset]] from a local Seq.
   * @since 1.6.0
   */
  implicit def localSeqToDatasetHolder[T : Encoder](s: Seq[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(s))
  }

  /**
   * An implicit conversion that turns a Scala `Symbol` into a [[Column]].
   * @since 1.3.0
   */
  implicit def symbolToColumn(s: Symbol): ColumnName = new ColumnName(s.name)

}

/**
 * Lower priority implicit methods for converting Scala objects into [[Dataset]]s.
 * Conflicting implicits are placed here to disambiguate resolution.
 *
 * Reasons for including specific implicits:
 * newProductEncoder - to disambiguate for `List`s which are both `Seq` and `Product`
 */
trait LowPrioritySQLImplicits {
  /** @since 1.6.0 */
  implicit def newProductEncoder[T <: Product : TypeTag]: Encoder[T] = Encoders.product[T]

}

[0m2021.03.02 10:39:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:39:15 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 10:39:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:39:30 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 10:39:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:39:36 INFO  time: compiled root in 0.58s[0m
[0m2021.03.02 10:40:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:40:55 INFO  time: compiled root in 0.59s[0m
[0m2021.03.02 10:41:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:41:49 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 10:42:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:42:20 INFO  time: compiled root in 0.57s[0m
[0m2021.03.02 10:42:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:42:51 INFO  time: compiled root in 0.59s[0m
[0m2021.03.02 10:43:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:43:34 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 10:44:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:44:03 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 10:44:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:44:57 INFO  time: compiled root in 0.7s[0m
[0m2021.03.02 10:45:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:45:53 INFO  time: compiled root in 0.66s[0m
[0m2021.03.02 10:46:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:46:57 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 10:47:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:47:06 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 10:47:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:47:34 INFO  time: compiled root in 0.59s[0m
[0m2021.03.02 10:50:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:50:28 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 10:50:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:50:31 INFO  time: compiled root in 0.61s[0m
[0m2021.03.02 10:52:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:52:04 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 10:52:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:52:57 INFO  time: compiled root in 0.6s[0m
[0m2021.03.02 10:53:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:53:54 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 10:54:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:54:00 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 10:54:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:54:06 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 10:54:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:54:21 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 11:03:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:03:08 INFO  time: compiled root in 0.1s[0m
[0m2021.03.02 11:15:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:15:11 INFO  time: compiled root in 0.1s[0m
[0m2021.03.02 11:15:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:15:29 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 11:15:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:15:34 INFO  time: compiled root in 0.63s[0m
[0m2021.03.02 11:16:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:16:56 INFO  time: compiled root in 0.12s[0m
Mar 02, 2021 11:19:31 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6547
[0m2021.03.02 11:20:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:20:44 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 11:21:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:21:02 INFO  time: compiled root in 0.63s[0m
[0m2021.03.02 11:21:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:21:36 INFO  time: compiled root in 0.63s[0m
[0m2021.03.02 11:21:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:21:46 INFO  time: compiled root in 0.56s[0m
[0m2021.03.02 11:22:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:22:09 INFO  time: compiled root in 0.56s[0m
[0m2021.03.02 11:22:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:22:47 INFO  time: compiled root in 0.64s[0m
[0m2021.03.02 11:29:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:29:22 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 11:29:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:29:31 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 11:29:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:29:44 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 11:29:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:29:57 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 11:30:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:30:03 INFO  time: compiled root in 0.79s[0m
[0m2021.03.02 11:34:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:34:01 INFO  time: compiled root in 0.68s[0m
Mar 02, 2021 11:34:43 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7070
[0m2021.03.02 11:34:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:34:47 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 11:35:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:35:09 INFO  time: compiled root in 0.72s[0m
[0m2021.03.02 11:36:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:36:19 INFO  time: compiled root in 0.7s[0m
[0m2021.03.02 11:36:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:36:26 INFO  time: compiled root in 0.82s[0m
[0m2021.03.02 11:40:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:40:38 INFO  time: compiled root in 0.85s[0m
[0m2021.03.02 11:49:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:49:26 INFO  time: compiled root in 1.09s[0m
[0m2021.03.02 11:50:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:50:47 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 11:50:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:50:51 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 11:50:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:50:55 INFO  time: compiled root in 0.67s[0m
[0m2021.03.02 11:52:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:52:13 INFO  time: compiled root in 0.56s[0m
[0m2021.03.02 11:52:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:52:16 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 11:53:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:53:44 INFO  time: compiled root in 0.59s[0m
[0m2021.03.02 11:53:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:53:49 INFO  time: compiled root in 0.54s[0m
[0m2021.03.02 11:53:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:53:51 INFO  time: compiled root in 0.57s[0m
[0m2021.03.02 11:54:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:54:44 INFO  time: compiled root in 0.64s[0m
[0m2021.03.02 12:02:55 INFO  shutting down Metals[0m
[0m2021.03.02 12:02:55 INFO  Shut down connection with build server.[0m
[0m2021.03.02 12:02:55 INFO  Shut down connection with build server.[0m
[0m2021.03.02 12:02:55 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.02 12:38:01 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.53.2.[0m
[0m2021.03.02 12:38:02 INFO  time: initialize in 0.48s[0m
[0m2021.03.02 12:38:02 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.02 12:38:02 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher390828301782089496/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.02 12:38:02 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.02 12:38:05 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions


object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?


//warc file paths
    //"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/warc.paths.gz"
//wet file paths
    //"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz"

    //"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00000.warc.gz"
    //"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00000.warc.wet.gz"
    val s3bucket = spark.read.json(
      "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    )

    //"Container", "Envelope", "_corrupt_record"
    s3bucket.printSchema()
    // s3bucket.show(false)
  
    // s3bucket
    // .filter(_.contains("job"))
    // .show()

    // val reg = "%<a%href%job%".r
    
    
    

  }
}

[0m2021.03.02 12:38:06 INFO  time: code lens generation in 3.61s[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher390828301782089496/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher390828301782089496/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.02 12:38:06 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 12:38:06 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.02 12:38:06 Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4050323436262897851/bsp.socket'...INFO  Attempting to connect to the build server...
[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6221385158832249749/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4050323436262897851/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4050323436262897851/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6221385158832249749/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6221385158832249749/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.02 12:38:07 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 12:38:07 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 12:38:07 INFO  time: Connected to build server in 4.72s[0m
[0m2021.03.02 12:38:07 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.02 12:38:07 INFO  time: Imported build in 0.22s[0m
[0m2021.03.02 12:38:09 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.02 12:38:09 INFO  time: indexed workspace in 2.27s[0m
[0m2021.03.02 12:41:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:41:32 INFO  time: compiled root in 1.98s[0m
[0m2021.03.02 12:41:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:41:43 INFO  time: compiled root in 0.45s[0m
[0m2021.03.02 12:42:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:42:04 INFO  time: compiled root in 2.23s[0m
[0m2021.03.02 12:43:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:43:08 INFO  time: compiled root in 1.23s[0m
[0m2021.03.02 12:43:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:43:11 INFO  time: compiled root in 1.23s[0m
[0m2021.03.02 12:43:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:43:47 INFO  time: compiled root in 0.94s[0m
[0m2021.03.02 12:44:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:44:37 INFO  time: compiled root in 0.81s[0m
[0m2021.03.02 12:45:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:45:11 INFO  time: compiled root in 0.72s[0m
[0m2021.03.02 12:45:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:45:41 INFO  time: compiled root in 0.7s[0m
[0m2021.03.02 12:45:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:45:47 INFO  time: compiled root in 0.75s[0m
[0m2021.03.02 12:46:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:46:26 INFO  time: compiled root in 0.84s[0m
Mar 02, 2021 12:47:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 295
Mar 02, 2021 12:47:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 293
[0m2021.03.02 12:47:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:47:57 INFO  time: compiled root in 0.91s[0m
[0m2021.03.02 12:48:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:48:05 INFO  time: compiled root in 1.07s[0m
[0m2021.03.02 12:50:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:50:10 INFO  time: compiled root in 0.98s[0m
[0m2021.03.02 12:52:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:52:02 INFO  time: compiled root in 0.8s[0m
[0m2021.03.02 12:57:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:57:10 INFO  time: compiled root in 0.81s[0m
[0m2021.03.02 13:01:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:01:22 INFO  time: compiled root in 0.83s[0m
[0m2021.03.02 13:03:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:03:42 INFO  time: compiled root in 0.23s[0m
[0m2021.03.02 13:03:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:03:47 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 13:04:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:04:54 INFO  time: compiled root in 0.69s[0m
[0m2021.03.02 13:07:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:07:22 INFO  time: compiled root in 0.79s[0m
[0m2021.03.02 13:07:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:07:51 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 13:08:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:08:55 INFO  time: compiled root in 0.72s[0m
[0m2021.03.02 13:09:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:09:27 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 13:09:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:09:43 INFO  time: compiled root in 0.71s[0m
[0m2021.03.02 13:11:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:11:15 INFO  time: compiled root in 0.82s[0m
[0m2021.03.02 13:13:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:13:21 INFO  time: compiled root in 0.84s[0m
[0m2021.03.02 13:14:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:14:34 INFO  time: compiled root in 0.96s[0m
[0m2021.03.02 13:14:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:14:37 INFO  time: compiled root in 0.67s[0m
[0m2021.03.02 13:14:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:14:39 INFO  time: compiled root in 0.69s[0m
[0m2021.03.02 13:14:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:14:49 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 13:15:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:15:27 INFO  time: compiled root in 0.67s[0m
[0m2021.03.02 13:16:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:16:46 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 13:17:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:17:50 INFO  time: compiled root in 0.77s[0m
[0m2021.03.02 13:19:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:19:24 INFO  time: compiled root in 0.76s[0m
[0m2021.03.02 13:20:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:20:29 INFO  time: compiled root in 0.71s[0m
[0m2021.03.02 13:20:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:20:36 INFO  time: compiled root in 0.72s[0m
[0m2021.03.02 13:21:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:21:07 INFO  time: compiled root in 0.76s[0m
[0m2021.03.02 13:21:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:21:28 INFO  time: compiled root in 0.69s[0m
[0m2021.03.02 13:22:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:22:16 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 13:22:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:22:29 INFO  time: compiled root in 0.75s[0m
[0m2021.03.02 13:24:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:24:24 INFO  time: compiled root in 0.99s[0m
Mar 02, 2021 1:24:54 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2074
[0m2021.03.02 13:24:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:24:55 INFO  time: compiled root in 0.81s[0m
[0m2021.03.02 13:25:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:25:35 INFO  time: compiled root in 0.81s[0m
[0m2021.03.02 13:27:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:27:58 INFO  time: compiled root in 0.78s[0m
[0m2021.03.02 13:31:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:31:32 INFO  time: compiled root in 0.79s[0m
[0m2021.03.02 13:33:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:33:11 INFO  time: compiled root in 1.09s[0m
[0m2021.03.02 13:34:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:34:45 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 13:34:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:34:50 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 13:34:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:34:53 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 13:35:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:35:31 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 13:36:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:36:13 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 13:36:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:36:21 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 13:36:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:36:44 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 13:36:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:36:47 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 13:36:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:36:50 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 13:36:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:36:55 INFO  time: compiled root in 0.19s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.encoders.{encoderFor, ExpressionEncoder}
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
import org.apache.spark.sql.catalyst.util.toPrettySQL
import org.apache.spark.sql.execution.aggregate.TypedAggregateExpression
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.types._

private[sql] object Column {

  def apply(colName: String): Column = new Column(colName)

  def apply(expr: Expression): Column = new Column(expr)

  def unapply(col: Column): Option[Expression] = Some(col.expr)

  private[sql] def generateAlias(e: Expression): String = {
    e match {
      case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
        a.aggregateFunction.toString
      case expr => toPrettySQL(expr)
    }
  }
}

/**
 * A [[Column]] where an [[Encoder]] has been given for the expected input and return type.
 * To create a [[TypedColumn]], use the `as` function on a [[Column]].
 *
 * @tparam T The input type expected for this expression.  Can be `Any` if the expression is type
 *           checked by the analyzer instead of the compiler (i.e. `expr("sum(...)")`).
 * @tparam U The output type of this column.
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class TypedColumn[-T, U](
    expr: Expression,
    private[sql] val encoder: ExpressionEncoder[U])
  extends Column(expr) {

  /**
   * Inserts the specific input type and schema into any expressions that are expected to operate
   * on a decoded object.
   */
  private[sql] def withInputType(
      inputEncoder: ExpressionEncoder[_],
      inputAttributes: Seq[Attribute]): TypedColumn[T, U] = {
    val unresolvedDeserializer = UnresolvedDeserializer(inputEncoder.deserializer, inputAttributes)
    val newExpr = expr transform {
      case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty =>
        ta.withInputInfo(
          deser = unresolvedDeserializer,
          cls = inputEncoder.clsTag.runtimeClass,
          schema = inputEncoder.schema)
    }
    new TypedColumn[T, U](newExpr, encoder)
  }

  /**
   * Gives the [[TypedColumn]] a name (alias).
   * If the current `TypedColumn` has metadata associated with it, this metadata will be propagated
   * to the new column.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  override def name(alias: String): TypedColumn[T, U] =
    new TypedColumn[T, U](super.name(alias).expr, encoder)

}

/**
 * A column that will be computed based on the data in a `DataFrame`.
 *
 * A new column can be constructed based on the input columns present in a DataFrame:
 *
 * {{{
 *   df("columnName")            // On a specific `df` DataFrame.
 *   col("columnName")           // A generic column not yet associated with a DataFrame.
 *   col("columnName.field")     // Extracting a struct field
 *   col("`a.column.with.dots`") // Escape `.` in column names.
 *   $"columnName"               // Scala short hand for a named column.
 * }}}
 *
 * [[Column]] objects can be composed to form complex expressions:
 *
 * {{{
 *   $"a" + 1
 *   $"a" === $"b"
 * }}}
 *
 * @note The internal Catalyst expression can be accessed via [[expr]], but this method is for
 * debugging purposes only and can change in any future Spark releases.
 *
 * @groupname java_expr_ops Java-specific expression operators
 * @groupname expr_ops Expression operators
 * @groupname df_ops DataFrame functions
 * @groupname Ungrouped Support functions for DataFrames
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class Column(val expr: Expression) extends Logging {

  def this(name: String) = this(name match {
    case "*" => UnresolvedStar(None)
    case _ if name.endsWith(".*") =>
      val parts = UnresolvedAttribute.parseAttributeName(name.substring(0, name.length - 2))
      UnresolvedStar(Some(parts))
    case _ => UnresolvedAttribute.quotedString(name)
  })

  override def toString: String = toPrettySQL(expr)

  override def equals(that: Any): Boolean = that match {
    case that: Column => that.expr.equals(this.expr)
    case _ => false
  }

  override def hashCode: Int = this.expr.hashCode()

  /** Creates a column based on the given expression. */
  private def withExpr(newExpr: Expression): Column = new Column(newExpr)

  /**
   * Returns the expression for this column either with an existing or auto assigned name.
   */
  private[sql] def named: NamedExpression = expr match {
    // Wrap UnresolvedAttribute with UnresolvedAlias, as when we resolve UnresolvedAttribute, we
    // will remove intermediate Alias for ExtractValue chain, and we need to alias it again to
    // make it a NamedExpression.
    case u: UnresolvedAttribute => UnresolvedAlias(u)

    case u: UnresolvedExtractValue => UnresolvedAlias(u)

    case expr: NamedExpression => expr

    // Leave an unaliased generator with an empty list of names since the analyzer will generate
    // the correct defaults after the nested expression's type has been resolved.
    case g: Generator => MultiAlias(g, Nil)

    case func: UnresolvedFunction => UnresolvedAlias(func, Some(Column.generateAlias))

    // If we have a top level Cast, there is a chance to give it a better alias, if there is a
    // NamedExpression under this Cast.
    case c: Cast =>
      c.transformUp {
        case c @ Cast(_: NamedExpression, _, _) => UnresolvedAlias(c)
      } match {
        case ne: NamedExpression => ne
        case _ => Alias(expr, toPrettySQL(expr))()
      }

    case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
      UnresolvedAlias(a, Some(Column.generateAlias))

    // Wait until the struct is resolved. This will generate a nicer looking alias.
    case struct: CreateNamedStructLike => UnresolvedAlias(struct)

    case expr: Expression => Alias(expr, toPrettySQL(expr))()
  }

  /**
   * Provides a type hint about the expected return value of this column.  This information can
   * be used by operations such as `select` on a [[Dataset]] to automatically convert the
   * results into the correct JVM types.
   * @since 1.6.0
   */
  def as[U : Encoder]: TypedColumn[Any, U] = new TypedColumn[Any, U](expr, encoderFor[U])

  /**
   * Extracts a value or values from a complex type.
   * The following types of extraction are supported:
   * <ul>
   * <li>Given an Array, an integer ordinal can be used to retrieve a single value.</li>
   * <li>Given a Map, a key of the correct type can be used to retrieve an individual value.</li>
   * <li>Given a Struct, a string fieldName can be used to extract that field.</li>
   * <li>Given an Array of Structs, a string fieldName can be used to extract filed
   *    of every struct in that array, and return an Array of fields.</li>
   * </ul>
   * @group expr_ops
   * @since 1.4.0
   */
  def apply(extraction: Any): Column = withExpr {
    UnresolvedExtractValue(expr, lit(extraction).expr)
  }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Scala: select the amount column and negates all values.
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.select( negate(col("amount") );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_- : Column = withExpr { UnaryMinus(expr) }

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_! : Column = withExpr { Not(expr) }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def === (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} = $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualTo(expr, right)
  }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def equalTo(other: Any): Column = this === other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") =!= df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
    */
  def =!= (other: Any): Column = withExpr{ Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
    */
  @deprecated("!== does not have the same precedence as ===, use =!= instead", "2.0.0")
  def !== (other: Any): Column = this =!= other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def notEqual(other: Any): Column = withExpr { Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > 21 )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def > (other: Any): Column = withExpr { GreaterThan(expr, lit(other).expr) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > lit(21) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def gt(other: Any): Column = this > other

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def < (other: Any): Column = withExpr { LessThan(expr, lit(other).expr) }

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def lt(other: Any): Column = this < other

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <= (other: Any): Column = withExpr { LessThanOrEqual(expr, lit(other).expr) }

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def leq(other: Any): Column = this <= other

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def >= (other: Any): Column = withExpr { GreaterThanOrEqual(expr, lit(other).expr) }

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def geq(other: Any): Column = this >= other

  /**
   * Equality test that is safe for null values.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <=> (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} <=> $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualNullSafe(expr, right)
  }

  /**
   * Equality test that is safe for null values.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def eqNullSafe(other: Any): Column = this <=> other

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches :+ ((condition.expr, lit(value).expr))) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "when() cannot be applied once otherwise() is applied")
    case _ =>
      throw new IllegalArgumentException(
        "when() can only be applied on a Column previously generated by when() function")
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def otherwise(value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches, Option(lit(value).expr)) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied once on a Column previously generated by when()")
    case _ =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied on a Column previously generated by when()")
  }

  /**
   * True if the current column is between the lower bound and upper bound, inclusive.
   *
   * @group java_expr_ops
   * @since 1.4.0
   */
  def between(lowerBound: Any, upperBound: Any): Column = {
    (this >= lowerBound) && (this <= upperBound)
  }

  /**
   * True if the current expression is NaN.
   *
   * @group expr_ops
   * @since 1.5.0
   */
  def isNaN: Column = withExpr { IsNaN(expr) }

  /**
   * True if the current expression is null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNull: Column = withExpr { IsNull(expr) }

  /**
   * True if the current expression is NOT null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNotNull: Column = withExpr { IsNotNull(expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def || (other: Any): Column = withExpr { Or(expr, lit(other).expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def or(other: Column): Column = this || other

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def && (other: Any): Column = withExpr { And(expr, lit(other).expr) }

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def and(other: Column): Column = this && other

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def + (other: Any): Column = withExpr { Add(expr, lit(other).expr) }

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def plus(other: Any): Column = this + other

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def - (other: Any): Column = withExpr { Subtract(expr, lit(other).expr) }

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def minus(other: Any): Column = this - other

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def * (other: Any): Column = withExpr { Multiply(expr, lit(other).expr) }

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def multiply(other: Any): Column = this * other

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def / (other: Any): Column = withExpr { Divide(expr, lit(other).expr) }

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def divide(other: Any): Column = this / other

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def % (other: Any): Column = withExpr { Remainder(expr, lit(other).expr) }

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def mod(other: Any): Column = this % other

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the evaluated values of the arguments.
   *
   * Note: Since the type of the elements in the list are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def isin(list: Any*): Column = withExpr { In(expr, list.map(lit(_).expr)) }

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: scala.collection.Iterable[_]): Column = isin(values.toSeq: _*)

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group java_expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: java.lang.Iterable[_]): Column = isInCollection(values.asScala)

  /**
   * SQL like expression. Returns a boolean column based on a SQL LIKE match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def like(literal: String): Column = withExpr { Like(expr, lit(literal).expr) }

  /**
   * SQL RLIKE expression (LIKE with Regex). Returns a boolean column based on a regex
   * match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def rlike(literal: String): Column = withExpr { RLike(expr, lit(literal).expr) }

  /**
   * An expression that gets an item at position `ordinal` out of an array,
   * or gets a value by key `key` in a `MapType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getItem(key: Any): Column = withExpr { UnresolvedExtractValue(expr, Literal(key)) }

  /**
   * An expression that gets a field by name in a `StructType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getField(fieldName: String): Column = withExpr {
    UnresolvedExtractValue(expr, Literal(fieldName))
  }

  /**
   * An expression that returns a substring.
   * @param startPos expression for the starting position.
   * @param len expression for the length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Column, len: Column): Column = withExpr {
    Substring(expr, startPos.expr, len.expr)
  }

  /**
   * An expression that returns a substring.
   * @param startPos starting position.
   * @param len length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Int, len: Int): Column = withExpr {
    Substring(expr, lit(startPos).expr, lit(len).expr)
  }

  /**
   * Contains the other element. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def contains(other: Any): Column = withExpr { Contains(expr, lit(other).expr) }

  /**
   * String starts with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(other: Column): Column = withExpr { StartsWith(expr, lit(other).expr) }

  /**
   * String starts with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(literal: String): Column = this.startsWith(lit(literal))

  /**
   * String ends with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(other: Column): Column = withExpr { EndsWith(expr, lit(other).expr) }

  /**
   * String ends with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(literal: String): Column = this.endsWith(lit(literal))

  /**
   * Gives the column an alias. Same as `as`.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".alias("colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def alias(alias: String): Column = name(alias)

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String): Column = name(alias)

  /**
   * (Scala-specific) Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Seq[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Array[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as('colB))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: Symbol): Column = name(alias.name)

  /**
   * Gives the column an alias with metadata.
   * {{{
   *   val metadata: Metadata = ...
   *   df.select($"colA".as("colB", metadata))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String, metadata: Metadata): Column = withExpr {
    Alias(expr, alias)(explicitMetadata = Some(metadata))
  }

  /**
   * Gives the column a name (alias).
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".name("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def name(alias: String): Column = withExpr {
    expr match {
      case ne: NamedExpression => Alias(expr, alias)(explicitMetadata = Some(ne.metadata))
      case other => Alias(other, alias)()
    }
  }

  /**
   * Casts the column to a different data type.
   * {{{
   *   // Casts colA to IntegerType.
   *   import org.apache.spark.sql.types.IntegerType
   *   df.select(df("colA").cast(IntegerType))
   *
   *   // equivalent to
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: DataType): Column = withExpr { Cast(expr, to) }

  /**
   * Casts the column to a different data type, using the canonical string representation
   * of the type. The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
   * `float`, `double`, `decimal`, `date`, `timestamp`.
   * {{{
   *   // Casts colA to integer.
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: String): Column = cast(CatalystSqlParser.parseDataType(to))

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   // Scala
   *   df.sort(df("age").desc)
   *
   *   // Java
   *   df.sort(df.col("age").desc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def desc: Column = withExpr { SortOrder(expr, Descending) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing first.
   *   df.sort(df("age").desc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_first: Column = withExpr { SortOrder(expr, Descending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing last.
   *   df.sort(df("age").desc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_last: Column = withExpr { SortOrder(expr, Descending, NullsLast, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order.
   *   df.sort(df("age").asc)
   *
   *   // Java
   *   df.sort(df.col("age").asc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def asc: Column = withExpr { SortOrder(expr, Ascending) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing first.
   *   df.sort(df("age").asc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_first: Column = withExpr { SortOrder(expr, Ascending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing last.
   *   df.sort(df("age").asc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_last: Column = withExpr { SortOrder(expr, Ascending, NullsLast, Set.empty) }

  /**
   * Prints the expression to the console for debugging purposes.
   *
   * @group df_ops
   * @since 1.3.0
   */
  def explain(extended: Boolean): Unit = {
    // scalastyle:off println
    if (extended) {
      println(expr)
    } else {
      println(expr.sql)
    }
    // scalastyle:on println
  }

  /**
   * Compute bitwise OR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseOR(other: Any): Column = withExpr { BitwiseOr(expr, lit(other).expr) }

  /**
   * Compute bitwise AND of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseAND($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseAND(other: Any): Column = withExpr { BitwiseAnd(expr, lit(other).expr) }

  /**
   * Compute bitwise XOR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseXOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseXOR(other: Any): Column = withExpr { BitwiseXor(expr, lit(other).expr) }

  /**
   * Defines a windowing column.
   *
   * {{{
   *   val w = Window.partitionBy("name").orderBy("id")
   *   df.select(
   *     sum("price").over(w.rangeBetween(Window.unboundedPreceding, 2)),
   *     avg("price").over(w.rowsBetween(Window.currentRow, 4))
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def over(window: expressions.WindowSpec): Column = window.withAggregate(this)

  /**
   * Defines an empty analytic clause. In this case the analytic function is applied
   * and presented for all rows in the result set.
   *
   * {{{
   *   df.select(
   *     sum("price").over(),
   *     avg("price").over()
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def over(): Column = over(Window.spec)

}


/**
 * A convenient class used for constructing schema.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class ColumnName(name: String) extends Column(name) {

  /**
   * Creates a new `StructField` of type boolean.
   * @since 1.3.0
   */
  def boolean: StructField = StructField(name, BooleanType)

  /**
   * Creates a new `StructField` of type byte.
   * @since 1.3.0
   */
  def byte: StructField = StructField(name, ByteType)

  /**
   * Creates a new `StructField` of type short.
   * @since 1.3.0
   */
  def short: StructField = StructField(name, ShortType)

  /**
   * Creates a new `StructField` of type int.
   * @since 1.3.0
   */
  def int: StructField = StructField(name, IntegerType)

  /**
   * Creates a new `StructField` of type long.
   * @since 1.3.0
   */
  def long: StructField = StructField(name, LongType)

  /**
   * Creates a new `StructField` of type float.
   * @since 1.3.0
   */
  def float: StructField = StructField(name, FloatType)

  /**
   * Creates a new `StructField` of type double.
   * @since 1.3.0
   */
  def double: StructField = StructField(name, DoubleType)

  /**
   * Creates a new `StructField` of type string.
   * @since 1.3.0
   */
  def string: StructField = StructField(name, StringType)

  /**
   * Creates a new `StructField` of type date.
   * @since 1.3.0
   */
  def date: StructField = StructField(name, DateType)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal: StructField = StructField(name, DecimalType.USER_DEFAULT)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal(precision: Int, scale: Int): StructField =
    StructField(name, DecimalType(precision, scale))

  /**
   * Creates a new `StructField` of type timestamp.
   * @since 1.3.0
   */
  def timestamp: StructField = StructField(name, TimestampType)

  /**
   * Creates a new `StructField` of type binary.
   * @since 1.3.0
   */
  def binary: StructField = StructField(name, BinaryType)

  /**
   * Creates a new `StructField` of type array.
   * @since 1.3.0
   */
  def array(dataType: DataType): StructField = StructField(name, ArrayType(dataType))

  /**
   * Creates a new `StructField` of type map.
   * @since 1.3.0
   */
  def map(keyType: DataType, valueType: DataType): StructField =
    map(MapType(keyType, valueType))

  def map(mapType: MapType): StructField = StructField(name, mapType)

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(fields: StructField*): StructField = struct(StructType(fields))

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(structType: StructType): StructField = StructField(name, structType)
}

[0m2021.03.02 13:38:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:38:06 INFO  time: compiled root in 0.74s[0m
[0m2021.03.02 13:39:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:39:04 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 13:40:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:40:36 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 13:41:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:41:10 INFO  time: compiled root in 0.66s[0m
Mar 02, 2021 1:41:41 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2803
[0m2021.03.02 13:41:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:41:44 INFO  time: compiled root in 0.69s[0m
[0m2021.03.02 13:41:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:41:48 INFO  time: compiled root in 0.69s[0m
[0m2021.03.02 13:42:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:42:00 INFO  time: compiled root in 0.77s[0m
[0m2021.03.02 13:42:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:42:03 INFO  time: compiled root in 0.76s[0m
[0m2021.03.02 13:42:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:42:36 INFO  time: compiled root in 0.79s[0m
[0m2021.03.02 13:43:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:43:32 INFO  time: compiled root in 0.74s[0m
[0m2021.03.02 13:43:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:43:55 INFO  time: compiled root in 0.92s[0m
[0m2021.03.02 13:44:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:44:24 INFO  time: compiled root in 0.79s[0m
[0m2021.03.02 13:44:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:44:50 INFO  time: compiled root in 0.76s[0m
[0m2021.03.02 13:45:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:45:35 INFO  time: compiled root in 0.73s[0m
[0m2021.03.02 13:46:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:46:13 INFO  time: compiled root in 0.76s[0m
[0m2021.03.02 13:46:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:46:58 INFO  time: compiled root in 0.79s[0m
[0m2021.03.02 13:47:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:47:18 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 13:49:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:49:33 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 13:50:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:50:14 INFO  time: compiled root in 0.72s[0m
[0m2021.03.02 13:51:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:51:43 INFO  time: compiled root in 1s[0m
[0m2021.03.02 13:53:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 13:53:20 INFO  time: compiled root in 0.78s[0m
[0m2021.03.02 14:00:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:00:58 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 14:01:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:01:51 INFO  time: compiled root in 0.66s[0m
[0m2021.03.02 14:02:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:02:37 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 14:03:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:03:19 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 14:11:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:11:16 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 14:12:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:12:00 INFO  time: compiled root in 0.67s[0m
[0m2021.03.02 14:14:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:14:09 INFO  time: compiled root in 0.58s[0m
[0m2021.03.02 14:15:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:15:09 INFO  time: compiled root in 0.63s[0m
[0m2021.03.02 14:15:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:15:34 INFO  time: compiled root in 0.58s[0m
[0m2021.03.02 14:17:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:17:56 INFO  time: compiled root in 0.64s[0m
[0m2021.03.02 14:18:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:18:48 INFO  time: compiled root in 0.6s[0m
[0m2021.03.02 14:20:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:20:10 INFO  time: compiled root in 0.67s[0m
[0m2021.03.02 14:21:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:21:32 INFO  time: compiled root in 0.69s[0m
[0m2021.03.02 14:25:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:25:34 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 14:25:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:25:38 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 14:25:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:25:41 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 14:25:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:20: stale bloop error: overloaded method value filter with alternatives:
  (func: org.apache.spark.api.java.function.FilterFunction[String])org.apache.spark.sql.Dataset[String] <and>
  (func: String => Boolean)org.apache.spark.sql.Dataset[String] <and>
  (conditionExpr: String)org.apache.spark.sql.Dataset[String] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[String]
 cannot be applied to (scala.util.matching.Regex)
> s3bucket
>       .filter($"value".contains("<"))
>       .filter[0m
[0m2021.03.02 14:25:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:20: stale bloop error: overloaded method value filter with alternatives:
  (func: org.apache.spark.api.java.function.FilterFunction[String])org.apache.spark.sql.Dataset[String] <and>
  (func: String => Boolean)org.apache.spark.sql.Dataset[String] <and>
  (conditionExpr: String)org.apache.spark.sql.Dataset[String] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[String]
 cannot be applied to (scala.util.matching.Regex)
> s3bucket
>       .filter($"value".contains("<"))
>       .filter[0m
[0m2021.03.02 14:25:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:20: stale bloop error: overloaded method value filter with alternatives:
  (func: org.apache.spark.api.java.function.FilterFunction[String])org.apache.spark.sql.Dataset[String] <and>
  (func: String => Boolean)org.apache.spark.sql.Dataset[String] <and>
  (conditionExpr: String)org.apache.spark.sql.Dataset[String] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[String]
 cannot be applied to (scala.util.matching.Regex)
> s3bucket
>       .filter($"value".contains("<"))
>       .filter[0m
[0m2021.03.02 14:25:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:20: stale bloop error: overloaded method value filter with alternatives:
  (func: org.apache.spark.api.java.function.FilterFunction[String])org.apache.spark.sql.Dataset[String] <and>
  (func: String => Boolean)org.apache.spark.sql.Dataset[String] <and>
  (conditionExpr: String)org.apache.spark.sql.Dataset[String] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[String]
 cannot be applied to (scala.util.matching.Regex)
> s3bucket
>       .filter($"value".contains("<"))
>       .filter[0m
[0m2021.03.02 14:25:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:20: stale bloop error: overloaded method value filter with alternatives:
  (func: org.apache.spark.api.java.function.FilterFunction[String])org.apache.spark.sql.Dataset[String] <and>
  (func: String => Boolean)org.apache.spark.sql.Dataset[String] <and>
  (conditionExpr: String)org.apache.spark.sql.Dataset[String] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[String]
 cannot be applied to (scala.util.matching.Regex)
> s3bucket
>       .filter($"value".contains("<"))
>       .filter[0m
[0m2021.03.02 14:25:48 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:50:20: stale bloop error: overloaded method value filter with alternatives:
  (func: org.apache.spark.api.java.function.FilterFunction[String])org.apache.spark.sql.Dataset[String] <and>
  (func: String => Boolean)org.apache.spark.sql.Dataset[String] <and>
  (conditionExpr: String)org.apache.spark.sql.Dataset[String] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[String]
 cannot be applied to (scala.util.matching.Regex)
> s3bucket
>       .filter($"value".contains("<"))
>       .filter[0m
[0m2021.03.02 14:26:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:26:25 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 14:27:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:27:06 INFO  time: compiled root in 0.62s[0m
[0m2021.03.02 14:28:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:28:53 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 14:30:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:30:31 INFO  time: compiled root in 0.69s[0m
[0m2021.03.02 14:33:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:33:16 INFO  time: compiled root in 0.68s[0m
[0m2021.03.02 14:34:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:34:07 INFO  time: compiled root in 0.65s[0m
[0m2021.03.02 14:37:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:37:59 INFO  time: compiled root in 0.64s[0m
[0m2021.03.02 14:40:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:40:56 INFO  time: compiled root in 0.61s[0m
[0m2021.03.02 14:41:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:41:08 INFO  time: compiled root in 0.81s[0m
[0m2021.03.02 14:42:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:42:39 INFO  time: compiled root in 0.7s[0m
[0m2021.03.02 14:45:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:45:47 INFO  time: compiled root in 0.73s[0m
[0m2021.03.02 14:45:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:45:54 INFO  time: compiled root in 0.66s[0m
[0m2021.03.02 19:14:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:14:05 INFO  time: compiled root in 4.52s[0m
[0m2021.03.02 19:17:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:17:47 INFO  time: compiled root in 2.22s[0m
[0m2021.03.02 19:26:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:26:58 INFO  time: compiled root in 1.71s[0m
[0m2021.03.02 19:27:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:27:01 INFO  time: compiled root in 1.55s[0m
Mar 02, 2021 7:48:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4470
[0m2021.03.02 19:49:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:49:05 INFO  time: compiled root in 0.99s[0m
[0m2021.03.02 19:51:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:51:59 INFO  time: compiled root in 0.76s[0m
[0m2021.03.02 19:52:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:52:08 INFO  time: compiled root in 0.8s[0m
[0m2021.03.02 19:52:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:52:12 INFO  time: compiled root in 0.26s[0m
[0m2021.03.02 19:52:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:52:18 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 19:52:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:52:33 INFO  time: compiled root in 0.23s[0m
[0m2021.03.02 19:52:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:52:45 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 19:52:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:52:52 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 19:54:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:54:10 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 19:56:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:56:29 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 19:57:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:57:31 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 19:57:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:57:39 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 19:57:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:57:41 INFO  time: compiled root in 0.7s[0m
[0m2021.03.02 19:58:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 19:58:56 INFO  time: compiled root in 0.74s[0m
[0m2021.03.03 09:03:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:03:33 INFO  time: compiled root in 1.04s[0m
[0m2021.03.03 09:06:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:06:17 INFO  time: compiled root in 0.87s[0m
[0m2021.03.03 09:06:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:06:21 INFO  time: compiled root in 0.9s[0m
[0m2021.03.03 09:10:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:10:53 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 09:13:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:13:14 INFO  time: compiled root in 0.68s[0m
[0m2021.03.03 09:14:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:14:34 INFO  time: compiled root in 0.67s[0m
[0m2021.03.03 09:18:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:18:58 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 09:20:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:20:32 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 09:21:14 WARN  no build target for: /home/amburkee/tmp/210104-usf-bigdata/week4/hello-spark/src/main/scala/com/revature/hellospark/Runner.scala[0m
[0m2021.03.03 09:21:14 INFO  no build target: using presentation compiler with only scala-library: 2.12.12[0m
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
[0m2021.03.03 09:21:17 INFO  time: code lens generation in 2.85s[0m
Mar 03, 2021 9:21:39 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5616
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:21:51 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5622
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:22:03 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5627
[0m2021.03.03 09:22:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:22:09 INFO  time: compiled root in 0.15s[0m
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:22:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5651
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:22:30 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5657
[0m2021.03.03 09:22:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:22:42 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 09:22:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:22:58 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 09:23:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:23:05 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 09:23:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:23:15 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 09:23:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:23:17 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 09:23:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:23:33 INFO  time: compiled root in 0.13s[0m
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:23:35 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5787
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:23:40 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5795
[0m2021.03.03 09:24:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:24:03 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 09:24:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:24:07 INFO  time: compiled root in 0.18s[0m
[0m2021.03.03 09:24:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:24:44 INFO  time: compiled root in 98ms[0m
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:25:07 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5894
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:25:13 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5903
[0m2021.03.03 09:25:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:25:33 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 09:25:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:25:41 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 09:25:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:25:53 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 09:25:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:25:54 INFO  time: compiled root in 0.57s[0m
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in org.apache.spark.sql.Dataset[<error>]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 1838, 1838, 1855)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in org.apache.spark.sql.Dataset[<error>]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 1838, 1838, 1848)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in org.apache.spark.sql.Dataset[<error>]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 1838, 1838, 1850)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in org.apache.spark.sql.Dataset[String]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 1838, 1838, 1853)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in org.apache.spark.sql.Dataset[String]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 1838, 1838, 1853)
[0m2021.03.03 09:27:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:27:05 INFO  time: compiled root in 0.59s[0m
[0m2021.03.03 09:27:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:27:14 INFO  time: compiled root in 0.55s[0m
[0m2021.03.03 09:27:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:27:48 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 09:29:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:29:38 INFO  time: compiled root in 0.62s[0m
[0m2021.03.03 09:29:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:29:47 INFO  time: compiled root in 0.64s[0m
[0m2021.03.03 09:30:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:30:22 INFO  time: compiled root in 0.66s[0m
[0m2021.03.03 09:30:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:30:57 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 09:31:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:31:07 INFO  time: compiled root in 0.14s[0m
Mar 03, 2021 9:31:20 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6521
[0m2021.03.03 09:31:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:31:26 INFO  time: compiled root in 0.71s[0m
[0m2021.03.03 09:32:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:32:56 INFO  time: compiled root in 0.14s[0m
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in org.apache.spark.sql.Dataset[String]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 1803, 1803, 1818)
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:33:03 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6625
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:33:08 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6631
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in org.apache.spark.sql.Dataset[String]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 1803, 1803, 1818)
[0m2021.03.03 09:33:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:33:16 INFO  time: compiled root in 0.17s[0m
Mar 03, 2021 9:33:19 AM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: String index out of range: -1
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
	at java.lang.String.<init>(String.java:196)
	at scala.tools.nsc.interactive.Global.typeCompletions$1(Global.scala:1229)
	at scala.tools.nsc.interactive.Global.completionsAt(Global.scala:1252)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:375)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:373)
	at scala.Option.map(Option.scala:146)
	at scala.meta.internal.pc.SignatureHelpProvider.treeSymbol(SignatureHelpProvider.scala:373)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCall$.unapply(SignatureHelpProvider.scala:198)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.visit(SignatureHelpProvider.scala:309)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.traverse(SignatureHelpProvider.scala:303)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.fromTree(SignatureHelpProvider.scala:272)
	at scala.meta.internal.pc.SignatureHelpProvider.signatureHelp(SignatureHelpProvider.scala:27)

[0m2021.03.03 09:33:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:33:27 INFO  time: compiled root in 0.17s[0m
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:33:28 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6712
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:33:40 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6719
[0m2021.03.03 09:34:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:34:00 INFO  time: compiled root in 0.16s[0m
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:34:18 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6756
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:34:20 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6761
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark

import java.io._
import java.net.URI
import java.util.{Arrays, Locale, Properties, ServiceLoader, UUID}
import java.util.concurrent.{ConcurrentHashMap, ConcurrentMap}
import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger, AtomicReference}

import scala.collection.JavaConverters._
import scala.collection.Map
import scala.collection.generic.Growable
import scala.collection.mutable.HashMap
import scala.language.implicitConversions
import scala.reflect.{classTag, ClassTag}
import scala.util.control.NonFatal

import com.google.common.collect.MapMaker
import org.apache.commons.lang3.SerializationUtils
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.io.{ArrayWritable, BooleanWritable, BytesWritable, DoubleWritable, FloatWritable, IntWritable, LongWritable, NullWritable, Text, Writable}
import org.apache.hadoop.mapred.{FileInputFormat, InputFormat, JobConf, SequenceFileInputFormat, TextInputFormat}
import org.apache.hadoop.mapreduce.{InputFormat => NewInputFormat, Job => NewHadoopJob}
import org.apache.hadoop.mapreduce.lib.input.{FileInputFormat => NewFileInputFormat}

import org.apache.spark.annotation.DeveloperApi
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.deploy.{LocalSparkCluster, SparkHadoopUtil}
import org.apache.spark.input.{FixedLengthBinaryInputFormat, PortableDataStream, StreamInputFormat, WholeTextFileInputFormat}
import org.apache.spark.internal.Logging
import org.apache.spark.internal.config._
import org.apache.spark.io.CompressionCodec
import org.apache.spark.partial.{ApproximateEvaluator, PartialResult}
import org.apache.spark.rdd._
import org.apache.spark.rpc.RpcEndpointRef
import org.apache.spark.scheduler._
import org.apache.spark.scheduler.cluster.{CoarseGrainedSchedulerBackend, StandaloneSchedulerBackend}
import org.apache.spark.scheduler.local.LocalSchedulerBackend
import org.apache.spark.status.AppStatusStore
import org.apache.spark.status.api.v1.ThreadStackTrace
import org.apache.spark.storage._
import org.apache.spark.storage.BlockManagerMessages.TriggerThreadDump
import org.apache.spark.ui.{ConsoleProgressBar, SparkUI}
import org.apache.spark.util._

/**
 * Main entry point for Spark functionality. A SparkContext represents the connection to a Spark
 * cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.
 *
 * Only one SparkContext may be active per JVM.  You must `stop()` the active SparkContext before
 * creating a new one.  This limitation may eventually be removed; see SPARK-2243 for more details.
 *
 * @param config a Spark Config object describing the application configuration. Any settings in
 *   this config overrides the default configs as well as system properties.
 */
class SparkContext(config: SparkConf) extends Logging {

  // The call site where this SparkContext was constructed.
  private val creationSite: CallSite = Utils.getCallSite()

  // If true, log warnings instead of throwing exceptions when multiple SparkContexts are active
  private val allowMultipleContexts: Boolean =
    config.getBoolean("spark.driver.allowMultipleContexts", false)

  // In order to prevent multiple SparkContexts from being active at the same time, mark this
  // context as having started construction.
  // NOTE: this must be placed at the beginning of the SparkContext constructor.
  SparkContext.markPartiallyConstructed(this, allowMultipleContexts)

  val startTime = System.currentTimeMillis()

  private[spark] val stopped: AtomicBoolean = new AtomicBoolean(false)

  private[spark] def assertNotStopped(): Unit = {
    if (stopped.get()) {
      val activeContext = SparkContext.activeContext.get()
      val activeCreationSite =
        if (activeContext == null) {
          "(No active SparkContext.)"
        } else {
          activeContext.creationSite.longForm
        }
      throw new IllegalStateException(
        s"""Cannot call methods on a stopped SparkContext.
           |This stopped SparkContext was created at:
           |
           |${creationSite.longForm}
           |
           |The currently active SparkContext was created at:
           |
           |$activeCreationSite
         """.stripMargin)
    }
  }

  /**
   * Create a SparkContext that loads settings from system properties (for instance, when
   * launching with ./bin/spark-submit).
   */
  def this() = this(new SparkConf())

  /**
   * Alternative constructor that allows setting common Spark properties directly
   *
   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).
   * @param appName A name for your application, to display on the cluster web UI
   * @param conf a [[org.apache.spark.SparkConf]] object specifying other Spark parameters
   */
  def this(master: String, appName: String, conf: SparkConf) =
    this(SparkContext.updatedConf(conf, master, appName))

  /**
   * Alternative constructor that allows setting common Spark properties directly
   *
   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).
   * @param appName A name for your application, to display on the cluster web UI.
   * @param sparkHome Location where Spark is installed on cluster nodes.
   * @param jars Collection of JARs to send to the cluster. These can be paths on the local file
   *             system or HDFS, HTTP, HTTPS, or FTP URLs.
   * @param environment Environment variables to set on worker nodes.
   */
  def this(
      master: String,
      appName: String,
      sparkHome: String = null,
      jars: Seq[String] = Nil,
      environment: Map[String, String] = Map()) = {
    this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment))
  }

  // The following constructors are required when Java code accesses SparkContext directly.
  // Please see SI-4278

  /**
   * Alternative constructor that allows setting common Spark properties directly
   *
   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).
   * @param appName A name for your application, to display on the cluster web UI.
   */
  private[spark] def this(master: String, appName: String) =
    this(master, appName, null, Nil, Map())

  /**
   * Alternative constructor that allows setting common Spark properties directly
   *
   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).
   * @param appName A name for your application, to display on the cluster web UI.
   * @param sparkHome Location where Spark is installed on cluster nodes.
   */
  private[spark] def this(master: String, appName: String, sparkHome: String) =
    this(master, appName, sparkHome, Nil, Map())

  /**
   * Alternative constructor that allows setting common Spark properties directly
   *
   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).
   * @param appName A name for your application, to display on the cluster web UI.
   * @param sparkHome Location where Spark is installed on cluster nodes.
   * @param jars Collection of JARs to send to the cluster. These can be paths on the local file
   *             system or HDFS, HTTP, HTTPS, or FTP URLs.
   */
  private[spark] def this(master: String, appName: String, sparkHome: String, jars: Seq[String]) =
    this(master, appName, sparkHome, jars, Map())

  // log out Spark Version in Spark driver log
  logInfo(s"Running Spark version $SPARK_VERSION")

  /* ------------------------------------------------------------------------------------- *
   | Private variables. These variables keep the internal state of the context, and are    |
   | not accessible by the outside world. They're mutable since we want to initialize all  |
   | of them to some neutral value ahead of time, so that calling "stop()" while the       |
   | constructor is still running is safe.                                                 |
   * ------------------------------------------------------------------------------------- */

  private var _conf: SparkConf = _
  private var _eventLogDir: Option[URI] = None
  private var _eventLogCodec: Option[String] = None
  private var _listenerBus: LiveListenerBus = _
  private var _env: SparkEnv = _
  private var _statusTracker: SparkStatusTracker = _
  private var _progressBar: Option[ConsoleProgressBar] = None
  private var _ui: Option[SparkUI] = None
  private var _hadoopConfiguration: Configuration = _
  private var _executorMemory: Int = _
  private var _schedulerBackend: SchedulerBackend = _
  private var _taskScheduler: TaskScheduler = _
  private var _heartbeatReceiver: RpcEndpointRef = _
  @volatile private var _dagScheduler: DAGScheduler = _
  private var _applicationId: String = _
  private var _applicationAttemptId: Option[String] = None
  private var _eventLogger: Option[EventLoggingListener] = None
  private var _executorAllocationManager: Option[ExecutorAllocationManager] = None
  private var _cleaner: Option[ContextCleaner] = None
  private var _listenerBusStarted: Boolean = false
  private var _jars: Seq[String] = _
  private var _files: Seq[String] = _
  private var _shutdownHookRef: AnyRef = _
  private var _statusStore: AppStatusStore = _

  /* ------------------------------------------------------------------------------------- *
   | Accessors and public fields. These provide access to the internal state of the        |
   | context.                                                                              |
   * ------------------------------------------------------------------------------------- */

  private[spark] def conf: SparkConf = _conf

  /**
   * Return a copy of this SparkContext's configuration. The configuration ''cannot'' be
   * changed at runtime.
   */
  def getConf: SparkConf = conf.clone()

  def jars: Seq[String] = _jars
  def files: Seq[String] = _files
  def master: String = _conf.get("spark.master")
  def deployMode: String = _conf.getOption("spark.submit.deployMode").getOrElse("client")
  def appName: String = _conf.get("spark.app.name")

  private[spark] def isEventLogEnabled: Boolean = _conf.getBoolean("spark.eventLog.enabled", false)
  private[spark] def eventLogDir: Option[URI] = _eventLogDir
  private[spark] def eventLogCodec: Option[String] = _eventLogCodec

  def isLocal: Boolean = Utils.isLocalMaster(_conf)

  /**
   * @return true if context is stopped or in the midst of stopping.
   */
  def isStopped: Boolean = stopped.get()

  private[spark] def statusStore: AppStatusStore = _statusStore

  // An asynchronous listener bus for Spark events
  private[spark] def listenerBus: LiveListenerBus = _listenerBus

  // This function allows components created by SparkEnv to be mocked in unit tests:
  private[spark] def createSparkEnv(
      conf: SparkConf,
      isLocal: Boolean,
      listenerBus: LiveListenerBus): SparkEnv = {
    SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master, conf))
  }

  private[spark] def env: SparkEnv = _env

  // Used to store a URL for each static file/jar together with the file's local timestamp
  private[spark] val addedFiles = new ConcurrentHashMap[String, Long]().asScala
  private[spark] val addedJars = new ConcurrentHashMap[String, Long]().asScala

  // Keeps track of all persisted RDDs
  private[spark] val persistentRdds = {
    val map: ConcurrentMap[Int, RDD[_]] = new MapMaker().weakValues().makeMap[Int, RDD[_]]()
    map.asScala
  }
  def statusTracker: SparkStatusTracker = _statusTracker

  private[spark] def progressBar: Option[ConsoleProgressBar] = _progressBar

  private[spark] def ui: Option[SparkUI] = _ui

  def uiWebUrl: Option[String] = _ui.map(_.webUrl)

  /**
   * A default Hadoop Configuration for the Hadoop code (e.g. file systems) that we reuse.
   *
   * @note As it will be reused in all Hadoop RDDs, it's better not to modify it unless you
   * plan to set some global configurations for all Hadoop RDDs.
   */
  def hadoopConfiguration: Configuration = _hadoopConfiguration

  private[spark] def executorMemory: Int = _executorMemory

  // Environment variables to pass to our executors.
  private[spark] val executorEnvs = HashMap[String, String]()

  // Set SPARK_USER for user who is running SparkContext.
  val sparkUser = Utils.getCurrentUserName()

  private[spark] def schedulerBackend: SchedulerBackend = _schedulerBackend

  private[spark] def taskScheduler: TaskScheduler = _taskScheduler
  private[spark] def taskScheduler_=(ts: TaskScheduler): Unit = {
    _taskScheduler = ts
  }

  private[spark] def dagScheduler: DAGScheduler = _dagScheduler
  private[spark] def dagScheduler_=(ds: DAGScheduler): Unit = {
    _dagScheduler = ds
  }

  /**
   * A unique identifier for the Spark application.
   * Its format depends on the scheduler implementation.
   * (i.e.
   *  in case of local spark app something like 'local-1433865536131'
   *  in case of YARN something like 'application_1433865536131_34483'
   *  in case of MESOS something like 'driver-20170926223339-0001'
   * )
   */
  def applicationId: String = _applicationId
  def applicationAttemptId: Option[String] = _applicationAttemptId

  private[spark] def eventLogger: Option[EventLoggingListener] = _eventLogger

  private[spark] def executorAllocationManager: Option[ExecutorAllocationManager] =
    _executorAllocationManager

  private[spark] def cleaner: Option[ContextCleaner] = _cleaner

  private[spark] var checkpointDir: Option[String] = None

  // Thread Local variable that can be used by users to pass information down the stack
  protected[spark] val localProperties = new InheritableThreadLocal[Properties] {
    override protected def childValue(parent: Properties): Properties = {
      // Note: make a clone such that changes in the parent properties aren't reflected in
      // the those of the children threads, which has confusing semantics (SPARK-10563).
      SerializationUtils.clone(parent)
    }
    override protected def initialValue(): Properties = new Properties()
  }

  /* ------------------------------------------------------------------------------------- *
   | Initialization. This code initializes the context in a manner that is exception-safe. |
   | All internal fields holding state are initialized here, and any error prompts the     |
   | stop() method to be called.                                                           |
   * ------------------------------------------------------------------------------------- */

  private def warnSparkMem(value: String): String = {
    logWarning("Using SPARK_MEM to set amount of memory to use per executor process is " +
      "deprecated, please use spark.executor.memory instead.")
    value
  }

  /** Control our logLevel. This overrides any user-defined log settings.
   * @param logLevel The desired log level as a string.
   * Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
   */
  def setLogLevel(logLevel: String) {
    // let's allow lowercase or mixed case too
    val upperCased = logLevel.toUpperCase(Locale.ROOT)
    require(SparkContext.VALID_LOG_LEVELS.contains(upperCased),
      s"Supplied level $logLevel did not match one of:" +
        s" ${SparkContext.VALID_LOG_LEVELS.mkString(",")}")
    Utils.setLogLevel(org.apache.log4j.Level.toLevel(upperCased))
  }

  try {
    _conf = config.clone()
    _conf.validateSettings()

    if (!_conf.contains("spark.master")) {
      throw new SparkException("A master URL must be set in your configuration")
    }
    if (!_conf.contains("spark.app.name")) {
      throw new SparkException("An application name must be set in your configuration")
    }

    // log out spark.app.name in the Spark driver logs
    logInfo(s"Submitted application: $appName")

    // System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster
    if (master == "yarn" && deployMode == "cluster" && !_conf.contains("spark.yarn.app.id")) {
      throw new SparkException("Detected yarn cluster mode, but isn't running on a cluster. " +
        "Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.")
    }

    if (_conf.getBoolean("spark.logConf", false)) {
      logInfo("Spark configuration:\n" + _conf.toDebugString)
    }

    // Set Spark driver host and port system properties. This explicitly sets the configuration
    // instead of relying on the default value of the config constant.
    _conf.set(DRIVER_HOST_ADDRESS, _conf.get(DRIVER_HOST_ADDRESS))
    _conf.setIfMissing("spark.driver.port", "0")

    _conf.set("spark.executor.id", SparkContext.DRIVER_IDENTIFIER)

    _jars = Utils.getUserJars(_conf)
    _files = _conf.getOption("spark.files").map(_.split(",")).map(_.filter(_.nonEmpty))
      .toSeq.flatten

    _eventLogDir =
      if (isEventLogEnabled) {
        val unresolvedDir = conf.get("spark.eventLog.dir", EventLoggingListener.DEFAULT_LOG_DIR)
          .stripSuffix("/")
        Some(Utils.resolveURI(unresolvedDir))
      } else {
        None
      }

    _eventLogCodec = {
      val compress = _conf.getBoolean("spark.eventLog.compress", false)
      if (compress && isEventLogEnabled) {
        Some(CompressionCodec.getCodecName(_conf)).map(CompressionCodec.getShortName)
      } else {
        None
      }
    }

    _listenerBus = new LiveListenerBus(_conf)

    // Initialize the app status store and listener before SparkEnv is created so that it gets
    // all events.
    _statusStore = AppStatusStore.createLiveStore(conf)
    listenerBus.addToStatusQueue(_statusStore.listener.get)

    // Create the Spark execution environment (cache, map output tracker, etc)
    _env = createSparkEnv(_conf, isLocal, listenerBus)
    SparkEnv.set(_env)

    // If running the REPL, register the repl's output dir with the file server.
    _conf.getOption("spark.repl.class.outputDir").foreach { path =>
      val replUri = _env.rpcEnv.fileServer.addDirectory("/classes", new File(path))
      _conf.set("spark.repl.class.uri", replUri)
    }

    _statusTracker = new SparkStatusTracker(this, _statusStore)

    _progressBar =
      if (_conf.get(UI_SHOW_CONSOLE_PROGRESS) && !log.isInfoEnabled) {
        Some(new ConsoleProgressBar(this))
      } else {
        None
      }

    _ui =
      if (conf.getBoolean("spark.ui.enabled", true)) {
        Some(SparkUI.create(Some(this), _statusStore, _conf, _env.securityManager, appName, "",
          startTime))
      } else {
        // For tests, do not enable the UI
        None
      }
    // Bind the UI before starting the task scheduler to communicate
    // the bound port to the cluster manager properly
    _ui.foreach(_.bind())

    _hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf)

    // Add each JAR given through the constructor
    if (jars != null) {
      jars.foreach(addJar)
    }

    if (files != null) {
      files.foreach(addFile)
    }

    _executorMemory = _conf.getOption("spark.executor.memory")
      .orElse(Option(System.getenv("SPARK_EXECUTOR_MEMORY")))
      .orElse(Option(System.getenv("SPARK_MEM"))
      .map(warnSparkMem))
      .map(Utils.memoryStringToMb)
      .getOrElse(1024)

    // Convert java options to env vars as a work around
    // since we can't set env vars directly in sbt.
    for { (envKey, propKey) <- Seq(("SPARK_TESTING", "spark.testing"))
      value <- Option(System.getenv(envKey)).orElse(Option(System.getProperty(propKey)))} {
      executorEnvs(envKey) = value
    }
    Option(System.getenv("SPARK_PREPEND_CLASSES")).foreach { v =>
      executorEnvs("SPARK_PREPEND_CLASSES") = v
    }
    // The Mesos scheduler backend relies on this environment variable to set executor memory.
    // TODO: Set this only in the Mesos scheduler.
    executorEnvs("SPARK_EXECUTOR_MEMORY") = executorMemory + "m"
    executorEnvs ++= _conf.getExecutorEnv
    executorEnvs("SPARK_USER") = sparkUser

    // We need to register "HeartbeatReceiver" before "createTaskScheduler" because Executor will
    // retrieve "HeartbeatReceiver" in the constructor. (SPARK-6640)
    _heartbeatReceiver = env.rpcEnv.setupEndpoint(
      HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this))

    // Create and start the scheduler
    val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
    _schedulerBackend = sched
    _taskScheduler = ts
    _dagScheduler = new DAGScheduler(this)
    _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)

    // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's
    // constructor
    _taskScheduler.start()

    _applicationId = _taskScheduler.applicationId()
    _applicationAttemptId = taskScheduler.applicationAttemptId()
    _conf.set("spark.app.id", _applicationId)
    if (_conf.getBoolean("spark.ui.reverseProxy", false)) {
      System.setProperty("spark.ui.proxyBase", "/proxy/" + _applicationId)
    }
    _ui.foreach(_.setAppId(_applicationId))
    _env.blockManager.initialize(_applicationId)

    // The metrics system for Driver need to be set spark.app.id to app ID.
    // So it should start after we get app ID from the task scheduler and set spark.app.id.
    _env.metricsSystem.start()
    // Attach the driver metrics servlet handler to the web ui after the metrics system is started.
    _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler)))

    _eventLogger =
      if (isEventLogEnabled) {
        val logger =
          new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get,
            _conf, _hadoopConfiguration)
        logger.start()
        listenerBus.addToEventLogQueue(logger)
        Some(logger)
      } else {
        None
      }

    // Optionally scale number of executors dynamically based on workload. Exposed for testing.
    val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf)
    _executorAllocationManager =
      if (dynamicAllocationEnabled) {
        schedulerBackend match {
          case b: ExecutorAllocationClient =>
            Some(new ExecutorAllocationManager(
              schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf,
              _env.blockManager.master))
          case _ =>
            None
        }
      } else {
        None
      }
    _executorAllocationManager.foreach(_.start())

    _cleaner =
      if (_conf.getBoolean("spark.cleaner.referenceTracking", true)) {
        Some(new ContextCleaner(this))
      } else {
        None
      }
    _cleaner.foreach(_.start())

    setupAndStartListenerBus()
    postEnvironmentUpdate()
    postApplicationStart()

    // Post init
    _taskScheduler.postStartHook()
    _env.metricsSystem.registerSource(_dagScheduler.metricsSource)
    _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))
    _executorAllocationManager.foreach { e =>
      _env.metricsSystem.registerSource(e.executorAllocationManagerSource)
    }

    // Make sure the context is stopped if the user forgets about it. This avoids leaving
    // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM
    // is killed, though.
    logDebug("Adding shutdown hook") // force eager creation of logger
    _shutdownHookRef = ShutdownHookManager.addShutdownHook(
      ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () =>
      logInfo("Invoking stop() from shutdown hook")
      try {
        stop()
      } catch {
        case e: Throwable =>
          logWarning("Ignoring Exception while stopping SparkContext from shutdown hook", e)
      }
    }
  } catch {
    case NonFatal(e) =>
      logError("Error initializing SparkContext.", e)
      try {
        stop()
      } catch {
        case NonFatal(inner) =>
          logError("Error stopping SparkContext after init error.", inner)
      } finally {
        throw e
      }
  }

  /**
   * Called by the web UI to obtain executor thread dumps.  This method may be expensive.
   * Logs an error and returns None if we failed to obtain a thread dump, which could occur due
   * to an executor being dead or unresponsive or due to network issues while sending the thread
   * dump message back to the driver.
   */
  private[spark] def getExecutorThreadDump(executorId: String): Option[Array[ThreadStackTrace]] = {
    try {
      if (executorId == SparkContext.DRIVER_IDENTIFIER) {
        Some(Utils.getThreadDump())
      } else {
        val endpointRef = env.blockManager.master.getExecutorEndpointRef(executorId).get
        Some(endpointRef.askSync[Array[ThreadStackTrace]](TriggerThreadDump))
      }
    } catch {
      case e: Exception =>
        logError(s"Exception getting thread dump from executor $executorId", e)
        None
    }
  }

  private[spark] def getLocalProperties: Properties = localProperties.get()

  private[spark] def setLocalProperties(props: Properties) {
    localProperties.set(props)
  }

  /**
   * Set a local property that affects jobs submitted from this thread, such as the Spark fair
   * scheduler pool. User-defined properties may also be set here. These properties are propagated
   * through to worker tasks and can be accessed there via
   * [[org.apache.spark.TaskContext#getLocalProperty]].
   *
   * These properties are inherited by child threads spawned from this thread. This
   * may have unexpected consequences when working with thread pools. The standard java
   * implementation of thread pools have worker threads spawn other worker threads.
   * As a result, local properties may propagate unpredictably.
   */
  def setLocalProperty(key: String, value: String) {
    if (value == null) {
      localProperties.get.remove(key)
    } else {
      localProperties.get.setProperty(key, value)
    }
  }

  /**
   * Get a local property set in this thread, or null if it is missing. See
   * `org.apache.spark.SparkContext.setLocalProperty`.
   */
  def getLocalProperty(key: String): String =
    Option(localProperties.get).map(_.getProperty(key)).orNull

  /** Set a human readable description of the current job. */
  def setJobDescription(value: String) {
    setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, value)
  }

  /**
   * Assigns a group ID to all the jobs started by this thread until the group ID is set to a
   * different value or cleared.
   *
   * Often, a unit of execution in an application consists of multiple Spark actions or jobs.
   * Application programmers can use this method to group all those jobs together and give a
   * group description. Once set, the Spark web UI will associate such jobs with this group.
   *
   * The application can also use `org.apache.spark.SparkContext.cancelJobGroup` to cancel all
   * running jobs in this group. For example,
   * {{{
   * // In the main thread:
   * sc.setJobGroup("some_job_to_cancel", "some job description")
   * sc.parallelize(1 to 10000, 2).map { i => Thread.sleep(10); i }.count()
   *
   * // In a separate thread:
   * sc.cancelJobGroup("some_job_to_cancel")
   * }}}
   *
   * @param interruptOnCancel If true, then job cancellation will result in `Thread.interrupt()`
   * being called on the job's executor threads. This is useful to help ensure that the tasks
   * are actually stopped in a timely manner, but is off by default due to HDFS-1208, where HDFS
   * may respond to Thread.interrupt() by marking nodes as dead.
   */
  def setJobGroup(groupId: String, description: String, interruptOnCancel: Boolean = false) {
    setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, description)
    setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, groupId)
    // Note: Specifying interruptOnCancel in setJobGroup (rather than cancelJobGroup) avoids
    // changing several public APIs and allows Spark cancellations outside of the cancelJobGroup
    // APIs to also take advantage of this property (e.g., internal job failures or canceling from
    // JobProgressTab UI) on a per-job basis.
    setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, interruptOnCancel.toString)
  }

  /** Clear the current thread's job group ID and its description. */
  def clearJobGroup() {
    setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, null)
    setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, null)
    setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, null)
  }

  /**
   * Execute a block of code in a scope such that all new RDDs created in this body will
   * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}.
   *
   * @note Return statements are NOT allowed in the given body.
   */
  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](this)(body)

  // Methods for creating RDDs

  /** Distribute a local Scala collection to form an RDD.
   *
   * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call
   * to parallelize and before the first action on the RDD, the resultant RDD will reflect the
   * modified collection. Pass a copy of the argument to avoid this.
   * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an
   * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions.
   * @param seq Scala collection to distribute
   * @param numSlices number of partitions to divide the collection into
   * @return RDD representing distributed collection
   */
  def parallelize[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T] = withScope {
    assertNotStopped()
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }

  /**
   * Creates a new RDD[Long] containing elements from `start` to `end`(exclusive), increased by
   * `step` every element.
   *
   * @note if we need to cache this RDD, we should make sure each partition does not exceed limit.
   *
   * @param start the start value.
   * @param end the end value.
   * @param step the incremental step
   * @param numSlices number of partitions to divide the collection into
   * @return RDD representing distributed range
   */
  def range(
      start: Long,
      end: Long,
      step: Long = 1,
      numSlices: Int = defaultParallelism): RDD[Long] = withScope {
    assertNotStopped()
    // when step is 0, range will run infinitely
    require(step != 0, "step cannot be 0")
    val numElements: BigInt = {
      val safeStart = BigInt(start)
      val safeEnd = BigInt(end)
      if ((safeEnd - safeStart) % step == 0 || (safeEnd > safeStart) != (step > 0)) {
        (safeEnd - safeStart) / step
      } else {
        // the remainder has the same sign with range, could add 1 more
        (safeEnd - safeStart) / step + 1
      }
    }
    parallelize(0 until numSlices, numSlices).mapPartitionsWithIndex { (i, _) =>
      val partitionStart = (i * numElements) / numSlices * step + start
      val partitionEnd = (((i + 1) * numElements) / numSlices) * step + start
      def getSafeMargin(bi: BigInt): Long =
        if (bi.isValidLong) {
          bi.toLong
        } else if (bi > 0) {
          Long.MaxValue
        } else {
          Long.MinValue
        }
      val safePartitionStart = getSafeMargin(partitionStart)
      val safePartitionEnd = getSafeMargin(partitionEnd)

      new Iterator[Long] {
        private[this] var number: Long = safePartitionStart
        private[this] var overflow: Boolean = false

        override def hasNext =
          if (!overflow) {
            if (step > 0) {
              number < safePartitionEnd
            } else {
              number > safePartitionEnd
            }
          } else false

        override def next() = {
          val ret = number
          number += step
          if (number < ret ^ step < 0) {
            // we have Long.MaxValue + Long.MaxValue < Long.MaxValue
            // and Long.MinValue + Long.MinValue > Long.MinValue, so iff the step causes a step
            // back, we are pretty sure that we have an overflow.
            overflow = true
          }
          ret
        }
      }
    }
  }

  /** Distribute a local Scala collection to form an RDD.
   *
   * This method is identical to `parallelize`.
   * @param seq Scala collection to distribute
   * @param numSlices number of partitions to divide the collection into
   * @return RDD representing distributed collection
   */
  def makeRDD[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T] = withScope {
    parallelize(seq, numSlices)
  }

  /**
   * Distribute a local Scala collection to form an RDD, with one or more
   * location preferences (hostnames of Spark nodes) for each object.
   * Create a new partition for each collection item.
   * @param seq list of tuples of data and location preferences (hostnames of Spark nodes)
   * @return RDD representing data partitioned according to location preferences
   */
  def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope {
    assertNotStopped()
    val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap
    new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs)
  }

  /**
   * Read a text file from HDFS, a local file system (available on all nodes), or any
   * Hadoop-supported file system URI, and return it as an RDD of Strings.
   * @param path path to the text file on a supported file system
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of lines of the text file
   */
  def textFile(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope {
    assertNotStopped()
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],
      minPartitions).map(pair => pair._2.toString).setName(path)
  }

  /**
   * Read a directory of text files from HDFS, a local file system (available on all nodes), or any
   * Hadoop-supported file system URI. Each file is read as a single record and returned in a
   * key-value pair, where the key is the path of each file, the value is the content of each file.
   *
   * <p> For example, if you have the following files:
   * {{{
   *   hdfs://a-hdfs-path/part-00000
   *   hdfs://a-hdfs-path/part-00001
   *   ...
   *   hdfs://a-hdfs-path/part-nnnnn
   * }}}
   *
   * Do `val rdd = sparkContext.wholeTextFile("hdfs://a-hdfs-path")`,
   *
   * <p> then `rdd` contains
   * {{{
   *   (a-hdfs-path/part-00000, its content)
   *   (a-hdfs-path/part-00001, its content)
   *   ...
   *   (a-hdfs-path/part-nnnnn, its content)
   * }}}
   *
   * @note Small files are preferred, large file is also allowable, but may cause bad performance.
   * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files
   *       in a directory rather than `.../path/` or `.../path`
   * @note Partitioning is determined by data locality. This may result in too few partitions
   *       by default.
   *
   * @param path Directory to the input data files, the path can be comma separated paths as the
   *             list of inputs.
   * @param minPartitions A suggestion value of the minimal splitting number for input data.
   * @return RDD representing tuples of file path and the corresponding file content
   */
  def wholeTextFiles(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[(String, String)] = withScope {
    assertNotStopped()
    val job = NewHadoopJob.getInstance(hadoopConfiguration)
    // Use setInputPaths so that wholeTextFiles aligns with hadoopFile/textFile in taking
    // comma separated files as input. (see SPARK-7155)
    NewFileInputFormat.setInputPaths(job, path)
    val updateConf = job.getConfiguration
    new WholeTextFileRDD(
      this,
      classOf[WholeTextFileInputFormat],
      classOf[Text],
      classOf[Text],
      updateConf,
      minPartitions).map(record => (record._1.toString, record._2.toString)).setName(path)
  }

  /**
   * Get an RDD for a Hadoop-readable dataset as PortableDataStream for each file
   * (useful for binary data)
   *
   * For example, if you have the following files:
   * {{{
   *   hdfs://a-hdfs-path/part-00000
   *   hdfs://a-hdfs-path/part-00001
   *   ...
   *   hdfs://a-hdfs-path/part-nnnnn
   * }}}
   *
   * Do
   * `val rdd = sparkContext.binaryFiles("hdfs://a-hdfs-path")`,
   *
   * then `rdd` contains
   * {{{
   *   (a-hdfs-path/part-00000, its content)
   *   (a-hdfs-path/part-00001, its content)
   *   ...
   *   (a-hdfs-path/part-nnnnn, its content)
   * }}}
   *
   * @note Small files are preferred; very large files may cause bad performance.
   * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files
   *       in a directory rather than `.../path/` or `.../path`
   * @note Partitioning is determined by data locality. This may result in too few partitions
   *       by default.
   *
   * @param path Directory to the input data files, the path can be comma separated paths as the
   *             list of inputs.
   * @param minPartitions A suggestion value of the minimal splitting number for input data.
   * @return RDD representing tuples of file path and corresponding file content
   */
  def binaryFiles(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[(String, PortableDataStream)] = withScope {
    assertNotStopped()
    val job = NewHadoopJob.getInstance(hadoopConfiguration)
    // Use setInputPaths so that binaryFiles aligns with hadoopFile/textFile in taking
    // comma separated files as input. (see SPARK-7155)
    NewFileInputFormat.setInputPaths(job, path)
    val updateConf = job.getConfiguration
    new BinaryFileRDD(
      this,
      classOf[StreamInputFormat],
      classOf[String],
      classOf[PortableDataStream],
      updateConf,
      minPartitions).setName(path)
  }

  /**
   * Load data from a flat binary file, assuming the length of each record is constant.
   *
   * @note We ensure that the byte array for each record in the resulting RDD
   * has the provided record length.
   *
   * @param path Directory to the input data files, the path can be comma separated paths as the
   *             list of inputs.
   * @param recordLength The length at which to split the records
   * @param conf Configuration for setting up the dataset.
   *
   * @return An RDD of data with values, represented as byte arrays
   */
  def binaryRecords(
      path: String,
      recordLength: Int,
      conf: Configuration = hadoopConfiguration): RDD[Array[Byte]] = withScope {
    assertNotStopped()
    conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength)
    val br = newAPIHadoopFile[LongWritable, BytesWritable, FixedLengthBinaryInputFormat](path,
      classOf[FixedLengthBinaryInputFormat],
      classOf[LongWritable],
      classOf[BytesWritable],
      conf = conf)
    br.map { case (k, v) =>
      val bytes = v.copyBytes()
      assert(bytes.length == recordLength, "Byte array does not have correct length")
      bytes
    }
  }

  /**
   * Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf given its InputFormat and other
   * necessary info (e.g. file name for a filesystem-based dataset, table name for HyperTable),
   * using the older MapReduce API (`org.apache.hadoop.mapred`).
   *
   * @param conf JobConf for setting up the dataset. Note: This will be put into a Broadcast.
   *             Therefore if you plan to reuse this conf to create multiple RDDs, you need to make
   *             sure you won't modify the conf. A safe approach is always creating a new conf for
   *             a new RDD.
   * @param inputFormatClass storage format of the data to be read
   * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter
   * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter
   * @param minPartitions Minimum number of Hadoop Splits to generate.
   * @return RDD of tuples of key and corresponding value
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   */
  def hadoopRDD[K, V](
      conf: JobConf,
      inputFormatClass: Class[_ <: InputFormat[K, V]],
      keyClass: Class[K],
      valueClass: Class[V],
      minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope {
    assertNotStopped()

    // This is a hack to enforce loading hdfs-site.xml.
    // See SPARK-11227 for details.
    FileSystem.getLocal(conf)

    // Add necessary security credentials to the JobConf before broadcasting it.
    SparkHadoopUtil.get.addCredentials(conf)
    new HadoopRDD(this, conf, inputFormatClass, keyClass, valueClass, minPartitions)
  }

  /** Get an RDD for a Hadoop file with an arbitrary InputFormat
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param inputFormatClass storage format of the data to be read
   * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter
   * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of tuples of key and corresponding value
   */
  def hadoopFile[K, V](
      path: String,
      inputFormatClass: Class[_ <: InputFormat[K, V]],
      keyClass: Class[K],
      valueClass: Class[V],
      minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope {
    assertNotStopped()

    // This is a hack to enforce loading hdfs-site.xml.
    // See SPARK-11227 for details.
    FileSystem.getLocal(hadoopConfiguration)

    // A Hadoop configuration can be about 10 KB, which is pretty big, so broadcast it.
    val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration))
    val setInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path)
    new HadoopRDD(
      this,
      confBroadcast,
      Some(setInputPathsFunc),
      inputFormatClass,
      keyClass,
      valueClass,
      minPartitions).setName(path)
  }

  /**
   * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys,
   * values and the InputFormat so that users don't need to pass them directly. Instead, callers
   * can just write, for example,
   * {{{
   * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path, minPartitions)
   * }}}
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of tuples of key and corresponding value
   */
  def hadoopFile[K, V, F <: InputFormat[K, V]]
      (path: String, minPartitions: Int)
      (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope {
    hadoopFile(path,
      fm.runtimeClass.asInstanceOf[Class[F]],
      km.runtimeClass.asInstanceOf[Class[K]],
      vm.runtimeClass.asInstanceOf[Class[V]],
      minPartitions)
  }

  /**
   * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys,
   * values and the InputFormat so that users don't need to pass them directly. Instead, callers
   * can just write, for example,
   * {{{
   * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path)
   * }}}
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths as
   * a list of inputs
   * @return RDD of tuples of key and corresponding value
   */
  def hadoopFile[K, V, F <: InputFormat[K, V]](path: String)
      (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope {
    hadoopFile[K, V, F](path, defaultMinPartitions)
  }

  /**
   * Smarter version of `newApiHadoopFile` that uses class tags to figure out the classes of keys,
   * values and the `org.apache.hadoop.mapreduce.InputFormat` (new MapReduce API) so that user
   * don't need to pass them directly. Instead, callers can just write, for example:
   * ```
   * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path)
   * ```
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @return RDD of tuples of key and corresponding value
   */
  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]
      (path: String)
      (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope {
    newAPIHadoopFile(
      path,
      fm.runtimeClass.asInstanceOf[Class[F]],
      km.runtimeClass.asInstanceOf[Class[K]],
      vm.runtimeClass.asInstanceOf[Class[V]])
  }

  /**
   * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat
   * and extra configuration options to pass to the input format.
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param fClass storage format of the data to be read
   * @param kClass `Class` of the key associated with the `fClass` parameter
   * @param vClass `Class` of the value associated with the `fClass` parameter
   * @param conf Hadoop configuration
   * @return RDD of tuples of key and corresponding value
   */
  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]](
      path: String,
      fClass: Class[F],
      kClass: Class[K],
      vClass: Class[V],
      conf: Configuration = hadoopConfiguration): RDD[(K, V)] = withScope {
    assertNotStopped()

    // This is a hack to enforce loading hdfs-site.xml.
    // See SPARK-11227 for details.
    FileSystem.getLocal(hadoopConfiguration)

    // The call to NewHadoopJob automatically adds security credentials to conf,
    // so we don't need to explicitly add them ourselves
    val job = NewHadoopJob.getInstance(conf)
    // Use setInputPaths so that newAPIHadoopFile aligns with hadoopFile/textFile in taking
    // comma separated files as input. (see SPARK-7155)
    NewFileInputFormat.setInputPaths(job, path)
    val updatedConf = job.getConfiguration
    new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf).setName(path)
  }

  /**
   * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat
   * and extra configuration options to pass to the input format.
   *
   * @param conf Configuration for setting up the dataset. Note: This will be put into a Broadcast.
   *             Therefore if you plan to reuse this conf to create multiple RDDs, you need to make
   *             sure you won't modify the conf. A safe approach is always creating a new conf for
   *             a new RDD.
   * @param fClass storage format of the data to be read
   * @param kClass `Class` of the key associated with the `fClass` parameter
   * @param vClass `Class` of the value associated with the `fClass` parameter
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   */
  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]](
      conf: Configuration = hadoopConfiguration,
      fClass: Class[F],
      kClass: Class[K],
      vClass: Class[V]): RDD[(K, V)] = withScope {
    assertNotStopped()

    // This is a hack to enforce loading hdfs-site.xml.
    // See SPARK-11227 for details.
    FileSystem.getLocal(conf)

    // Add necessary security credentials to the JobConf. Required to access secure HDFS.
    val jconf = new JobConf(conf)
    SparkHadoopUtil.get.addCredentials(jconf)
    new NewHadoopRDD(this, fClass, kClass, vClass, jconf)
  }

  /**
   * Get an RDD for a Hadoop SequenceFile with given key and value types.
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param keyClass `Class` of the key associated with `SequenceFileInputFormat`
   * @param valueClass `Class` of the value associated with `SequenceFileInputFormat`
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of tuples of key and corresponding value
   */
  def sequenceFile[K, V](path: String,
      keyClass: Class[K],
      valueClass: Class[V],
      minPartitions: Int
      ): RDD[(K, V)] = withScope {
    assertNotStopped()
    val inputFormatClass = classOf[SequenceFileInputFormat[K, V]]
    hadoopFile(path, inputFormatClass, keyClass, valueClass, minPartitions)
  }

  /**
   * Get an RDD for a Hadoop SequenceFile with given key and value types.
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param keyClass `Class` of the key associated with `SequenceFileInputFormat`
   * @param valueClass `Class` of the value associated with `SequenceFileInputFormat`
   * @return RDD of tuples of key and corresponding value
   */
  def sequenceFile[K, V](
      path: String,
      keyClass: Class[K],
      valueClass: Class[V]): RDD[(K, V)] = withScope {
    assertNotStopped()
    sequenceFile(path, keyClass, valueClass, defaultMinPartitions)
  }

  /**
   * Version of sequenceFile() for types implicitly convertible to Writables through a
   * WritableConverter. For example, to access a SequenceFile where the keys are Text and the
   * values are IntWritable, you could simply write
   * {{{
   * sparkContext.sequenceFile[String, Int](path, ...)
   * }}}
   *
   * WritableConverters are provided in a somewhat strange way (by an implicit function) to support
   * both subclasses of Writable and types for which we define a converter (e.g. Int to
   * IntWritable). The most natural thing would've been to have implicit objects for the
   * converters, but then we couldn't have an object for every subclass of Writable (you can't
   * have a parameterized singleton object). We use functions instead to create a new converter
   * for the appropriate type. In addition, we pass the converter a ClassTag of its type to
   * allow it to figure out the Writable class to use in the subclass case.
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of tuples of key and corresponding value
   */
   def sequenceFile[K, V]
       (path: String, minPartitions: Int = defaultMinPartitions)
       (implicit km: ClassTag[K], vm: ClassTag[V],
        kcf: () => WritableConverter[K], vcf: () => WritableConverter[V]): RDD[(K, V)] = {
    withScope {
      assertNotStopped()
      val kc = clean(kcf)()
      val vc = clean(vcf)()
      val format = classOf[SequenceFileInputFormat[Writable, Writable]]
      val writables = hadoopFile(path, format,
        kc.writableClass(km).asInstanceOf[Class[Writable]],
        vc.writableClass(vm).asInstanceOf[Class[Writable]], minPartitions)
      writables.map { case (k, v) => (kc.convert(k), vc.convert(v)) }
    }
  }

  /**
   * Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and
   * BytesWritable values that contain a serialized partition. This is still an experimental
   * storage format and may not be supported exactly as is in future Spark releases. It will also
   * be pretty slow if you use the default serializer (Java serialization),
   * though the nice thing about it is that there's very little effort required to save arbitrary
   * objects.
   *
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD representing deserialized data from the file(s)
   */
  def objectFile[T: ClassTag](
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[T] = withScope {
    assertNotStopped()
    sequenceFile(path, classOf[NullWritable], classOf[BytesWritable], minPartitions)
      .flatMap(x => Utils.deserialize[Array[T]](x._2.getBytes, Utils.getContextOrSparkClassLoader))
  }

  protected[spark] def checkpointFile[T: ClassTag](path: String): RDD[T] = withScope {
    new ReliableCheckpointRDD[T](this, path)
  }

  /** Build the union of a list of RDDs. */
  def union[T: ClassTag](rdds: Seq[RDD[T]]): RDD[T] = withScope {
    val nonEmptyRdds = rdds.filter(!_.partitions.isEmpty)
    val partitioners = nonEmptyRdds.flatMap(_.partitioner).toSet
    if (nonEmptyRdds.forall(_.partitioner.isDefined) && partitioners.size == 1) {
      new PartitionerAwareUnionRDD(this, nonEmptyRdds)
    } else {
      new UnionRDD(this, nonEmptyRdds)
    }
  }

  /** Build the union of a list of RDDs passed as variable-length arguments. */
  def union[T: ClassTag](first: RDD[T], rest: RDD[T]*): RDD[T] = withScope {
    union(Seq(first) ++ rest)
  }

  /** Get an RDD that has no partitions or elements. */
  def emptyRDD[T: ClassTag]: RDD[T] = new EmptyRDD[T](this)

  // Methods for creating shared variables

  /**
   * Create an [[org.apache.spark.Accumulator]] variable of a given type, which tasks can "add"
   * values to using the `+=` method. Only the driver can access the accumulator's `value`.
   */
  @deprecated("use AccumulatorV2", "2.0.0")
  def accumulator[T](initialValue: T)(implicit param: AccumulatorParam[T]): Accumulator[T] = {
    val acc = new Accumulator(initialValue, param)
    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
    acc
  }

  /**
   * Create an [[org.apache.spark.Accumulator]] variable of a given type, with a name for display
   * in the Spark UI. Tasks can "add" values to the accumulator using the `+=` method. Only the
   * driver can access the accumulator's `value`.
   */
  @deprecated("use AccumulatorV2", "2.0.0")
  def accumulator[T](initialValue: T, name: String)(implicit param: AccumulatorParam[T])
    : Accumulator[T] = {
    val acc = new Accumulator(initialValue, param, Option(name))
    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
    acc
  }

  /**
   * Create an [[org.apache.spark.Accumulable]] shared variable, to which tasks can add values
   * with `+=`. Only the driver can access the accumulable's `value`.
   * @tparam R accumulator result type
   * @tparam T type that can be added to the accumulator
   */
  @deprecated("use AccumulatorV2", "2.0.0")
  def accumulable[R, T](initialValue: R)(implicit param: AccumulableParam[R, T])
    : Accumulable[R, T] = {
    val acc = new Accumulable(initialValue, param)
    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
    acc
  }

  /**
   * Create an [[org.apache.spark.Accumulable]] shared variable, with a name for display in the
   * Spark UI. Tasks can add values to the accumulable using the `+=` operator. Only the driver can
   * access the accumulable's `value`.
   * @tparam R accumulator result type
   * @tparam T type that can be added to the accumulator
   */
  @deprecated("use AccumulatorV2", "2.0.0")
  def accumulable[R, T](initialValue: R, name: String)(implicit param: AccumulableParam[R, T])
    : Accumulable[R, T] = {
    val acc = new Accumulable(initialValue, param, Option(name))
    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
    acc
  }

  /**
   * Create an accumulator from a "mutable collection" type.
   *
   * Growable and TraversableOnce are the standard APIs that guarantee += and ++=, implemented by
   * standard mutable collections. So you can use this with mutable Map, Set, etc.
   */
  @deprecated("use AccumulatorV2", "2.0.0")
  def accumulableCollection[R <% Growable[T] with TraversableOnce[T] with Serializable: ClassTag, T]
      (initialValue: R): Accumulable[R, T] = {
    // TODO the context bound (<%) above should be replaced with simple type bound and implicit
    // conversion but is a breaking change. This should be fixed in Spark 3.x.
    val param = new GrowableAccumulableParam[R, T]
    val acc = new Accumulable(initialValue, param)
    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
    acc
  }

  /**
   * Register the given accumulator.
   *
   * @note Accumulators must be registered before use, or it will throw exception.
   */
  def register(acc: AccumulatorV2[_, _]): Unit = {
    acc.register(this)
  }

  /**
   * Register the given accumulator with given name.
   *
   * @note Accumulators must be registered before use, or it will throw exception.
   */
  def register(acc: AccumulatorV2[_, _], name: String): Unit = {
    acc.register(this, name = Option(name))
  }

  /**
   * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def longAccumulator: LongAccumulator = {
    val acc = new LongAccumulator
    register(acc)
    acc
  }

  /**
   * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def longAccumulator(name: String): LongAccumulator = {
    val acc = new LongAccumulator
    register(acc, name)
    acc
  }

  /**
   * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def doubleAccumulator: DoubleAccumulator = {
    val acc = new DoubleAccumulator
    register(acc)
    acc
  }

  /**
   * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def doubleAccumulator(name: String): DoubleAccumulator = {
    val acc = new DoubleAccumulator
    register(acc, name)
    acc
  }

  /**
   * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates
   * inputs by adding them into the list.
   */
  def collectionAccumulator[T]: CollectionAccumulator[T] = {
    val acc = new CollectionAccumulator[T]
    register(acc)
    acc
  }

  /**
   * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates
   * inputs by adding them into the list.
   */
  def collectionAccumulator[T](name: String): CollectionAccumulator[T] = {
    val acc = new CollectionAccumulator[T]
    register(acc, name)
    acc
  }

  /**
   * Broadcast a read-only variable to the cluster, returning a
   * [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions.
   * The variable will be sent to each cluster only once.
   *
   * @param value value to broadcast to the Spark nodes
   * @return `Broadcast` object, a read-only variable cached on each machine
   */
  def broadcast[T: ClassTag](value: T): Broadcast[T] = {
    assertNotStopped()
    require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass),
      "Can not directly broadcast RDDs; instead, call collect() and broadcast the result.")
    val bc = env.broadcastManager.newBroadcast[T](value, isLocal)
    val callSite = getCallSite
    logInfo("Created broadcast " + bc.id + " from " + callSite.shortForm)
    cleaner.foreach(_.registerBroadcastForCleanup(bc))
    bc
  }

  /**
   * Add a file to be downloaded with this Spark job on every node.
   *
   * If a file is added during execution, it will not be available until the next TaskSet starts.
   *
   * @param path can be either a local file, a file in HDFS (or other Hadoop-supported
   * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,
   * use `SparkFiles.get(fileName)` to find its download location.
   *
   * @note A path can be added only once. Subsequent additions of the same path are ignored.
   */
  def addFile(path: String): Unit = {
    addFile(path, false)
  }

  /**
   * Returns a list of file paths that are added to resources.
   */
  def listFiles(): Seq[String] = addedFiles.keySet.toSeq

  /**
   * Add a file to be downloaded with this Spark job on every node.
   *
   * If a file is added during execution, it will not be available until the next TaskSet starts.
   *
   * @param path can be either a local file, a file in HDFS (or other Hadoop-supported
   * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,
   * use `SparkFiles.get(fileName)` to find its download location.
   * @param recursive if true, a directory can be given in `path`. Currently directories are
   * only supported for Hadoop-supported filesystems.
   *
   * @note A path can be added only once. Subsequent additions of the same path are ignored.
   */
  def addFile(path: String, recursive: Boolean): Unit = {
    val uri = new Path(path).toUri
    val schemeCorrectedPath = uri.getScheme match {
      case null => new File(path).getCanonicalFile.toURI.toString
      case "local" =>
        logWarning("File with 'local' scheme is not supported to add to file server, since " +
          "it is already available on every node.")
        return
      case _ => path
    }

    val hadoopPath = new Path(schemeCorrectedPath)
    val scheme = new URI(schemeCorrectedPath).getScheme
    if (!Array("http", "https", "ftp").contains(scheme)) {
      val fs = hadoopPath.getFileSystem(hadoopConfiguration)
      val isDir = fs.getFileStatus(hadoopPath).isDirectory
      if (!isLocal && scheme == "file" && isDir) {
        throw new SparkException(s"addFile does not support local directories when not running " +
          "local mode.")
      }
      if (!recursive && isDir) {
        throw new SparkException(s"Added file $hadoopPath is a directory and recursive is not " +
          "turned on.")
      }
    } else {
      // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies
      Utils.validateURL(uri)
    }

    val key = if (!isLocal && scheme == "file") {
      env.rpcEnv.fileServer.addFile(new File(uri.getPath))
    } else {
      schemeCorrectedPath
    }
    val timestamp = System.currentTimeMillis
    if (addedFiles.putIfAbsent(key, timestamp).isEmpty) {
      logInfo(s"Added file $path at $key with timestamp $timestamp")
      // Fetch the file locally so that closures which are run on the driver can still use the
      // SparkFiles API to access files.
      Utils.fetchFile(uri.toString, new File(SparkFiles.getRootDirectory()), conf,
        env.securityManager, hadoopConfiguration, timestamp, useCache = false)
      postEnvironmentUpdate()
    } else {
      logWarning(s"The path $path has been added already. Overwriting of added paths " +
       "is not supported in the current version.")
    }
  }

  /**
   * :: DeveloperApi ::
   * Register a listener to receive up-calls from events that happen during execution.
   */
  @DeveloperApi
  def addSparkListener(listener: SparkListenerInterface) {
    listenerBus.addToSharedQueue(listener)
  }

  /**
   * :: DeveloperApi ::
   * Deregister the listener from Spark's listener bus.
   */
  @DeveloperApi
  def removeSparkListener(listener: SparkListenerInterface): Unit = {
    listenerBus.removeListener(listener)
  }

  private[spark] def getExecutorIds(): Seq[String] = {
    schedulerBackend match {
      case b: ExecutorAllocationClient =>
        b.getExecutorIds()
      case _ =>
        logWarning("Requesting executors is not supported by current scheduler.")
        Nil
    }
  }

  /**
   * Get the max number of tasks that can be concurrent launched currently.
   * Note that please don't cache the value returned by this method, because the number can change
   * due to add/remove executors.
   *
   * @return The max number of tasks that can be concurrent launched currently.
   */
  private[spark] def maxNumConcurrentTasks(): Int = schedulerBackend.maxNumConcurrentTasks()

  /**
   * Update the cluster manager on our scheduling needs. Three bits of information are included
   * to help it make decisions.
   * @param numExecutors The total number of executors we'd like to have. The cluster manager
   *                     shouldn't kill any running executor to reach this number, but,
   *                     if all existing executors were to die, this is the number of executors
   *                     we'd want to be allocated.
   * @param localityAwareTasks The number of tasks in all active stages that have a locality
   *                           preferences. This includes running, pending, and completed tasks.
   * @param hostToLocalTaskCount A map of hosts to the number of tasks from all active stages
   *                             that would like to like to run on that host.
   *                             This includes running, pending, and completed tasks.
   * @return whether the request is acknowledged by the cluster manager.
   */
  @DeveloperApi
  def requestTotalExecutors(
      numExecutors: Int,
      localityAwareTasks: Int,
      hostToLocalTaskCount: scala.collection.immutable.Map[String, Int]
    ): Boolean = {
    schedulerBackend match {
      case b: ExecutorAllocationClient =>
        b.requestTotalExecutors(numExecutors, localityAwareTasks, hostToLocalTaskCount)
      case _ =>
        logWarning("Requesting executors is not supported by current scheduler.")
        false
    }
  }

  /**
   * :: DeveloperApi ::
   * Request an additional number of executors from the cluster manager.
   * @return whether the request is received.
   */
  @DeveloperApi
  def requestExecutors(numAdditionalExecutors: Int): Boolean = {
    schedulerBackend match {
      case b: ExecutorAllocationClient =>
        b.requestExecutors(numAdditionalExecutors)
      case _ =>
        logWarning("Requesting executors is not supported by current scheduler.")
        false
    }
  }

  /**
   * :: DeveloperApi ::
   * Request that the cluster manager kill the specified executors.
   *
   * This is not supported when dynamic allocation is turned on.
   *
   * @note This is an indication to the cluster manager that the application wishes to adjust
   * its resource usage downwards. If the application wishes to replace the executors it kills
   * through this method with new ones, it should follow up explicitly with a call to
   * {{SparkContext#requestExecutors}}.
   *
   * @return whether the request is received.
   */
  @DeveloperApi
  def killExecutors(executorIds: Seq[String]): Boolean = {
    schedulerBackend match {
      case b: ExecutorAllocationClient =>
        require(executorAllocationManager.isEmpty,
          "killExecutors() unsupported with Dynamic Allocation turned on")
        b.killExecutors(executorIds, adjustTargetNumExecutors = true, countFailures = false,
          force = true).nonEmpty
      case _ =>
        logWarning("Killing executors is not supported by current scheduler.")
        false
    }
  }

  /**
   * :: DeveloperApi ::
   * Request that the cluster manager kill the specified executor.
   *
   * @note This is an indication to the cluster manager that the application wishes to adjust
   * its resource usage downwards. If the application wishes to replace the executor it kills
   * through this method with a new one, it should follow up explicitly with a call to
   * {{SparkContext#requestExecutors}}.
   *
   * @return whether the request is received.
   */
  @DeveloperApi
  def killExecutor(executorId: String): Boolean = killExecutors(Seq(executorId))

  /**
   * Request that the cluster manager kill the specified executor without adjusting the
   * application resource requirements.
   *
   * The effect is that a new executor will be launched in place of the one killed by
   * this request. This assumes the cluster manager will automatically and eventually
   * fulfill all missing application resource requests.
   *
   * @note The replace is by no means guaranteed; another application on the same cluster
   * can steal the window of opportunity and acquire this application's resources in the
   * mean time.
   *
   * @return whether the request is received.
   */
  private[spark] def killAndReplaceExecutor(executorId: String): Boolean = {
    schedulerBackend match {
      case b: ExecutorAllocationClient =>
        b.killExecutors(Seq(executorId), adjustTargetNumExecutors = false, countFailures = true,
          force = true).nonEmpty
      case _ =>
        logWarning("Killing executors is not supported by current scheduler.")
        false
    }
  }

  /** The version of Spark on which this application is running. */
  def version: String = SPARK_VERSION

  /**
   * Return a map from the slave to the max memory available for caching and the remaining
   * memory available for caching.
   */
  def getExecutorMemoryStatus: Map[String, (Long, Long)] = {
    assertNotStopped()
    env.blockManager.master.getMemoryStatus.map { case(blockManagerId, mem) =>
      (blockManagerId.host + ":" + blockManagerId.port, mem)
    }
  }

  /**
   * :: DeveloperApi ::
   * Return information about what RDDs are cached, if they are in mem or on disk, how much space
   * they take, etc.
   */
  @DeveloperApi
  def getRDDStorageInfo: Array[RDDInfo] = {
    getRDDStorageInfo(_ => true)
  }

  private[spark] def getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] = {
    assertNotStopped()
    val rddInfos = persistentRdds.values.filter(filter).map(RDDInfo.fromRdd).toArray
    rddInfos.foreach { rddInfo =>
      val rddId = rddInfo.id
      val rddStorageInfo = statusStore.asOption(statusStore.rdd(rddId))
      rddInfo.numCachedPartitions = rddStorageInfo.map(_.numCachedPartitions).getOrElse(0)
      rddInfo.memSize = rddStorageInfo.map(_.memoryUsed).getOrElse(0L)
      rddInfo.diskSize = rddStorageInfo.map(_.diskUsed).getOrElse(0L)
    }
    rddInfos.filter(_.isCached)
  }

  /**
   * Returns an immutable map of RDDs that have marked themselves as persistent via cache() call.
   *
   * @note This does not necessarily mean the caching or computation was successful.
   */
  def getPersistentRDDs: Map[Int, RDD[_]] = persistentRdds.toMap

  /**
   * :: DeveloperApi ::
   * Return pools for fair scheduler
   */
  @DeveloperApi
  def getAllPools: Seq[Schedulable] = {
    assertNotStopped()
    // TODO(xiajunluan): We should take nested pools into account
    taskScheduler.rootPool.schedulableQueue.asScala.toSeq
  }

  /**
   * :: DeveloperApi ::
   * Return the pool associated with the given name, if one exists
   */
  @DeveloperApi
  def getPoolForName(pool: String): Option[Schedulable] = {
    assertNotStopped()
    Option(taskScheduler.rootPool.schedulableNameToSchedulable.get(pool))
  }

  /**
   * Return current scheduling mode
   */
  def getSchedulingMode: SchedulingMode.SchedulingMode = {
    assertNotStopped()
    taskScheduler.schedulingMode
  }

  /**
   * Gets the locality information associated with the partition in a particular rdd
   * @param rdd of interest
   * @param partition to be looked up for locality
   * @return list of preferred locations for the partition
   */
  private [spark] def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = {
    dagScheduler.getPreferredLocs(rdd, partition)
  }

  /**
   * Register an RDD to be persisted in memory and/or disk storage
   */
  private[spark] def persistRDD(rdd: RDD[_]) {
    persistentRdds(rdd.id) = rdd
  }

  /**
   * Unpersist an RDD from memory and/or disk storage
   */
  private[spark] def unpersistRDD(rddId: Int, blocking: Boolean = true) {
    env.blockManager.master.removeRdd(rddId, blocking)
    persistentRdds.remove(rddId)
    listenerBus.post(SparkListenerUnpersistRDD(rddId))
  }

  /**
   * Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future.
   *
   * If a jar is added during execution, it will not be available until the next TaskSet starts.
   *
   * @param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems),
   * an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node.
   *
   * @note A path can be added only once. Subsequent additions of the same path are ignored.
   */
  def addJar(path: String) {
    def addJarFile(file: File): String = {
      try {
        if (!file.exists()) {
          throw new FileNotFoundException(s"Jar ${file.getAbsolutePath} not found")
        }
        if (file.isDirectory) {
          throw new IllegalArgumentException(
            s"Directory ${file.getAbsoluteFile} is not allowed for addJar")
        }
        env.rpcEnv.fileServer.addJar(file)
      } catch {
        case NonFatal(e) =>
          logError(s"Failed to add $path to Spark environment", e)
          null
      }
    }

    if (path == null) {
      logWarning("null specified as parameter to addJar")
    } else {
      val key = if (path.contains("\\")) {
        // For local paths with backslashes on Windows, URI throws an exception
        addJarFile(new File(path))
      } else {
        val uri = new URI(path)
        // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies
        Utils.validateURL(uri)
        uri.getScheme match {
          // A JAR file which exists only on the driver node
          case null =>
            // SPARK-22585 path without schema is not url encoded
            addJarFile(new File(uri.getRawPath))
          // A JAR file which exists only on the driver node
          case "file" => addJarFile(new File(uri.getPath))
          // A JAR file which exists locally on every worker node
          case "local" => "file:" + uri.getPath
          case _ => path
        }
      }
      if (key != null) {
        val timestamp = System.currentTimeMillis
        if (addedJars.putIfAbsent(key, timestamp).isEmpty) {
          logInfo(s"Added JAR $path at $key with timestamp $timestamp")
          postEnvironmentUpdate()
        } else {
          logWarning(s"The jar $path has been added already. Overwriting of added jars " +
            "is not supported in the current version.")
        }
      }
    }
  }

  /**
   * Returns a list of jar files that are added to resources.
   */
  def listJars(): Seq[String] = addedJars.keySet.toSeq

  /**
   * When stopping SparkContext inside Spark components, it's easy to cause dead-lock since Spark
   * may wait for some internal threads to finish. It's better to use this method to stop
   * SparkContext instead.
   */
  private[spark] def stopInNewThread(): Unit = {
    new Thread("stop-spark-context") {
      setDaemon(true)

      override def run(): Unit = {
        try {
          SparkContext.this.stop()
        } catch {
          case e: Throwable =>
            logError(e.getMessage, e)
            throw e
        }
      }
    }.start()
  }

  /**
   * Shut down the SparkContext.
   */
  def stop(): Unit = {
    if (LiveListenerBus.withinListenerThread.value) {
      throw new SparkException(s"Cannot stop SparkContext within listener bus thread.")
    }
    // Use the stopping variable to ensure no contention for the stop scenario.
    // Still track the stopped variable for use elsewhere in the code.
    if (!stopped.compareAndSet(false, true)) {
      logInfo("SparkContext already stopped.")
      return
    }
    if (_shutdownHookRef != null) {
      ShutdownHookManager.removeShutdownHook(_shutdownHookRef)
    }

    Utils.tryLogNonFatalError {
      postApplicationEnd()
    }
    Utils.tryLogNonFatalError {
      _ui.foreach(_.stop())
    }
    if (env != null) {
      Utils.tryLogNonFatalError {
        env.metricsSystem.report()
      }
    }
    Utils.tryLogNonFatalError {
      _cleaner.foreach(_.stop())
    }
    Utils.tryLogNonFatalError {
      _executorAllocationManager.foreach(_.stop())
    }
    if (_dagScheduler != null) {
      Utils.tryLogNonFatalError {
        _dagScheduler.stop()
      }
      _dagScheduler = null
    }
    if (_listenerBusStarted) {
      Utils.tryLogNonFatalError {
        listenerBus.stop()
        _listenerBusStarted = false
      }
    }
    Utils.tryLogNonFatalError {
      _eventLogger.foreach(_.stop())
    }
    if (env != null && _heartbeatReceiver != null) {
      Utils.tryLogNonFatalError {
        env.rpcEnv.stop(_heartbeatReceiver)
      }
    }
    Utils.tryLogNonFatalError {
      _progressBar.foreach(_.stop())
    }
    _taskScheduler = null
    // TODO: Cache.stop()?
    if (_env != null) {
      Utils.tryLogNonFatalError {
        _env.stop()
      }
      SparkEnv.set(null)
    }
    if (_statusStore != null) {
      _statusStore.close()
    }
    // Clear this `InheritableThreadLocal`, or it will still be inherited in child threads even this
    // `SparkContext` is stopped.
    localProperties.remove()
    // Unset YARN mode system env variable, to allow switching between cluster types.
    SparkContext.clearActiveContext()
    logInfo("Successfully stopped SparkContext")
  }


  /**
   * Get Spark's home location from either a value set through the constructor,
   * or the spark.home Java property, or the SPARK_HOME environment variable
   * (in that order of preference). If neither of these is set, return None.
   */
  private[spark] def getSparkHome(): Option[String] = {
    conf.getOption("spark.home").orElse(Option(System.getenv("SPARK_HOME")))
  }

  /**
   * Set the thread-local property for overriding the call sites
   * of actions and RDDs.
   */
  def setCallSite(shortCallSite: String) {
    setLocalProperty(CallSite.SHORT_FORM, shortCallSite)
  }

  /**
   * Set the thread-local property for overriding the call sites
   * of actions and RDDs.
   */
  private[spark] def setCallSite(callSite: CallSite) {
    setLocalProperty(CallSite.SHORT_FORM, callSite.shortForm)
    setLocalProperty(CallSite.LONG_FORM, callSite.longForm)
  }

  /**
   * Clear the thread-local property for overriding the call sites
   * of actions and RDDs.
   */
  def clearCallSite() {
    setLocalProperty(CallSite.SHORT_FORM, null)
    setLocalProperty(CallSite.LONG_FORM, null)
  }

  /**
   * Capture the current user callsite and return a formatted version for printing. If the user
   * has overridden the call site using `setCallSite()`, this will return the user's version.
   */
  private[spark] def getCallSite(): CallSite = {
    lazy val callSite = Utils.getCallSite()
    CallSite(
      Option(getLocalProperty(CallSite.SHORT_FORM)).getOrElse(callSite.shortForm),
      Option(getLocalProperty(CallSite.LONG_FORM)).getOrElse(callSite.longForm)
    )
  }

  /**
   * Run a function on a given set of partitions in an RDD and pass the results to the given
   * handler function. This is the main entry point for all actions in Spark.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @param partitions set of partitions to run on; some jobs may not want to compute on all
   * partitions of the target RDD, e.g. for operations like `first()`
   * @param resultHandler callback to pass each result to
   */
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit): Unit = {
    if (stopped.get()) {
      throw new IllegalStateException("SparkContext has been shutdown")
    }
    val callSite = getCallSite
    val cleanedFunc = clean(func)
    logInfo("Starting job: " + callSite.shortForm)
    if (conf.getBoolean("spark.logLineage", false)) {
      logInfo("RDD's recursive dependencies:\n" + rdd.toDebugString)
    }
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint()
  }

  /**
   * Run a function on a given set of partitions in an RDD and return the results as an array.
   * The function that is run against each partition additionally takes `TaskContext` argument.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @param partitions set of partitions to run on; some jobs may not want to compute on all
   * partitions of the target RDD, e.g. for operations like `first()`
   * @return in-memory collection with a result of the job (each collection element will contain
   * a result from one partition)
   */
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int]): Array[U] = {
    val results = new Array[U](partitions.size)
    runJob[T, U](rdd, func, partitions, (index, res) => results(index) = res)
    results
  }

  /**
   * Run a function on a given set of partitions in an RDD and return the results as an array.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @param partitions set of partitions to run on; some jobs may not want to compute on all
   * partitions of the target RDD, e.g. for operations like `first()`
   * @return in-memory collection with a result of the job (each collection element will contain
   * a result from one partition)
   */
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: Iterator[T] => U,
      partitions: Seq[Int]): Array[U] = {
    val cleanedFunc = clean(func)
    runJob(rdd, (ctx: TaskContext, it: Iterator[T]) => cleanedFunc(it), partitions)
  }

  /**
   * Run a job on all partitions in an RDD and return the results in an array. The function
   * that is run against each partition additionally takes `TaskContext` argument.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @return in-memory collection with a result of the job (each collection element will contain
   * a result from one partition)
   */
  def runJob[T, U: ClassTag](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = {
    runJob(rdd, func, 0 until rdd.partitions.length)
  }

  /**
   * Run a job on all partitions in an RDD and return the results in an array.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @return in-memory collection with a result of the job (each collection element will contain
   * a result from one partition)
   */
  def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = {
    runJob(rdd, func, 0 until rdd.partitions.length)
  }

  /**
   * Run a job on all partitions in an RDD and pass the results to a handler function. The function
   * that is run against each partition additionally takes `TaskContext` argument.
   *
   * @param rdd target RDD to run tasks on
   * @param processPartition a function to run on each partition of the RDD
   * @param resultHandler callback to pass each result to
   */
  def runJob[T, U: ClassTag](
    rdd: RDD[T],
    processPartition: (TaskContext, Iterator[T]) => U,
    resultHandler: (Int, U) => Unit)
  {
    runJob[T, U](rdd, processPartition, 0 until rdd.partitions.length, resultHandler)
  }

  /**
   * Run a job on all partitions in an RDD and pass the results to a handler function.
   *
   * @param rdd target RDD to run tasks on
   * @param processPartition a function to run on each partition of the RDD
   * @param resultHandler callback to pass each result to
   */
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      processPartition: Iterator[T] => U,
      resultHandler: (Int, U) => Unit)
  {
    val processFunc = (context: TaskContext, iter: Iterator[T]) => processPartition(iter)
    runJob[T, U](rdd, processFunc, 0 until rdd.partitions.length, resultHandler)
  }

  /**
   * :: DeveloperApi ::
   * Run a job that can return approximate results.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @param evaluator `ApproximateEvaluator` to receive the partial results
   * @param timeout maximum time to wait for the job, in milliseconds
   * @return partial result (how partial depends on whether the job was finished before or
   * after timeout)
   */
  @DeveloperApi
  def runApproximateJob[T, U, R](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      evaluator: ApproximateEvaluator[U, R],
      timeout: Long): PartialResult[R] = {
    assertNotStopped()
    val callSite = getCallSite
    logInfo("Starting job: " + callSite.shortForm)
    val start = System.nanoTime
    val cleanedFunc = clean(func)
    val result = dagScheduler.runApproximateJob(rdd, cleanedFunc, evaluator, callSite, timeout,
      localProperties.get)
    logInfo(
      "Job finished: " + callSite.shortForm + ", took " + (System.nanoTime - start) / 1e9 + " s")
    result
  }

  /**
   * Submit a job for execution and return a FutureJob holding the result.
   *
   * @param rdd target RDD to run tasks on
   * @param processPartition a function to run on each partition of the RDD
   * @param partitions set of partitions to run on; some jobs may not want to compute on all
   * partitions of the target RDD, e.g. for operations like `first()`
   * @param resultHandler callback to pass each result to
   * @param resultFunc function to be executed when the result is ready
   */
  def submitJob[T, U, R](
      rdd: RDD[T],
      processPartition: Iterator[T] => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit,
      resultFunc: => R): SimpleFutureAction[R] =
  {
    assertNotStopped()
    val cleanF = clean(processPartition)
    val callSite = getCallSite
    val waiter = dagScheduler.submitJob(
      rdd,
      (context: TaskContext, iter: Iterator[T]) => cleanF(iter),
      partitions,
      callSite,
      resultHandler,
      localProperties.get)
    new SimpleFutureAction(waiter, resultFunc)
  }

  /**
   * Submit a map stage for execution. This is currently an internal API only, but might be
   * promoted to DeveloperApi in the future.
   */
  private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C])
      : SimpleFutureAction[MapOutputStatistics] = {
    assertNotStopped()
    val callSite = getCallSite()
    var result: MapOutputStatistics = null
    val waiter = dagScheduler.submitMapStage(
      dependency,
      (r: MapOutputStatistics) => { result = r },
      callSite,
      localProperties.get)
    new SimpleFutureAction[MapOutputStatistics](waiter, result)
  }

  /**
   * Cancel active jobs for the specified group. See `org.apache.spark.SparkContext.setJobGroup`
   * for more information.
   */
  def cancelJobGroup(groupId: String) {
    assertNotStopped()
    dagScheduler.cancelJobGroup(groupId)
  }

  /** Cancel all jobs that have been scheduled or are running.  */
  def cancelAllJobs() {
    assertNotStopped()
    dagScheduler.cancelAllJobs()
  }

  /**
   * Cancel a given job if it's scheduled or running.
   *
   * @param jobId the job ID to cancel
   * @param reason optional reason for cancellation
   * @note Throws `InterruptedException` if the cancel message cannot be sent
   */
  def cancelJob(jobId: Int, reason: String): Unit = {
    dagScheduler.cancelJob(jobId, Option(reason))
  }

  /**
   * Cancel a given job if it's scheduled or running.
   *
   * @param jobId the job ID to cancel
   * @note Throws `InterruptedException` if the cancel message cannot be sent
   */
  def cancelJob(jobId: Int): Unit = {
    dagScheduler.cancelJob(jobId, None)
  }

  /**
   * Cancel a given stage and all jobs associated with it.
   *
   * @param stageId the stage ID to cancel
   * @param reason reason for cancellation
   * @note Throws `InterruptedException` if the cancel message cannot be sent
   */
  def cancelStage(stageId: Int, reason: String): Unit = {
    dagScheduler.cancelStage(stageId, Option(reason))
  }

  /**
   * Cancel a given stage and all jobs associated with it.
   *
   * @param stageId the stage ID to cancel
   * @note Throws `InterruptedException` if the cancel message cannot be sent
   */
  def cancelStage(stageId: Int): Unit = {
    dagScheduler.cancelStage(stageId, None)
  }

  /**
   * Kill and reschedule the given task attempt. Task ids can be obtained from the Spark UI
   * or through SparkListener.onTaskStart.
   *
   * @param taskId the task ID to kill. This id uniquely identifies the task attempt.
   * @param interruptThread whether to interrupt the thread running the task.
   * @param reason the reason for killing the task, which should be a short string. If a task
   *   is killed multiple times with different reasons, only one reason will be reported.
   *
   * @return Whether the task was successfully killed.
   */
  def killTaskAttempt(
      taskId: Long,
      interruptThread: Boolean = true,
      reason: String = "killed via SparkContext.killTaskAttempt"): Boolean = {
    dagScheduler.killTaskAttempt(taskId, interruptThread, reason)
  }

  /**
   * Clean a closure to make it ready to be serialized and sent to tasks
   * (removes unreferenced variables in $outer's, updates REPL variables)
   * If <tt>checkSerializable</tt> is set, <tt>clean</tt> will also proactively
   * check to see if <tt>f</tt> is serializable and throw a <tt>SparkException</tt>
   * if not.
   *
   * @param f the closure to clean
   * @param checkSerializable whether or not to immediately check <tt>f</tt> for serializability
   * @throws SparkException if <tt>checkSerializable</tt> is set but <tt>f</tt> is not
   *   serializable
   * @return the cleaned closure
   */
  private[spark] def clean[F <: AnyRef](f: F, checkSerializable: Boolean = true): F = {
    ClosureCleaner.clean(f, checkSerializable)
    f
  }

  /**
   * Set the directory under which RDDs are going to be checkpointed.
   * @param directory path to the directory where checkpoint files will be stored
   * (must be HDFS path if running in cluster)
   */
  def setCheckpointDir(directory: String) {

    // If we are running on a cluster, log a warning if the directory is local.
    // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from
    // its own local file system, which is incorrect because the checkpoint files
    // are actually on the executor machines.
    if (!isLocal && Utils.nonLocalPaths(directory).isEmpty) {
      logWarning("Spark is not running in local mode, therefore the checkpoint directory " +
        s"must not be on the local filesystem. Directory '$directory' " +
        "appears to be on the local filesystem.")
    }

    checkpointDir = Option(directory).map { dir =>
      val path = new Path(dir, UUID.randomUUID().toString)
      val fs = path.getFileSystem(hadoopConfiguration)
      fs.mkdirs(path)
      fs.getFileStatus(path).getPath.toString
    }
  }

  def getCheckpointDir: Option[String] = checkpointDir

  /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD). */
  def defaultParallelism: Int = {
    assertNotStopped()
    taskScheduler.defaultParallelism
  }

  /**
   * Default min number of partitions for Hadoop RDDs when not given by user
   * Notice that we use math.min so the "defaultMinPartitions" cannot be higher than 2.
   * The reasons for this are discussed in https://github.com/mesos/spark/pull/718
   */
  def defaultMinPartitions: Int = math.min(defaultParallelism, 2)

  private val nextShuffleId = new AtomicInteger(0)

  private[spark] def newShuffleId(): Int = nextShuffleId.getAndIncrement()

  private val nextRddId = new AtomicInteger(0)

  /** Register a new RDD, returning its RDD ID */
  private[spark] def newRddId(): Int = nextRddId.getAndIncrement()

  /**
   * Registers listeners specified in spark.extraListeners, then starts the listener bus.
   * This should be called after all internal listeners have been registered with the listener bus
   * (e.g. after the web UI and event logging listeners have been registered).
   */
  private def setupAndStartListenerBus(): Unit = {
    try {
      conf.get(EXTRA_LISTENERS).foreach { classNames =>
        val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf)
        listeners.foreach { listener =>
          listenerBus.addToSharedQueue(listener)
          logInfo(s"Registered listener ${listener.getClass().getName()}")
        }
      }
    } catch {
      case e: Exception =>
        try {
          stop()
        } finally {
          throw new SparkException(s"Exception when registering SparkListener", e)
        }
    }

    listenerBus.start(this, _env.metricsSystem)
    _listenerBusStarted = true
  }

  /** Post the application start event */
  private def postApplicationStart() {
    // Note: this code assumes that the task scheduler has been initialized and has contacted
    // the cluster manager to get an application ID (in case the cluster manager provides one).
    listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId),
      startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls))
  }

  /** Post the application end event */
  private def postApplicationEnd() {
    listenerBus.post(SparkListenerApplicationEnd(System.currentTimeMillis))
  }

  /** Post the environment update event once the task scheduler is ready */
  private def postEnvironmentUpdate() {
    if (taskScheduler != null) {
      val schedulingMode = getSchedulingMode.toString
      val addedJarPaths = addedJars.keys.toSeq
      val addedFilePaths = addedFiles.keys.toSeq
      val environmentDetails = SparkEnv.environmentDetails(conf, schedulingMode, addedJarPaths,
        addedFilePaths)
      val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails)
      listenerBus.post(environmentUpdate)
    }
  }

  // In order to prevent multiple SparkContexts from being active at the same time, mark this
  // context as having finished construction.
  // NOTE: this must be placed at the end of the SparkContext constructor.
  SparkContext.setActiveContext(this, allowMultipleContexts)
}

/**
 * The SparkContext object contains a number of implicit conversions and parameters for use with
 * various Spark features.
 */
object SparkContext extends Logging {
  private val VALID_LOG_LEVELS =
    Set("ALL", "DEBUG", "ERROR", "FATAL", "INFO", "OFF", "TRACE", "WARN")

  /**
   * Lock that guards access to global variables that track SparkContext construction.
   */
  private val SPARK_CONTEXT_CONSTRUCTOR_LOCK = new Object()

  /**
   * The active, fully-constructed SparkContext.  If no SparkContext is active, then this is `null`.
   *
   * Access to this field is guarded by SPARK_CONTEXT_CONSTRUCTOR_LOCK.
   */
  private val activeContext: AtomicReference[SparkContext] =
    new AtomicReference[SparkContext](null)

  /**
   * Points to a partially-constructed SparkContext if some thread is in the SparkContext
   * constructor, or `None` if no SparkContext is being constructed.
   *
   * Access to this field is guarded by SPARK_CONTEXT_CONSTRUCTOR_LOCK
   */
  private var contextBeingConstructed: Option[SparkContext] = None

  /**
   * Called to ensure that no other SparkContext is running in this JVM.
   *
   * Throws an exception if a running context is detected and logs a warning if another thread is
   * constructing a SparkContext.  This warning is necessary because the current locking scheme
   * prevents us from reliably distinguishing between cases where another context is being
   * constructed and cases where another constructor threw an exception.
   */
  private def assertNoOtherContextIsRunning(
      sc: SparkContext,
      allowMultipleContexts: Boolean): Unit = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      Option(activeContext.get()).filter(_ ne sc).foreach { ctx =>
          val errMsg = "Only one SparkContext may be running in this JVM (see SPARK-2243)." +
            " To ignore this error, set spark.driver.allowMultipleContexts = true. " +
            s"The currently running SparkContext was created at:\n${ctx.creationSite.longForm}"
          val exception = new SparkException(errMsg)
          if (allowMultipleContexts) {
            logWarning("Multiple running SparkContexts detected in the same JVM!", exception)
          } else {
            throw exception
          }
        }

      contextBeingConstructed.filter(_ ne sc).foreach { otherContext =>
        // Since otherContext might point to a partially-constructed context, guard against
        // its creationSite field being null:
        val otherContextCreationSite =
          Option(otherContext.creationSite).map(_.longForm).getOrElse("unknown location")
        val warnMsg = "Another SparkContext is being constructed (or threw an exception in its" +
          " constructor).  This may indicate an error, since only one SparkContext may be" +
          " running in this JVM (see SPARK-2243)." +
          s" The other SparkContext was created at:\n$otherContextCreationSite"
        logWarning(warnMsg)
      }
    }
  }

  /**
   * This function may be used to get or instantiate a SparkContext and register it as a
   * singleton object. Because we can only have one active SparkContext per JVM,
   * this is useful when applications may wish to share a SparkContext.
   *
   * @note This function cannot be used to create multiple SparkContext instances
   * even if multiple contexts are allowed.
   * @param config `SparkConfig` that will be used for initialisation of the `SparkContext`
   * @return current `SparkContext` (or a new one if it wasn't created before the function call)
   */
  def getOrCreate(config: SparkConf): SparkContext = {
    // Synchronize to ensure that multiple create requests don't trigger an exception
    // from assertNoOtherContextIsRunning within setActiveContext
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      if (activeContext.get() == null) {
        setActiveContext(new SparkContext(config), allowMultipleContexts = false)
      } else {
        if (config.getAll.nonEmpty) {
          logWarning("Using an existing SparkContext; some configuration may not take effect.")
        }
      }
      activeContext.get()
    }
  }

  /**
   * This function may be used to get or instantiate a SparkContext and register it as a
   * singleton object. Because we can only have one active SparkContext per JVM,
   * this is useful when applications may wish to share a SparkContext.
   *
   * This method allows not passing a SparkConf (useful if just retrieving).
   *
   * @note This function cannot be used to create multiple SparkContext instances
   * even if multiple contexts are allowed.
   * @return current `SparkContext` (or a new one if wasn't created before the function call)
   */
  def getOrCreate(): SparkContext = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      if (activeContext.get() == null) {
        setActiveContext(new SparkContext(), allowMultipleContexts = false)
      }
      activeContext.get()
    }
  }

  /** Return the current active [[SparkContext]] if any. */
  private[spark] def getActive: Option[SparkContext] = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      Option(activeContext.get())
    }
  }

  /**
   * Called at the beginning of the SparkContext constructor to ensure that no SparkContext is
   * running.  Throws an exception if a running context is detected and logs a warning if another
   * thread is constructing a SparkContext.  This warning is necessary because the current locking
   * scheme prevents us from reliably distinguishing between cases where another context is being
   * constructed and cases where another constructor threw an exception.
   */
  private[spark] def markPartiallyConstructed(
      sc: SparkContext,
      allowMultipleContexts: Boolean): Unit = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      assertNoOtherContextIsRunning(sc, allowMultipleContexts)
      contextBeingConstructed = Some(sc)
    }
  }

  /**
   * Called at the end of the SparkContext constructor to ensure that no other SparkContext has
   * raced with this constructor and started.
   */
  private[spark] def setActiveContext(
      sc: SparkContext,
      allowMultipleContexts: Boolean): Unit = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      assertNoOtherContextIsRunning(sc, allowMultipleContexts)
      contextBeingConstructed = None
      activeContext.set(sc)
    }
  }

  /**
   * Clears the active SparkContext metadata.  This is called by `SparkContext#stop()`.  It's
   * also called in unit tests to prevent a flood of warnings from test suites that don't / can't
   * properly clean up their SparkContexts.
   */
  private[spark] def clearActiveContext(): Unit = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      activeContext.set(null)
    }
  }

  private[spark] val SPARK_JOB_DESCRIPTION = "spark.job.description"
  private[spark] val SPARK_JOB_GROUP_ID = "spark.jobGroup.id"
  private[spark] val SPARK_JOB_INTERRUPT_ON_CANCEL = "spark.job.interruptOnCancel"
  private[spark] val RDD_SCOPE_KEY = "spark.rdd.scope"
  private[spark] val RDD_SCOPE_NO_OVERRIDE_KEY = "spark.rdd.scope.noOverride"

  /**
   * Executor id for the driver.  In earlier versions of Spark, this was `<driver>`, but this was
   * changed to `driver` because the angle brackets caused escaping issues in URLs and XML (see
   * SPARK-6716 for more details).
   */
  private[spark] val DRIVER_IDENTIFIER = "driver"

  /**
   * Legacy version of DRIVER_IDENTIFIER, retained for backwards-compatibility.
   */
  private[spark] val LEGACY_DRIVER_IDENTIFIER = "<driver>"

  private implicit def arrayToArrayWritable[T <: Writable : ClassTag](arr: Traversable[T])
    : ArrayWritable = {
    def anyToWritable[U <: Writable](u: U): Writable = u

    new ArrayWritable(classTag[T].runtimeClass.asInstanceOf[Class[Writable]],
        arr.map(x => anyToWritable(x)).toArray)
  }

  /**
   * Find the JAR from which a given class was loaded, to make it easy for users to pass
   * their JARs to SparkContext.
   *
   * @param cls class that should be inside of the jar
   * @return jar that contains the Class, `None` if not found
   */
  def jarOfClass(cls: Class[_]): Option[String] = {
    val uri = cls.getResource("/" + cls.getName.replace('.', '/') + ".class")
    if (uri != null) {
      val uriStr = uri.toString
      if (uriStr.startsWith("jar:file:")) {
        // URI will be of the form "jar:file:/path/foo.jar!/package/cls.class",
        // so pull out the /path/foo.jar
        Some(uriStr.substring("jar:file:".length, uriStr.indexOf('!')))
      } else {
        None
      }
    } else {
      None
    }
  }

  /**
   * Find the JAR that contains the class of a particular object, to make it easy for users
   * to pass their JARs to SparkContext. In most cases you can call jarOfObject(this) in
   * your driver program.
   *
   * @param obj reference to an instance which class should be inside of the jar
   * @return jar that contains the class of the instance, `None` if not found
   */
  def jarOfObject(obj: AnyRef): Option[String] = jarOfClass(obj.getClass)

  /**
   * Creates a modified version of a SparkConf with the parameters that can be passed separately
   * to SparkContext, to make it easier to write SparkContext's constructors. This ignores
   * parameters that are passed as the default value of null, instead of throwing an exception
   * like SparkConf would.
   */
  private[spark] def updatedConf(
      conf: SparkConf,
      master: String,
      appName: String,
      sparkHome: String = null,
      jars: Seq[String] = Nil,
      environment: Map[String, String] = Map()): SparkConf =
  {
    val res = conf.clone()
    res.setMaster(master)
    res.setAppName(appName)
    if (sparkHome != null) {
      res.setSparkHome(sparkHome)
    }
    if (jars != null && !jars.isEmpty) {
      res.setJars(jars)
    }
    res.setExecutorEnv(environment.toSeq)
    res
  }

  /**
   * The number of cores available to the driver to use for tasks such as I/O with Netty
   */
  private[spark] def numDriverCores(master: String): Int = {
    numDriverCores(master, null)
  }

  /**
   * The number of cores available to the driver to use for tasks such as I/O with Netty
   */
  private[spark] def numDriverCores(master: String, conf: SparkConf): Int = {
    def convertToInt(threads: String): Int = {
      if (threads == "*") Runtime.getRuntime.availableProcessors() else threads.toInt
    }
    master match {
      case "local" => 1
      case SparkMasterRegex.LOCAL_N_REGEX(threads) => convertToInt(threads)
      case SparkMasterRegex.LOCAL_N_FAILURES_REGEX(threads, _) => convertToInt(threads)
      case "yarn" =>
        if (conf != null && conf.getOption("spark.submit.deployMode").contains("cluster")) {
          conf.getInt("spark.driver.cores", 0)
        } else {
          0
        }
      case _ => 0 // Either driver is not being used, or its core count will be interpolated later
    }
  }

  /**
   * Create a task scheduler based on a given master URL.
   * Return a 2-tuple of the scheduler backend and the task scheduler.
   */
  private def createTaskScheduler(
      sc: SparkContext,
      master: String,
      deployMode: String): (SchedulerBackend, TaskScheduler) = {
    import SparkMasterRegex._

    // When running locally, don't try to re-execute tasks on failure.
    val MAX_LOCAL_TASK_FAILURES = 1

    master match {
      case "local" =>
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1)
        scheduler.initialize(backend)
        (backend, scheduler)

      case LOCAL_N_REGEX(threads) =>
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads.
        val threadCount = if (threads == "*") localCpuCount else threads.toInt
        if (threadCount <= 0) {
          throw new SparkException(s"Asked to run locally with $threadCount threads")
        }
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)

      case LOCAL_N_FAILURES_REGEX(threads, maxFailures) =>
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*, M] means the number of cores on the computer with M failures
        // local[N, M] means exactly N threads with M failures
        val threadCount = if (threads == "*") localCpuCount else threads.toInt
        val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)

      case SPARK_REGEX(sparkUrl) =>
        val scheduler = new TaskSchedulerImpl(sc)
        val masterUrls = sparkUrl.split(",").map("spark://" + _)
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        (backend, scheduler)

      case LOCAL_CLUSTER_REGEX(numSlaves, coresPerSlave, memoryPerSlave) =>
        // Check to make sure memory requested <= memoryPerSlave. Otherwise Spark will just hang.
        val memoryPerSlaveInt = memoryPerSlave.toInt
        if (sc.executorMemory > memoryPerSlaveInt) {
          throw new SparkException(
            "Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker".format(
              memoryPerSlaveInt, sc.executorMemory))
        }

        val scheduler = new TaskSchedulerImpl(sc)
        val localCluster = new LocalSparkCluster(
          numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt, sc.conf)
        val masterUrls = localCluster.start()
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => {
          localCluster.stop()
        }
        (backend, scheduler)

      case masterUrl =>
        val cm = getClusterManager(masterUrl) match {
          case Some(clusterMgr) => clusterMgr
          case None => throw new SparkException("Could not parse Master URL: '" + master + "'")
        }
        try {
          val scheduler = cm.createTaskScheduler(sc, masterUrl)
          val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)
          cm.initialize(scheduler, backend)
          (backend, scheduler)
        } catch {
          case se: SparkException => throw se
          case NonFatal(e) =>
            throw new SparkException("External scheduler cannot be instantiated", e)
        }
    }
  }

  private def getClusterManager(url: String): Option[ExternalClusterManager] = {
    val loader = Utils.getContextOrSparkClassLoader
    val serviceLoaders =
      ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url))
    if (serviceLoaders.size > 1) {
      throw new SparkException(
        s"Multiple external cluster managers registered for the url $url: $serviceLoaders")
    }
    serviceLoaders.headOption
  }
}

/**
 * A collection of regexes for extracting information from the master string.
 */
private object SparkMasterRegex {
  // Regular expression used for local[N] and local[*] master formats
  val LOCAL_N_REGEX = """local\[([0-9]+|\*)\]""".r
  // Regular expression for local[N, maxRetries], used in tests with failing tasks
  val LOCAL_N_FAILURES_REGEX = """local\[([0-9]+|\*)\s*,\s*([0-9]+)\]""".r
  // Regular expression for simulating a Spark cluster of [N, cores, memory] locally
  val LOCAL_CLUSTER_REGEX = """local-cluster\[\s*([0-9]+)\s*,\s*([0-9]+)\s*,\s*([0-9]+)\s*]""".r
  // Regular expression for connecting to Spark deploy clusters
  val SPARK_REGEX = """spark://(.*)""".r
}

/**
 * A class encapsulating how to convert some type `T` from `Writable`. It stores both the `Writable`
 * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the
 * conversion.
 * The getter for the writable class takes a `ClassTag[T]` in case this is a generic object
 * that doesn't know the type of `T` when it is created. This sounds strange but is necessary to
 * support converting subclasses of `Writable` to themselves (`writableWritableConverter()`).
 */
private[spark] class WritableConverter[T](
    val writableClass: ClassTag[T] => Class[_ <: Writable],
    val convert: Writable => T)
  extends Serializable

object WritableConverter {

  // Helper objects for converting common types to Writable
  private[spark] def simpleWritableConverter[T, W <: Writable: ClassTag](convert: W => T)
  : WritableConverter[T] = {
    val wClass = classTag[W].runtimeClass.asInstanceOf[Class[W]]
    new WritableConverter[T](_ => wClass, x => convert(x.asInstanceOf[W]))
  }

  // The following implicit functions were in SparkContext before 1.3 and users had to
  // `import SparkContext._` to enable them. Now we move them here to make the compiler find
  // them automatically. However, we still keep the old functions in SparkContext for backward
  // compatibility and forward to the following functions directly.

  // The following implicit declarations have been added on top of the very similar ones
  // below in order to enable compatibility with Scala 2.12. Scala 2.12 deprecates eta
  // expansion of zero-arg methods and thus won't match a no-arg method where it expects
  // an implicit that is a function of no args.

  implicit val intWritableConverterFn: () => WritableConverter[Int] =
    () => simpleWritableConverter[Int, IntWritable](_.get)

  implicit val longWritableConverterFn: () => WritableConverter[Long] =
    () => simpleWritableConverter[Long, LongWritable](_.get)

  implicit val doubleWritableConverterFn: () => WritableConverter[Double] =
    () => simpleWritableConverter[Double, DoubleWritable](_.get)

  implicit val floatWritableConverterFn: () => WritableConverter[Float] =
    () => simpleWritableConverter[Float, FloatWritable](_.get)

  implicit val booleanWritableConverterFn: () => WritableConverter[Boolean] =
    () => simpleWritableConverter[Boolean, BooleanWritable](_.get)

  implicit val bytesWritableConverterFn: () => WritableConverter[Array[Byte]] = {
    () => simpleWritableConverter[Array[Byte], BytesWritable] { bw =>
      // getBytes method returns array which is longer then data to be returned
      Arrays.copyOfRange(bw.getBytes, 0, bw.getLength)
    }
  }

  implicit val stringWritableConverterFn: () => WritableConverter[String] =
    () => simpleWritableConverter[String, Text](_.toString)

  implicit def writableWritableConverterFn[T <: Writable : ClassTag]: () => WritableConverter[T] =
    () => new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T])

  // These implicits remain included for backwards-compatibility. They fulfill the
  // same role as those above.

  implicit def intWritableConverter(): WritableConverter[Int] =
    simpleWritableConverter[Int, IntWritable](_.get)

  implicit def longWritableConverter(): WritableConverter[Long] =
    simpleWritableConverter[Long, LongWritable](_.get)

  implicit def doubleWritableConverter(): WritableConverter[Double] =
    simpleWritableConverter[Double, DoubleWritable](_.get)

  implicit def floatWritableConverter(): WritableConverter[Float] =
    simpleWritableConverter[Float, FloatWritable](_.get)

  implicit def booleanWritableConverter(): WritableConverter[Boolean] =
    simpleWritableConverter[Boolean, BooleanWritable](_.get)

  implicit def bytesWritableConverter(): WritableConverter[Array[Byte]] = {
    simpleWritableConverter[Array[Byte], BytesWritable] { bw =>
      // getBytes method returns array which is longer then data to be returned
      Arrays.copyOfRange(bw.getBytes, 0, bw.getLength)
    }
  }

  implicit def stringWritableConverter(): WritableConverter[String] =
    simpleWritableConverter[String, Text](_.toString)

  implicit def writableWritableConverter[T <: Writable](): WritableConverter[T] =
    new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T])
}

/**
 * A class encapsulating how to convert some type `T` to `Writable`. It stores both the `Writable`
 * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the
 * conversion.
 * The `Writable` class will be used in `SequenceFileRDDFunctions`.
 */
private[spark] class WritableFactory[T](
    val writableClass: ClassTag[T] => Class[_ <: Writable],
    val convert: T => Writable) extends Serializable

object WritableFactory {

  private[spark] def simpleWritableFactory[T: ClassTag, W <: Writable : ClassTag](convert: T => W)
    : WritableFactory[T] = {
    val writableClass = implicitly[ClassTag[W]].runtimeClass.asInstanceOf[Class[W]]
    new WritableFactory[T](_ => writableClass, convert)
  }

  implicit def intWritableFactory: WritableFactory[Int] =
    simpleWritableFactory(new IntWritable(_))

  implicit def longWritableFactory: WritableFactory[Long] =
    simpleWritableFactory(new LongWritable(_))

  implicit def floatWritableFactory: WritableFactory[Float] =
    simpleWritableFactory(new FloatWritable(_))

  implicit def doubleWritableFactory: WritableFactory[Double] =
    simpleWritableFactory(new DoubleWritable(_))

  implicit def booleanWritableFactory: WritableFactory[Boolean] =
    simpleWritableFactory(new BooleanWritable(_))

  implicit def bytesWritableFactory: WritableFactory[Array[Byte]] =
    simpleWritableFactory(new BytesWritable(_))

  implicit def stringWritableFactory: WritableFactory[String] =
    simpleWritableFactory(new Text(_))

  implicit def writableWritableFactory[T <: Writable: ClassTag]: WritableFactory[T] =
    simpleWritableFactory(w => w)

}

[0m2021.03.03 09:34:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:34:49 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 09:34:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:34:52 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 09:35:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:35:17 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 09:35:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:35:37 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 09:35:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:35:46 INFO  time: compiled root in 0.64s[0m
[0m2021.03.03 09:36:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:36:10 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 09:36:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:36:29 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 09:36:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:36:34 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 09:36:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:24: stale bloop error: identifier expected but string literal found.
      htmlFilter(df.as["ds"])
                       ^[0m
[0m2021.03.03 09:36:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:56:5: stale bloop error: ']' expected but '}' found.
    }
    ^[0m
[0m2021.03.03 09:36:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:24: stale bloop error: identifier expected but string literal found.
      htmlFilter(df.as["ds"])
                       ^[0m
[0m2021.03.03 09:36:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:56:5: stale bloop error: ']' expected but '}' found.
    }
    ^[0m
[0m2021.03.03 09:36:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:24: stale bloop error: identifier expected but string literal found.
      htmlFilter(df.as["ds"])
                       ^[0m
[0m2021.03.03 09:36:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:56:5: stale bloop error: ']' expected but '}' found.
    }
    ^[0m
[0m2021.03.03 09:36:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:24: stale bloop error: identifier expected but string literal found.
      htmlFilter(df.as["ds"])
                       ^[0m
[0m2021.03.03 09:36:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:56:5: stale bloop error: ']' expected but '}' found.
    }
    ^[0m
[0m2021.03.03 09:36:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:36:46 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 09:37:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:37:01 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 09:37:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:24: stale bloop error: not found: type htmlF
      htmlFilter(df.as[htmlF])
                       ^^^^^[0m
[0m2021.03.03 09:37:03 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:24: stale bloop error: not found: type htmlF
      htmlFilter(df.as[htmlF])
                       ^^^^^[0m
[0m2021.03.03 09:37:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:37:06 INFO  time: compiled root in 99ms[0m
[0m2021.03.03 09:37:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:37:23 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 09:37:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:37:35 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 09:38:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:38:05 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 09:38:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:38:09 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 09:38:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:38:16 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 09:38:18 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:38: stale bloop error: identifier expected but string literal found.
      val df = spark.read.load(e).as["ds"]
                                     ^[0m
[0m2021.03.03 09:38:18 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:1: stale bloop error: ']' expected but ';' found.
      htmlFilter(ds)
^[0m
[0m2021.03.03 09:38:18 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:38: stale bloop error: identifier expected but string literal found.
      val df = spark.read.load(e).as["ds"]
                                     ^[0m
[0m2021.03.03 09:38:18 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:1: stale bloop error: ']' expected but ';' found.
      htmlFilter(ds)
^[0m
[0m2021.03.03 09:38:18 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:38: stale bloop error: identifier expected but string literal found.
      val df = spark.read.load(e).as["ds"]
                                     ^[0m
[0m2021.03.03 09:38:18 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:1: stale bloop error: ']' expected but ';' found.
      htmlFilter(ds)
^[0m
[0m2021.03.03 09:38:19 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:38: stale bloop error: identifier expected but string literal found.
      val df = spark.read.load(e).as["ds"]
                                     ^[0m
[0m2021.03.03 09:38:19 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:1: stale bloop error: ']' expected but ';' found.
      htmlFilter(ds)
^[0m
[0m2021.03.03 09:38:18 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:38: stale bloop error: identifier expected but string literal found.
      val df = spark.read.load(e).as["ds"]
                                     ^[0m
[0m2021.03.03 09:38:18 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:1: stale bloop error: ']' expected but ';' found.
      htmlFilter(ds)
^[0m
[0m2021.03.03 09:38:19 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:54:38: stale bloop error: identifier expected but string literal found.
      val df = spark.read.load(e).as["ds"]
                                     ^[0m
[0m2021.03.03 09:38:19 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:55:1: stale bloop error: ']' expected but ';' found.
      htmlFilter(ds)
^[0m
[0m2021.03.03 09:38:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:38:22 INFO  time: compiled root in 0.16s[0m
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in org.apache.spark.sql.Dataset[String]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 1975, 1975, 1990)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in org.apache.spark.sql.Dataset[String]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 1975, 1975, 1990)
[0m2021.03.03 09:38:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:38:44 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 09:39:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:39:13 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 09:39:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:39:39 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 09:39:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:39:47 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 09:39:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:39:55 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 09:40:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:40:43 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 09:40:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:40:50 INFO  time: compiled root in 0.15s[0m
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:41:21 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7805
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 9:41:26 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7813
[0m2021.03.03 09:41:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:41:56 INFO  time: compiled root in 0.18s[0m
[0m2021.03.03 09:42:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:42:09 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 09:42:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:42:13 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 09:42:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:42:17 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 09:42:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:42:24 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 09:42:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:42:45 INFO  time: compiled root in 0.62s[0m
[0m2021.03.03 09:42:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:42:59 INFO  time: compiled root in 0.18s[0m
[0m2021.03.03 09:43:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:43:14 INFO  time: compiled root in 0.71s[0m
[0m2021.03.03 09:44:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:44:56 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 09:45:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:45:30 INFO  time: compiled root in 0.65s[0m
[0m2021.03.03 09:47:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:47:09 INFO  time: compiled root in 0.69s[0m
[0m2021.03.03 09:47:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:47:15 INFO  time: compiled root in 0.65s[0m
[0m2021.03.03 09:49:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:49:12 INFO  time: compiled root in 0.67s[0m
[0m2021.03.03 09:50:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:50:08 INFO  time: compiled root in 1.33s[0m
[0m2021.03.03 09:50:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:50:31 INFO  time: compiled root in 0.94s[0m
[0m2021.03.03 09:52:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:52:02 INFO  time: compiled root in 0.64s[0m
[0m2021.03.03 09:53:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:53:27 INFO  time: compiled root in 0.62s[0m
[0m2021.03.03 09:56:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:56:15 INFO  time: compiled root in 0.71s[0m
[0m2021.03.03 09:56:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:56:43 INFO  time: compiled root in 0.61s[0m
[0m2021.03.03 09:56:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:56:49 INFO  time: compiled root in 0.72s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.Closeable
import java.util.concurrent.atomic.AtomicReference

import scala.collection.JavaConverters._
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.spark.{SPARK_VERSION, SparkConf, SparkContext, TaskContext}
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd}
import org.apache.spark.sql.catalog.Catalog
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions.AttributeReference
import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, Range}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.internal._
import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
import org.apache.spark.sql.sources.BaseRelation
import org.apache.spark.sql.streaming._
import org.apache.spark.sql.types.{DataType, StructType}
import org.apache.spark.sql.util.ExecutionListenerManager
import org.apache.spark.util.{CallSite, Utils}


/**
 * The entry point to programming Spark with the Dataset and DataFrame API.
 *
 * In environments that this has been created upfront (e.g. REPL, notebooks), use the builder
 * to get an existing session:
 *
 * {{{
 *   SparkSession.builder().getOrCreate()
 * }}}
 *
 * The builder can also be used to create a new session:
 *
 * {{{
 *   SparkSession.builder
 *     .master("local")
 *     .appName("Word Count")
 *     .config("spark.some.config.option", "some-value")
 *     .getOrCreate()
 * }}}
 *
 * @param sparkContext The Spark context associated with this Spark session.
 * @param existingSharedState If supplied, use the existing shared state
 *                            instead of creating a new one.
 * @param parentSessionState If supplied, inherit all session state (i.e. temporary
 *                            views, SQL config, UDFs etc) from parent.
 */
@InterfaceStability.Stable
class SparkSession private(
    @transient val sparkContext: SparkContext,
    @transient private val existingSharedState: Option[SharedState],
    @transient private val parentSessionState: Option[SessionState],
    @transient private[sql] val extensions: SparkSessionExtensions)
  extends Serializable with Closeable with Logging { self =>

  // The call site where this SparkSession was constructed.
  private val creationSite: CallSite = Utils.getCallSite()

  private[sql] def this(sc: SparkContext) {
    this(sc, None, None, new SparkSessionExtensions)
  }

  sparkContext.assertNotStopped()

  // If there is no active SparkSession, uses the default SQL conf. Otherwise, use the session's.
  SQLConf.setSQLConfGetter(() => {
    SparkSession.getActiveSession.filterNot(_.sparkContext.isStopped).map(_.sessionState.conf)
      .getOrElse(SQLConf.getFallbackConf)
  })

  /**
   * The version of Spark on which this application is running.
   *
   * @since 2.0.0
   */
  def version: String = SPARK_VERSION

  /* ----------------------- *
   |  Session-related state  |
   * ----------------------- */

  /**
   * State shared across sessions, including the `SparkContext`, cached data, listener,
   * and a catalog that interacts with external systems.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sharedState: SharedState = {
    existingSharedState.getOrElse(new SharedState(sparkContext))
  }

  /**
   * Initial options for session. This options are applied once when sessionState is created.
   */
  @transient
  private[sql] val initialSessionOptions = new scala.collection.mutable.HashMap[String, String]

  /**
   * State isolated across sessions, including SQL configurations, temporary tables, registered
   * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]].
   * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sessionState: SessionState = {
    parentSessionState
      .map(_.clone(this))
      .getOrElse {
        val state = SparkSession.instantiateSessionState(
          SparkSession.sessionStateClassName(sparkContext.conf),
          self)
        initialSessionOptions.foreach { case (k, v) => state.conf.setConfString(k, v) }
        state
      }
  }

  /**
   * A wrapped version of this session in the form of a [[SQLContext]], for backward compatibility.
   *
   * @since 2.0.0
   */
  @transient
  val sqlContext: SQLContext = new SQLContext(this)

  /**
   * Runtime configuration interface for Spark.
   *
   * This is the interface through which the user can get and set all Spark and Hadoop
   * configurations that are relevant to Spark SQL. When getting the value of a config,
   * this defaults to the value set in the underlying `SparkContext`, if any.
   *
   * @since 2.0.0
   */
  @transient lazy val conf: RuntimeConfig = new RuntimeConfig(sessionState.conf)

  /**
   * :: Experimental ::
   * An interface to register custom [[org.apache.spark.sql.util.QueryExecutionListener]]s
   * that listen for execution metrics.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def listenerManager: ExecutionListenerManager = sessionState.listenerManager

  /**
   * :: Experimental ::
   * A collection of methods that are considered experimental, but can be used to hook into
   * the query planner for advanced functionality.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Unstable
  def experimental: ExperimentalMethods = sessionState.experimentalMethods

  /**
   * A collection of methods for registering user-defined functions (UDF).
   *
   * The following example registers a Scala closure as UDF:
   * {{{
   *   sparkSession.udf.register("myUDF", (arg1: Int, arg2: String) => arg2 + arg1)
   * }}}
   *
   * The following example registers a UDF in Java:
   * {{{
   *   sparkSession.udf().register("myUDF",
   *       (Integer arg1, String arg2) -> arg2 + arg1,
   *       DataTypes.StringType);
   * }}}
   *
   * @note The user-defined functions must be deterministic. Due to optimization,
   * duplicate invocations may be eliminated or the function may even be invoked more times than
   * it is present in the query.
   *
   * @since 2.0.0
   */
  def udf: UDFRegistration = sessionState.udfRegistration

  /**
   * :: Experimental ::
   * Returns a `StreamingQueryManager` that allows managing all the
   * `StreamingQuery`s active on `this`.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Unstable
  def streams: StreamingQueryManager = sessionState.streamingQueryManager

  /**
   * Start a new session with isolated SQL configurations, temporary tables, registered
   * functions are isolated, but sharing the underlying `SparkContext` and cached data.
   *
   * @note Other than the `SparkContext`, all shared state is initialized lazily.
   * This method will force the initialization of the shared state to ensure that parent
   * and child sessions are set up with the same shared state. If the underlying catalog
   * implementation is Hive, this will initialize the metastore, which may take some time.
   *
   * @since 2.0.0
   */
  def newSession(): SparkSession = {
    new SparkSession(sparkContext, Some(sharedState), parentSessionState = None, extensions)
  }

  /**
   * Create an identical copy of this `SparkSession`, sharing the underlying `SparkContext`
   * and shared state. All the state of this session (i.e. SQL configurations, temporary tables,
   * registered functions) is copied over, and the cloned session is set up with the same shared
   * state as this session. The cloned session is independent of this session, that is, any
   * non-global change in either session is not reflected in the other.
   *
   * @note Other than the `SparkContext`, all shared state is initialized lazily.
   * This method will force the initialization of the shared state to ensure that parent
   * and child sessions are set up with the same shared state. If the underlying catalog
   * implementation is Hive, this will initialize the metastore, which may take some time.
   */
  private[sql] def cloneSession(): SparkSession = {
    val result = new SparkSession(sparkContext, Some(sharedState), Some(sessionState), extensions)
    result.sessionState // force copy of SessionState
    result
  }


  /* --------------------------------- *
   |  Methods for creating DataFrames  |
   * --------------------------------- */

  /**
   * Returns a `DataFrame` with no rows or columns.
   *
   * @since 2.0.0
   */
  @transient
  lazy val emptyDataFrame: DataFrame = {
    createDataFrame(sparkContext.emptyRDD[Row].setName("empty"), StructType(Nil))
  }

  /**
   * :: Experimental ::
   * Creates a new [[Dataset]] of type T containing zero elements.
   *
   * @return 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def emptyDataset[T: Encoder]: Dataset[T] = {
    val encoder = implicitly[Encoder[T]]
    new Dataset(self, LocalRelation(encoder.schema.toAttributes), encoder)
  }

  /**
   * :: Experimental ::
   * Creates a `DataFrame` from an RDD of Product (e.g. case classes, tuples).
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = {
    SparkSession.setActiveSession(this)
    val encoder = Encoders.product[A]
    Dataset.ofRows(self, ExternalRDD(rdd, self)(encoder))
  }

  /**
   * :: Experimental ::
   * Creates a `DataFrame` from a local Seq of Product.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataFrame[A <: Product : TypeTag](data: Seq[A]): DataFrame = {
    SparkSession.setActiveSession(this)
    val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]
    val attributeSeq = schema.toAttributes
    Dataset.ofRows(self, LocalRelation.fromProduct(attributeSeq, data))
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from an `RDD` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided RDD matches
   * the provided schema. Otherwise, there will be runtime exception.
   * Example:
   * {{{
   *  import org.apache.spark.sql._
   *  import org.apache.spark.sql.types._
   *  val sparkSession = new org.apache.spark.sql.SparkSession(sc)
   *
   *  val schema =
   *    StructType(
   *      StructField("name", StringType, false) ::
   *      StructField("age", IntegerType, true) :: Nil)
   *
   *  val people =
   *    sc.textFile("examples/src/main/resources/people.txt").map(
   *      _.split(",")).map(p => Row(p(0), p(1).trim.toInt))
   *  val dataFrame = sparkSession.createDataFrame(people, schema)
   *  dataFrame.printSchema
   *  // root
   *  // |-- name: string (nullable = false)
   *  // |-- age: integer (nullable = true)
   *
   *  dataFrame.createOrReplaceTempView("people")
   *  sparkSession.sql("select name from people").collect.foreach(println)
   * }}}
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame = {
    createDataFrame(rowRDD, schema, needsConversion = true)
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from a `JavaRDD` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided RDD matches
   * the provided schema. Otherwise, there will be runtime exception.
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rowRDD: JavaRDD[Row], schema: StructType): DataFrame = {
    createDataFrame(rowRDD.rdd, schema)
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from a `java.util.List` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided List matches
   * the provided schema. Otherwise, there will be runtime exception.
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rows: java.util.List[Row], schema: StructType): DataFrame = {
    Dataset.ofRows(self, LocalRelation.fromExternalRows(schema.toAttributes, rows.asScala))
  }

  /**
   * Applies a schema to an RDD of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   * SELECT * queries will return the columns in an undefined order.
   *
   * @since 2.0.0
   */
  def createDataFrame(rdd: RDD[_], beanClass: Class[_]): DataFrame = {
    val attributeSeq: Seq[AttributeReference] = getSchema(beanClass)
    val className = beanClass.getName
    val rowRdd = rdd.mapPartitions { iter =>
    // BeanInfo is not serializable so we must rediscover it remotely for each partition.
      SQLContext.beansToRows(iter, Utils.classForName(className), attributeSeq)
    }
    Dataset.ofRows(self, LogicalRDD(attributeSeq, rowRdd.setName(rdd.name))(self))
  }

  /**
   * Applies a schema to an RDD of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   * SELECT * queries will return the columns in an undefined order.
   *
   * @since 2.0.0
   */
  def createDataFrame(rdd: JavaRDD[_], beanClass: Class[_]): DataFrame = {
    createDataFrame(rdd.rdd, beanClass)
  }

  /**
   * Applies a schema to a List of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   *          SELECT * queries will return the columns in an undefined order.
   * @since 1.6.0
   */
  def createDataFrame(data: java.util.List[_], beanClass: Class[_]): DataFrame = {
    val attrSeq = getSchema(beanClass)
    val rows = SQLContext.beansToRows(data.asScala.iterator, beanClass, attrSeq)
    Dataset.ofRows(self, LocalRelation(attrSeq, rows.toSeq))
  }

  /**
   * Convert a `BaseRelation` created for external data sources into a `DataFrame`.
   *
   * @since 2.0.0
   */
  def baseRelationToDataFrame(baseRelation: BaseRelation): DataFrame = {
    Dataset.ofRows(self, LogicalRelation(baseRelation))
  }

  /* ------------------------------- *
   |  Methods for creating DataSets  |
   * ------------------------------- */

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from a local Seq of data of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * == Example ==
   *
   * {{{
   *
   *   import spark.implicits._
   *   case class Person(name: String, age: Long)
   *   val data = Seq(Person("Michael", 29), Person("Andy", 30), Person("Justin", 19))
   *   val ds = spark.createDataset(data)
   *
   *   ds.show()
   *   // +-------+---+
   *   // |   name|age|
   *   // +-------+---+
   *   // |Michael| 29|
   *   // |   Andy| 30|
   *   // | Justin| 19|
   *   // +-------+---+
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: Seq[T]): Dataset[T] = {
    // `ExpressionEncoder` is not thread-safe, here we create a new encoder.
    val enc = encoderFor[T].copy()
    val attributes = enc.schema.toAttributes
    val encoded = data.map(d => enc.toRow(d).copy())
    val plan = new LocalRelation(attributes, encoded)
    Dataset[T](self, plan)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from an RDD of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: RDD[T]): Dataset[T] = {
    Dataset[T](self, ExternalRDD(data, self))
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from a `java.util.List` of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * == Java Example ==
   *
   * {{{
   *     List<String> data = Arrays.asList("hello", "world");
   *     Dataset<String> ds = spark.createDataset(data, Encoders.STRING());
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: java.util.List[T]): Dataset[T] = {
    createDataset(data.asScala)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from 0 to `end` (exclusive) with step value 1.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(end: Long): Dataset[java.lang.Long] = range(0, end)

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with step value 1.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long): Dataset[java.lang.Long] = {
    range(start, end, step = 1, numPartitions = sparkContext.defaultParallelism)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with a step value.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long, step: Long): Dataset[java.lang.Long] = {
    range(start, end, step, numPartitions = sparkContext.defaultParallelism)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with a step value, with partition number
   * specified.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long] = {
    new Dataset(self, Range(start, end, step, numPartitions), Encoders.LONG)
  }

  /**
   * Creates a `DataFrame` from an `RDD[InternalRow]`.
   */
  private[sql] def internalCreateDataFrame(
      catalystRows: RDD[InternalRow],
      schema: StructType,
      isStreaming: Boolean = false): DataFrame = {
    // TODO: use MutableProjection when rowRDD is another DataFrame and the applied
    // schema differs from the existing schema on any field data type.
    val logicalPlan = LogicalRDD(
      schema.toAttributes,
      catalystRows,
      isStreaming = isStreaming)(self)
    Dataset.ofRows(self, logicalPlan)
  }

  /**
   * Creates a `DataFrame` from an `RDD[Row]`.
   * User can specify whether the input rows should be converted to Catalyst rows.
   */
  private[sql] def createDataFrame(
      rowRDD: RDD[Row],
      schema: StructType,
      needsConversion: Boolean) = {
    // TODO: use MutableProjection when rowRDD is another DataFrame and the applied
    // schema differs from the existing schema on any field data type.
    val catalystRows = if (needsConversion) {
      val encoder = RowEncoder(schema)
      rowRDD.map(encoder.toRow)
    } else {
      rowRDD.map { r: Row => InternalRow.fromSeq(r.toSeq) }
    }
    internalCreateDataFrame(catalystRows.setName(rowRDD.name), schema)
  }


  /* ------------------------- *
   |  Catalog-related methods  |
   * ------------------------- */

  /**
   * Interface through which the user may create, drop, alter or query underlying
   * databases, tables, functions etc.
   *
   * @since 2.0.0
   */
  @transient lazy val catalog: Catalog = new CatalogImpl(self)

  /**
   * Returns the specified table/view as a `DataFrame`.
   *
   * @param tableName is either a qualified or unqualified name that designates a table or view.
   *                  If a database is specified, it identifies the table/view from the database.
   *                  Otherwise, it first attempts to find a temporary view with the given name
   *                  and then match the table/view from the current database.
   *                  Note that, the global temporary view database is also valid here.
   * @since 2.0.0
   */
  def table(tableName: String): DataFrame = {
    table(sessionState.sqlParser.parseTableIdentifier(tableName))
  }

  private[sql] def table(tableIdent: TableIdentifier): DataFrame = {
    Dataset.ofRows(self, UnresolvedRelation(tableIdent))
  }

  /* ----------------- *
   |  Everything else  |
   * ----------------- */

  /**
   * Executes a SQL query using Spark, returning the result as a `DataFrame`.
   * The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'.
   *
   * @since 2.0.0
   */
  def sql(sqlText: String): DataFrame = {
    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))
  }

  /**
   * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a
   * `DataFrame`.
   * {{{
   *   sparkSession.read.parquet("/path/to/file.parquet")
   *   sparkSession.read.schema(schema).json("/path/to/file.json")
   * }}}
   *
   * @since 2.0.0
   */
  def read: DataFrameReader = new DataFrameReader(self)

  /**
   * Returns a `DataStreamReader` that can be used to read streaming data in as a `DataFrame`.
   * {{{
   *   sparkSession.readStream.parquet("/path/to/directory/of/parquet/files")
   *   sparkSession.readStream.schema(schema).json("/path/to/directory/of/json/files")
   * }}}
   *
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def readStream: DataStreamReader = new DataStreamReader(self)

  /**
   * Executes some code block and prints to stdout the time taken to execute the block. This is
   * available in Scala only and is used primarily for interactive testing and debugging.
   *
   * @since 2.1.0
   */
  def time[T](f: => T): T = {
    val start = System.nanoTime()
    val ret = f
    val end = System.nanoTime()
    // scalastyle:off println
    println(s"Time taken: ${(end - start) / 1000 / 1000} ms")
    // scalastyle:on println
    ret
  }

  // scalastyle:off
  // Disable style checker so "implicits" object can start with lowercase i
  /**
   * :: Experimental ::
   * (Scala-specific) Implicit methods available in Scala for converting
   * common Scala objects into `DataFrame`s.
   *
   * {{{
   *   val sparkSession = SparkSession.builder.getOrCreate()
   *   import sparkSession.implicits._
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  object implicits extends SQLImplicits with Serializable {
    protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext
  }
  // scalastyle:on

  /**
   * Stop the underlying `SparkContext`.
   *
   * @since 2.0.0
   */
  def stop(): Unit = {
    sparkContext.stop()
  }

  /**
   * Synonym for `stop()`.
   *
   * @since 2.1.0
   */
  override def close(): Unit = stop()

  /**
   * Parses the data type in our internal string representation. The data type string should
   * have the same format as the one generated by `toString` in scala.
   * It is only used by PySpark.
   */
  protected[sql] def parseDataType(dataTypeString: String): DataType = {
    DataType.fromJson(dataTypeString)
  }

  /**
   * Apply a schema defined by the schemaString to an RDD. It is only used by PySpark.
   */
  private[sql] def applySchemaToPythonRDD(
      rdd: RDD[Array[Any]],
      schemaString: String): DataFrame = {
    val schema = DataType.fromJson(schemaString).asInstanceOf[StructType]
    applySchemaToPythonRDD(rdd, schema)
  }

  /**
   * Apply `schema` to an RDD.
   *
   * @note Used by PySpark only
   */
  private[sql] def applySchemaToPythonRDD(
      rdd: RDD[Array[Any]],
      schema: StructType): DataFrame = {
    val rowRdd = rdd.mapPartitions { iter =>
      val fromJava = python.EvaluatePython.makeFromJava(schema)
      iter.map(r => fromJava(r).asInstanceOf[InternalRow])
    }
    internalCreateDataFrame(rowRdd, schema)
  }

  /**
   * Returns a Catalyst Schema for the given java bean class.
   */
  private def getSchema(beanClass: Class[_]): Seq[AttributeReference] = {
    val (dataType, _) = JavaTypeInference.inferDataType(beanClass)
    dataType.asInstanceOf[StructType].fields.map { f =>
      AttributeReference(f.name, f.dataType, f.nullable)()
    }
  }

}


@InterfaceStability.Stable
object SparkSession extends Logging {

  /**
   * Builder for [[SparkSession]].
   */
  @InterfaceStability.Stable
  class Builder extends Logging {

    private[this] val options = new scala.collection.mutable.HashMap[String, String]

    private[this] val extensions = new SparkSessionExtensions

    private[this] var userSuppliedContext: Option[SparkContext] = None

    private[spark] def sparkContext(sparkContext: SparkContext): Builder = synchronized {
      userSuppliedContext = Option(sparkContext)
      this
    }

    /**
     * Sets a name for the application, which will be shown in the Spark web UI.
     * If no application name is set, a randomly generated name will be used.
     *
     * @since 2.0.0
     */
    def appName(name: String): Builder = config("spark.app.name", name)

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: String): Builder = synchronized {
      options += key -> value
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Long): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Double): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Boolean): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a list of config options based on the given `SparkConf`.
     *
     * @since 2.0.0
     */
    def config(conf: SparkConf): Builder = synchronized {
      conf.getAll.foreach { case (k, v) => options += k -> v }
      this
    }

    /**
     * Sets the Spark master URL to connect to, such as "local" to run locally, "local[4]" to
     * run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone cluster.
     *
     * @since 2.0.0
     */
    def master(master: String): Builder = config("spark.master", master)

    /**
     * Enables Hive support, including connectivity to a persistent Hive metastore, support for
     * Hive serdes, and Hive user-defined functions.
     *
     * @since 2.0.0
     */
    def enableHiveSupport(): Builder = synchronized {
      if (hiveClassesArePresent) {
        config(CATALOG_IMPLEMENTATION.key, "hive")
      } else {
        throw new IllegalArgumentException(
          "Unable to instantiate SparkSession with Hive support because " +
            "Hive classes are not found.")
      }
    }

    /**
     * Inject extensions into the [[SparkSession]]. This allows a user to add Analyzer rules,
     * Optimizer rules, Planning Strategies or a customized parser.
     *
     * @since 2.2.0
     */
    def withExtensions(f: SparkSessionExtensions => Unit): Builder = synchronized {
      f(extensions)
      this
    }

    /**
     * Gets an existing [[SparkSession]] or, if there is no existing one, creates a new
     * one based on the options set in this builder.
     *
     * This method first checks whether there is a valid thread-local SparkSession,
     * and if yes, return that one. It then checks whether there is a valid global
     * default SparkSession, and if yes, return that one. If no valid global default
     * SparkSession exists, the method creates a new SparkSession and assigns the
     * newly created SparkSession as the global default.
     *
     * In case an existing SparkSession is returned, the non-static config options specified in
     * this builder will be applied to the existing SparkSession.
     *
     * @since 2.0.0
     */
    def getOrCreate(): SparkSession = synchronized {
      assertOnDriver()
      // Get the session from current thread's active session.
      var session = activeThreadSession.get()
      if ((session ne null) && !session.sparkContext.isStopped) {
        applyModifiableSettings(session)
        return session
      }

      // Global synchronization so we will only set the default session once.
      SparkSession.synchronized {
        // If the current thread does not have an active session, get it from the global session.
        session = defaultSession.get()
        if ((session ne null) && !session.sparkContext.isStopped) {
          applyModifiableSettings(session)
          return session
        }

        // No active nor global default session. Create a new one.
        val sparkContext = userSuppliedContext.getOrElse {
          val sparkConf = new SparkConf()
          options.foreach { case (k, v) => sparkConf.set(k, v) }

          // set a random app name if not given.
          if (!sparkConf.contains("spark.app.name")) {
            sparkConf.setAppName(java.util.UUID.randomUUID().toString)
          }

          SparkContext.getOrCreate(sparkConf)
          // Do not update `SparkConf` for existing `SparkContext`, as it's shared by all sessions.
        }

        // Initialize extensions if the user has defined a configurator class.
        val extensionConfOption = sparkContext.conf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS)
        if (extensionConfOption.isDefined) {
          val extensionConfClassName = extensionConfOption.get
          try {
            val extensionConfClass = Utils.classForName(extensionConfClassName)
            val extensionConf = extensionConfClass.newInstance()
              .asInstanceOf[SparkSessionExtensions => Unit]
            extensionConf(extensions)
          } catch {
            // Ignore the error if we cannot find the class or when the class has the wrong type.
            case e @ (_: ClassCastException |
                      _: ClassNotFoundException |
                      _: NoClassDefFoundError) =>
              logWarning(s"Cannot use $extensionConfClassName to configure session extensions.", e)
          }
        }

        session = new SparkSession(sparkContext, None, None, extensions)
        options.foreach { case (k, v) => session.initialSessionOptions.put(k, v) }
        setDefaultSession(session)
        setActiveSession(session)

        // Register a successfully instantiated context to the singleton. This should be at the
        // end of the class definition so that the singleton is updated only if there is no
        // exception in the construction of the instance.
        sparkContext.addSparkListener(new SparkListener {
          override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {
            defaultSession.set(null)
          }
        })
      }

      return session
    }

    private def applyModifiableSettings(session: SparkSession): Unit = {
      val (staticConfs, otherConfs) =
        options.partition(kv => SQLConf.staticConfKeys.contains(kv._1))

      otherConfs.foreach { case (k, v) => session.sessionState.conf.setConfString(k, v) }

      if (staticConfs.nonEmpty) {
        logWarning("Using an existing SparkSession; the static sql configurations will not take" +
          " effect.")
      }
      if (otherConfs.nonEmpty) {
        logWarning("Using an existing SparkSession; some spark core configurations may not take" +
          " effect.")
      }
    }
  }

  /**
   * Creates a [[SparkSession.Builder]] for constructing a [[SparkSession]].
   *
   * @since 2.0.0
   */
  def builder(): Builder = new Builder

  /**
   * Changes the SparkSession that will be returned in this thread and its children when
   * SparkSession.getOrCreate() is called. This can be used to ensure that a given thread receives
   * a SparkSession with an isolated session, instead of the global (first created) context.
   *
   * @since 2.0.0
   */
  def setActiveSession(session: SparkSession): Unit = {
    activeThreadSession.set(session)
  }

  /**
   * Clears the active SparkSession for current thread. Subsequent calls to getOrCreate will
   * return the first created context instead of a thread-local override.
   *
   * @since 2.0.0
   */
  def clearActiveSession(): Unit = {
    activeThreadSession.remove()
  }

  /**
   * Sets the default SparkSession that is returned by the builder.
   *
   * @since 2.0.0
   */
  def setDefaultSession(session: SparkSession): Unit = {
    defaultSession.set(session)
  }

  /**
   * Clears the default SparkSession that is returned by the builder.
   *
   * @since 2.0.0
   */
  def clearDefaultSession(): Unit = {
    defaultSession.set(null)
  }

  /**
   * Returns the active SparkSession for the current thread, returned by the builder.
   *
   * @note Return None, when calling this function on executors
   *
   * @since 2.2.0
   */
  def getActiveSession: Option[SparkSession] = {
    if (TaskContext.get != null) {
      // Return None when running on executors.
      None
    } else {
      Option(activeThreadSession.get)
    }
  }

  /**
   * Returns the default SparkSession that is returned by the builder.
   *
   * @note Return None, when calling this function on executors
   *
   * @since 2.2.0
   */
  def getDefaultSession: Option[SparkSession] = {
    if (TaskContext.get != null) {
      // Return None when running on executors.
      None
    } else {
      Option(defaultSession.get)
    }
  }

  /**
   * Returns the currently active SparkSession, otherwise the default one. If there is no default
   * SparkSession, throws an exception.
   *
   * @since 2.4.0
   */
  def active: SparkSession = {
    getActiveSession.getOrElse(getDefaultSession.getOrElse(
      throw new IllegalStateException("No active or default Spark session found")))
  }

  ////////////////////////////////////////////////////////////////////////////////////////
  // Private methods from now on
  ////////////////////////////////////////////////////////////////////////////////////////

  /** The active SparkSession for the current thread. */
  private val activeThreadSession = new InheritableThreadLocal[SparkSession]

  /** Reference to the root SparkSession. */
  private val defaultSession = new AtomicReference[SparkSession]

  private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME =
    "org.apache.spark.sql.hive.HiveSessionStateBuilder"

  private def sessionStateClassName(conf: SparkConf): String = {
    conf.get(CATALOG_IMPLEMENTATION) match {
      case "hive" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME
      case "in-memory" => classOf[SessionStateBuilder].getCanonicalName
    }
  }

  private def assertOnDriver(): Unit = {
    if (Utils.isTesting && TaskContext.get != null) {
      // we're accessing it during task execution, fail.
      throw new IllegalStateException(
        "SparkSession should only be created and accessed on the driver.")
    }
  }

  /**
   * Helper method to create an instance of `SessionState` based on `className` from conf.
   * The result is either `SessionState` or a Hive based `SessionState`.
   */
  private def instantiateSessionState(
      className: String,
      sparkSession: SparkSession): SessionState = {
    try {
      // invoke `new [Hive]SessionStateBuilder(SparkSession, Option[SessionState])`
      val clazz = Utils.classForName(className)
      val ctor = clazz.getConstructors.head
      ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build()
    } catch {
      case NonFatal(e) =>
        throw new IllegalArgumentException(s"Error while instantiating '$className':", e)
    }
  }

  /**
   * @return true if Hive classes can be loaded, otherwise false.
   */
  private[spark] def hiveClassesArePresent: Boolean = {
    try {
      Utils.classForName(HIVE_SESSION_STATE_BUILDER_CLASS_NAME)
      Utils.classForName("org.apache.hadoop.hive.conf.HiveConf")
      true
    } catch {
      case _: ClassNotFoundException | _: NoClassDefFoundError => false
    }
  }

  private[spark] def cleanupAnyExistingSession(): Unit = {
    val session = getActiveSession.orElse(getDefaultSession)
    if (session.isDefined) {
      logWarning(
        s"""An existing Spark session exists as the active or default session.
           |This probably means another suite leaked it. Attempting to stop it before continuing.
           |This existing Spark session was created at:
           |
           |${session.get.creationSite.longForm}
           |
         """.stripMargin)
      session.get.stop()
      SparkSession.clearActiveSession()
      SparkSession.clearDefaultSession()
    }
  }
}

[0m2021.03.03 09:59:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 09:59:05 INFO  time: compiled root in 0.7s[0m
[0m2021.03.03 10:00:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:00:47 INFO  time: compiled root in 0.76s[0m
[0m2021.03.03 10:01:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:01:29 INFO  time: compiled root in 0.7s[0m
[0m2021.03.03 10:02:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:02:56 INFO  time: compiled root in 0.67s[0m
[0m2021.03.03 10:04:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:04:20 INFO  time: compiled root in 0.63s[0m
Mar 03, 2021 10:04:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8537
[0m2021.03.03 10:04:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:04:38 INFO  time: compiled root in 0.62s[0m
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 10:06:13 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8589
package com.revature.hellospark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

/**
  * Runner is our driver application that describes and kicks off the Spark job.
  * This is the driver program mentioned in spark docs.  It can communicate with
  * a Spark cluster -- the location of the cluster should be set in its SparkConf
  */
object Runner {
  def main(args: Array[String]): Unit = {
    //Set up a SparkConf here
    // This should almost certainly take command line arguments, but we'll hardcode for now
    // we're only setting two properties on the conf, the name and the master
    // Appname is straightforward
    // Master is the spark cluster we're communicating with.  We can pass setMaster the URL for a
    // cluster, then our job will be submitted to that cluster.  We can also specify that the job should
    // run locally.  We do this by setting the master to local[n], where n is the number of threads we
    // want Spark to use.
    val conf = new SparkConf().setAppName("hello-spark").setMaster("local[2]")
    //Build a SparkContext using that conf
    // SparkContext is the entrypoint for Spark functionality.
    // more specifically, SparkContext is the entrypoint for working with RDDs, the central abstraction
    // we'll see later on other ways of working with Spark, but those will be built on top of RDDs
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR") // setting the log level to ERROR means that log messages below
    // ERROR won't appear in the output
    // log levels (kind of standard): OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL

    helloDemo(sc)

    fileDemo(sc)

    closureDemo(sc)

    mapReduceWordCountDemo(sc)

  }

  def mapReduceWordCountDemo(sc: SparkContext) = {
    // we'll just quickly write the spark version of our mapreduce word count
    // MapReduce sorts during the shuffle and sort
    // shuffles in Spark don't necessarily sort, so we'll have to do it manually to get the same output

    val mrwcRdd = sc.textFile("somelines.txt")
      .flatMap(_.split("\\s+")) 
      //split lines on spaces and flatmap to words
      .filter(_.length() > 0)
      .map((_, 1)) 
      //map words to key, value pairs: (word, 1)
      .reduceByKey(_ + _)
      // a shuffle happens when we reduce by key!
      .sortByKey()

    // all the above are transformations, so nothing has happened yet
    // let's get some results and print them out, take(10) is the action:
    mrwcRdd.take(10).foreach(println)

    // We've mentioned that RDDs contain their lineage -- the process that produced them
    // we can see the lineage for any RDD by checking out its debugstring
    println(mrwcRdd.toDebugString)

    // each indentation is a *stage*, *stages* contain tasks and we get a new stage every time we shuffle
    // The number in () for each stage is the number of partitions at that stage.

//     (2) ShuffledRDD[17] at sortByKey at Runner.scala:54 []
//        +-(2) ShuffledRDD[14] at reduceByKey at Runner.scala:52 []
//          +-(2) MapPartitionsRDD[13] at map at Runner.scala:50 []
//              |  MapPartitionsRDD[12] at filter at Runner.scala:49 []
//              |  MapPartitionsRDD[11] at flatMap at Runner.scala:47 []
//              |  somelines.txt MapPartitionsRDD[10] at textFile at Runner.scala:46 []
//              |  somelines.txt HadoopRDD[9] at textFile at Runner.scala:46 []
// one job, 3 stages, 7 tasks.  Each stage has 2 partitions

    
  }

  def closureDemo(sc: SparkContext) = {
    // operations are broken up into tasks that run on the cluster
    // these tasks rely on values in memory
    // the *closure* of a task is all the variables and methods that the executor 
    // (the thing that runs the task) needs to complete the task.  Spark handles this for us
    // but we should know about it.
    // What we write as one value in our driver application here
    // become multiple values in memory distributed across the cluster

    //4 examples, using good and bad methods of having shared variables
    val listRdd = sc.parallelize(List(1,3,5,7,9), 3)

    //compute a sum, naive (bad) version:
    var sum = 0;
    //foreach is an action
    listRdd.foreach(sum += _)

    //behaviour is undefined, *may* work locally?
    println(s"bad sum is: $sum")

    //all of the addition to sum happened in tasks spread out across the cluster, no way
    // to retrieve/combine those values

    //compute a sum, good version:
    val sumAccumulator = sc.longAccumulator("goodsum")

    listRdd.foreach(sumAccumulator.add(_))

    println(s"good sum is: ${sumAccumulator.value}")

    //The next 2 examples use broadcast.  Not using a broadcast variable when you ought to
    // isn't catastrophic, it will just make your jobs slower.  When exactly to use a broadcast variable
    // is a bit of a judgment call, it depends on the job and the cluster.

    //Example: filter a list based on a cutoff value shared across the cluter

    val cutoff = 4

    println(s"OK List: ${listRdd.filter(_ > cutoff).collect().mkString(" ")}")

    //the above works just fine.  our cutoff val is copied and sent along with *every* task

    val cutoffBroadcast = sc.broadcast(4)

    println(s"good List: ${listRdd.filter(_ > cutoffBroadcast.value).collect().mkString(" ")}")

    //good list and OK list are going to be the same
    // the major advantage of broadcast variables appears when you have a *large* read only value.

  }

  def fileDemo(sc: SparkContext) = {
    //another way to create an RDD: from file.  second arg is suggested minimum partitions
    val distributedFile = sc.textFile("somelines.txt", 3)

    val lineLengths = distributedFile.map(_.length())

    //reduce is another action.  Use to take all the values in the RDD and reduce them to a single
    // value.  Reduce is a higher order function, meaning it takes a function to describe
    // this reduction
    println(s"Total Line Lengths: ${lineLengths.reduce(_ + _)}")
  }

  def helloDemo(sc: SparkContext) = {
    // From the paper, 4 ways of creating RDDs:
    // 1) from file, 2) from Scala collection,
    //   3) from transforming prior RDD, 4) changing persistence of RDD
    val data = List(1,2,3,4,5,6)

    //parallelize our Scala collection:
    val distributedData = sc.parallelize(data)

    //transform an existing RDD
    val dataPlusTwo = distributedData.map(_ + 2)

    //another transform
    val sampledData = distributedData.sample(false, 0.5, 11L)

    val cachedData = sampledData.cache()

    //Up until this point, we've been producing RDDs
    // RDDs are lazy, so while we've specified them, nothing will actually run.
    // We cause RDDs to run when we perform an *action* on them.  Actions
    // make use of the results of RDDs, which means the RDDs actually run.

    //Let's use collect
    println("Collect output:")
    println(cachedData.collect().mkString(" "))

    println("hello demo debugstring:")
    println(cachedData.toDebugString)

  }
}
Mar 03, 2021 10:06:22 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8594
[0m2021.03.03 10:06:43 WARN  no build target for: /home/amburkee/tmp/210104-usf-bigdata/week7/s3example/src/main/scala/com/revature/s3example/Runner.scala[0m
package com.revature.s3example

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    //Let's write a spark application to read from S3, do some processing, and write to S3
    // This is meant to run on EMR, so it will *not* work locally.
    // The s3 paths that we use require deps we're not including
    //If we wanted to use S3 files locally, we'd use s3a + configure access to s3 from our machine

    val spark = SparkSession.builder()
      .appName("S3 example")
      .getOrCreate()

    import spark.implicits._

    //create a dataframe from file, but the file path is on S3
    spark.read.json("s3://bigdata-pj2-teamadam/data/tweets.json")
      .select($"data.text")
      .write
      .text("s3://bigdata-pj2-teamadam/data/tweets-content.txt")
      //we may need to fiddle with permissions to make this work ^
      // We can give the machines in our EMR cluster "roles" in AWS
      // Writing to s3 is a permission associated with some roles.  We can
      // provide that permission if it isn't already present

  }
}
package com.revature.s3example

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    //Let's write a spark application to read from S3, do some processing, and write to S3
    // This is meant to run on EMR, so it will *not* work locally.
    // The s3 paths that we use require deps we're not including
    //If we wanted to use S3 files locally, we'd use s3a + configure access to s3 from our machine

    val spark = SparkSession.builder()
      .appName("S3 example")
      .getOrCreate()

    import spark.implicits._

    //create a dataframe from file, but the file path is on S3
    spark.read.json("s3://bigdata-pj2-teamadam/data/tweets.json")
      .select($"data.text")
      .write
      .text("s3://bigdata-pj2-teamadam/data/tweets-content.txt")
      //we may need to fiddle with permissions to make this work ^
      // We can give the machines in our EMR cluster "roles" in AWS
      // Writing to s3 is a permission associated with some roles.  We can
      // provide that permission if it isn't already present

  }
}
package com.revature.s3example

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    //Let's write a spark application to read from S3, do some processing, and write to S3
    // This is meant to run on EMR, so it will *not* work locally.
    // The s3 paths that we use require deps we're not including
    //If we wanted to use S3 files locally, we'd use s3a + configure access to s3 from our machine

    val spark = SparkSession.builder()
      .appName("S3 example")
      .getOrCreate()

    import spark.implicits._

    //create a dataframe from file, but the file path is on S3
    spark.read.json("s3://bigdata-pj2-teamadam/data/tweets.json")
      .select($"data.text")
      .write
      .text("s3://bigdata-pj2-teamadam/data/tweets-content.txt")
      //we may need to fiddle with permissions to make this work ^
      // We can give the machines in our EMR cluster "roles" in AWS
      // Writing to s3 is a permission associated with some roles.  We can
      // provide that permission if it isn't already present

  }
}
Mar 03, 2021 10:07:44 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8615
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

/*
 * Scala (https://www.scala-lang.org)
 *
 * Copyright EPFL and Lightbend, Inc.
 *
 * Licensed under Apache License 2.0
 * (http://www.apache.org/licenses/LICENSE-2.0).
 *
 * See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.
 */

package scala

import scala.language.implicitConversions

import scala.collection.{ mutable, immutable, generic }
import immutable.StringOps
import mutable.ArrayOps
import generic.CanBuildFrom
import scala.annotation.{ elidable, implicitNotFound }
import scala.annotation.elidable.ASSERTION
import scala.io.StdIn

/** The `Predef` object provides definitions that are accessible in all Scala
 *  compilation units without explicit qualification.
 *
 *  === Commonly Used Types ===
 *  Predef provides type aliases for types which are commonly used, such as
 *  the immutable collection types [[scala.collection.immutable.Map]],
 *  [[scala.collection.immutable.Set]], and the [[scala.collection.immutable.List]]
 *  constructors ([[scala.collection.immutable.::]] and
 *  [[scala.collection.immutable.Nil]]).
 *
 *  === Console Output ===
 *  For basic console output, `Predef` provides convenience methods [[print(x:Any* print]] and [[println(x:Any* println]],
 *  which are aliases of the methods in the object [[scala.Console]].
 *
 *  === Assertions ===
 *  A set of `assert` functions are provided for use as a way to document
 *  and dynamically check invariants in code. Invocations of `assert` can be elided
 *  at compile time by providing the command line option `-Xdisable-assertions`,
 *  which raises `-Xelide-below` above `elidable.ASSERTION`, to the `scalac` command.
 *
 *  Variants of `assert` intended for use with static analysis tools are also
 *  provided: `assume`, `require` and `ensuring`. `require` and `ensuring` are
 *  intended for use as a means of design-by-contract style specification
 *  of pre- and post-conditions on functions, with the intention that these
 *  specifications could be consumed by a static analysis tool. For instance,
 *
 *  {{{
 *  def addNaturals(nats: List[Int]): Int = {
 *    require(nats forall (_ >= 0), "List contains negative numbers")
 *    nats.foldLeft(0)(_ + _)
 *  } ensuring(_ >= 0)
 *  }}}
 *
 *  The declaration of `addNaturals` states that the list of integers passed should
 *  only contain natural numbers (i.e. non-negative), and that the result returned
 *  will also be natural. `require` is distinct from `assert` in that if the
 *  condition fails, then the caller of the function is to blame rather than a
 *  logical error having been made within `addNaturals` itself. `ensuring` is a
 *  form of `assert` that declares the guarantee the function is providing with
 *  regards to its return value.
 *
 *  === Implicit Conversions ===
 *  A number of commonly applied implicit conversions are also defined here, and
 *  in the parent type [[scala.LowPriorityImplicits]]. Implicit conversions
 *  are provided for the "widening" of numeric values, for instance, converting a
 *  Short value to a Long value as required, and to add additional higher-order
 *  functions to Array values. These are described in more detail in the documentation of [[scala.Array]].
 *
 * @groupname utilities Utility Methods
 * @groupprio utilities 10
 *
 * @groupname assertions Assertions
 * @groupprio assertions 20
 * @groupdesc assertions These methods support program verification and runtime correctness.
 *
 * @groupname console-output Console Output
 * @groupprio console-output 30
 * @groupdesc console-output These methods provide output via the console.
 *
 * @groupname type-constraints Type Constraints
 * @groupprio type-constraints 40
 * @groupdesc type-constraints These entities allows constraints between types to be stipulated.
 *
 * @groupname aliases Aliases
 * @groupprio aliases 50
 * @groupdesc aliases These aliases bring selected immutable types into scope without any imports.
 *
 * @groupname conversions-string String Conversions
 * @groupprio conversions-string 60
 * @groupdesc conversions-string Conversions to and from String and StringOps.
 *
 * @groupname implicit-classes-any Implicit Classes
 * @groupprio implicit-classes-any 70
 * @groupdesc implicit-classes-any These implicit classes add useful extension methods to every type.
 *
 * @groupname implicit-classes-char CharSequence Conversions
 * @groupprio implicit-classes-char 80
 * @groupdesc implicit-classes-char These implicit classes add CharSequence methods to Array[Char] and IndexedSeq[Char] instances.
 *
 * @groupname conversions-java-to-anyval Java to Scala
 * @groupprio conversions-java-to-anyval 90
 * @groupdesc conversions-java-to-anyval Implicit conversion from Java primitive wrapper types to Scala equivalents.
 *
 * @groupname conversions-anyval-to-java Scala to Java
 * @groupprio conversions-anyval-to-java 100
 * @groupdesc conversions-anyval-to-java Implicit conversion from Scala AnyVals to Java primitive wrapper types equivalents.
 *
 * @groupname conversions-array-to-wrapped-array Array to WrappedArray
 * @groupprio conversions-array-to-wrapped-array 110
 * @groupdesc conversions-array-to-wrapped-array Conversions from Arrays to WrappedArrays.
 */
object Predef extends LowPriorityImplicits with DeprecatedPredef {
  /**
   * Retrieve the runtime representation of a class type. `classOf[T]` is equivalent to
   * the class literal `T.class` in Java.
   *
   * @example {{{
   * val listClass = classOf[List[_]]
   * // listClass is java.lang.Class[List[_]] = class scala.collection.immutable.List
   *
   * val mapIntString = classOf[Map[Int,String]]
   * // mapIntString is java.lang.Class[Map[Int,String]] = interface scala.collection.immutable.Map
   * }}}
   * @group utilities
   */
  def classOf[T]: Class[T] = null // This is a stub method. The actual implementation is filled in by the compiler.

  /** The `String` type in Scala has methods that come either from the underlying
   *  Java String (see the documentation corresponding to your Java version, for
   *  example [[http://docs.oracle.com/javase/8/docs/api/java/lang/String.html]]) or
   *  are added implicitly through [[scala.collection.immutable.StringOps]].
   *  @group aliases
   */
  type String        = java.lang.String
  /**  @group aliases */
  type Class[T]      = java.lang.Class[T]

  // miscellaneous -----------------------------------------------------
  scala.`package`                         // to force scala package object to be seen.
  scala.collection.immutable.List         // to force Nil, :: to be seen.

  /**  @group aliases */
  type Function[-A, +B] = Function1[A, B]

  /**  @group aliases */
  type Map[A, +B] = immutable.Map[A, B]
  /**  @group aliases */
  type Set[A]     = immutable.Set[A]
  /**  @group aliases */
  val Map         = immutable.Map
  /**  @group aliases */
  val Set         = immutable.Set

  // Manifest types, companions, and incantations for summoning
  @annotation.implicitNotFound(msg = "No ClassManifest available for ${T}.")
  @deprecated("use `scala.reflect.ClassTag` instead", "2.10.0")
  type ClassManifest[T] = scala.reflect.ClassManifest[T]
  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("this notion doesn't have a corresponding concept in 2.10, because scala.reflect.runtime.universe.TypeTag can capture arbitrary types. Use type tags instead of manifests, and there will be no need in opt manifests.", "2.10.0")
  type OptManifest[T]   = scala.reflect.OptManifest[T]
  @annotation.implicitNotFound(msg = "No Manifest available for ${T}.")
  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("use `scala.reflect.ClassTag` (to capture erasures) or scala.reflect.runtime.universe.TypeTag (to capture types) or both instead", "2.10.0")
  type Manifest[T]      = scala.reflect.Manifest[T]
  @deprecated("use `scala.reflect.ClassTag` instead", "2.10.0")
  val ClassManifest     = scala.reflect.ClassManifest
  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("use `scala.reflect.ClassTag` (to capture erasures) or scala.reflect.runtime.universe.TypeTag (to capture types) or both instead", "2.10.0")
  val Manifest          = scala.reflect.Manifest
  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("this notion doesn't have a corresponding concept in 2.10, because scala.reflect.runtime.universe.TypeTag can capture arbitrary types. Use type tags instead of manifests, and there will be no need in opt manifests.", "2.10.0")
  val NoManifest        = scala.reflect.NoManifest

  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("use scala.reflect.classTag[T] and scala.reflect.runtime.universe.typeTag[T] instead", "2.10.0")
  def manifest[T](implicit m: Manifest[T])           = m
  @deprecated("use scala.reflect.classTag[T] instead", "2.10.0")
  def classManifest[T](implicit m: ClassManifest[T]) = m
  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("this notion doesn't have a corresponding concept in 2.10, because scala.reflect.runtime.universe.TypeTag can capture arbitrary types. Use type tags instead of manifests, and there will be no need in opt manifests.", "2.10.0")
  def optManifest[T](implicit m: OptManifest[T])     = m

  // Minor variations on identity functions
  /** @group utilities */
  @inline def identity[A](x: A): A         = x    // @see `conforms` for the implicit version
  /** @group utilities */
  @inline def implicitly[T](implicit e: T) = e    // for summoning implicit values from the nether world -- TODO: when dependent method types are on by default, give this result type `e.type`, so that inliner has better chance of knowing which method to inline in calls like `implicitly[MatchingStrategy[Option]].zero`
  /** @group utilities */
  @inline def locally[T](x: T): T  = x    // to communicate intent and avoid unmoored statements

  // assertions ---------------------------------------------------------

  /** Tests an expression, throwing an `AssertionError` if false.
   *  Calls to this method will not be generated if `-Xelide-below`
   *  is greater than `ASSERTION`.
   *
   *  @see [[scala.annotation.elidable elidable]]
   *  @param assertion   the expression to test
   *  @group assertions
   */
  @elidable(ASSERTION)
  def assert(assertion: Boolean) {
    if (!assertion)
      throw new java.lang.AssertionError("assertion failed")
  }

  /** Tests an expression, throwing an `AssertionError` if false.
   *  Calls to this method will not be generated if `-Xelide-below`
   *  is greater than `ASSERTION`.
   *
   *  @see [[scala.annotation.elidable elidable]]
   *  @param assertion   the expression to test
   *  @param message     a String to include in the failure message
   *  @group assertions
   */
  @elidable(ASSERTION) @inline
  final def assert(assertion: Boolean, message: => Any) {
    if (!assertion)
      throw new java.lang.AssertionError("assertion failed: "+ message)
  }

  /** Tests an expression, throwing an `AssertionError` if false.
   *  This method differs from assert only in the intent expressed:
   *  assert contains a predicate which needs to be proven, while
   *  assume contains an axiom for a static checker.  Calls to this method
   *  will not be generated if `-Xelide-below` is greater than `ASSERTION`.
   *
   *  @see [[scala.annotation.elidable elidable]]
   *  @param assumption   the expression to test
   *  @group assertions
   */
  @elidable(ASSERTION)
  def assume(assumption: Boolean) {
    if (!assumption)
      throw new java.lang.AssertionError("assumption failed")
  }

  /** Tests an expression, throwing an `AssertionError` if false.
   *  This method differs from assert only in the intent expressed:
   *  assert contains a predicate which needs to be proven, while
   *  assume contains an axiom for a static checker.  Calls to this method
   *  will not be generated if `-Xelide-below` is greater than `ASSERTION`.
   *
   *  @see [[scala.annotation.elidable elidable]]
   *  @param assumption   the expression to test
   *  @param message      a String to include in the failure message
   *  @group assertions
   */
  @elidable(ASSERTION) @inline
  final def assume(assumption: Boolean, message: => Any) {
    if (!assumption)
      throw new java.lang.AssertionError("assumption failed: "+ message)
  }

  /** Tests an expression, throwing an `IllegalArgumentException` if false.
   *  This method is similar to `assert`, but blames the caller of the method
   *  for violating the condition.
   *
   *  @param requirement   the expression to test
   *  @group assertions
   */
  def require(requirement: Boolean) {
    if (!requirement)
      throw new IllegalArgumentException("requirement failed")
  }

  /** Tests an expression, throwing an `IllegalArgumentException` if false.
   *  This method is similar to `assert`, but blames the caller of the method
   *  for violating the condition.
   *
   *  @param requirement   the expression to test
   *  @param message       a String to include in the failure message
   *  @group assertions
   */
  @inline final def require(requirement: Boolean, message: => Any) {
    if (!requirement)
      throw new IllegalArgumentException("requirement failed: "+ message)
  }

  /** `???` can be used for marking methods that remain to be implemented.
   *  @throws NotImplementedError
   *  @group utilities
   */
  def ??? : Nothing = throw new NotImplementedError

  // tupling ------------------------------------------------------------

  @deprecated("use built-in tuple syntax or Tuple2 instead", "2.11.0")
  type Pair[+A, +B] = Tuple2[A, B]
  @deprecated("use built-in tuple syntax or Tuple2 instead", "2.11.0")
  object Pair {
    def apply[A, B](x: A, y: B) = Tuple2(x, y)
    def unapply[A, B](x: Tuple2[A, B]): Option[Tuple2[A, B]] = Some(x)
  }

  @deprecated("use built-in tuple syntax or Tuple3 instead", "2.11.0")
  type Triple[+A, +B, +C] = Tuple3[A, B, C]
  @deprecated("use built-in tuple syntax or Tuple3 instead", "2.11.0")
  object Triple {
    def apply[A, B, C](x: A, y: B, z: C) = Tuple3(x, y, z)
    def unapply[A, B, C](x: Tuple3[A, B, C]): Option[Tuple3[A, B, C]] = Some(x)
  }

  // implicit classes -----------------------------------------------------

  /** @group implicit-classes-any */
  implicit final class ArrowAssoc[A](private val self: A) extends AnyVal {
    @inline def -> [B](y: B): Tuple2[A, B] = Tuple2(self, y)
    def →[B](y: B): Tuple2[A, B] = ->(y)
  }

  /** @group implicit-classes-any */
  implicit final class Ensuring[A](private val self: A) extends AnyVal {
    def ensuring(cond: Boolean): A = { assert(cond); self }
    def ensuring(cond: Boolean, msg: => Any): A = { assert(cond, msg); self }
    def ensuring(cond: A => Boolean): A = { assert(cond(self)); self }
    def ensuring(cond: A => Boolean, msg: => Any): A = { assert(cond(self), msg); self }
  }

  /** @group implicit-classes-any */
  implicit final class StringFormat[A](private val self: A) extends AnyVal {
    /** Returns string formatted according to given `format` string.
     *  Format strings are as for `String.format`
     *  (@see java.lang.String.format).
     */
    @inline def formatted(fmtstr: String): String = fmtstr format self
  }

  // scala/bug#8229 retaining the pre 2.11 name for source compatibility in shadowing this implicit
  /** @group implicit-classes-any */
  implicit final class any2stringadd[A](private val self: A) extends AnyVal {
    def +(other: String): String = String.valueOf(self) + other
  }

  implicit final class RichException(private val self: Throwable) extends AnyVal {
    import scala.compat.Platform.EOL
    @deprecated("use Throwable#getStackTrace", "2.11.0") def getStackTraceString = self.getStackTrace().mkString("", EOL, EOL)
  }

  // Sadly we have to do `@deprecatedName(null, "2.12.0")` because
  // `@deprecatedName(since="2.12.0")` incurs a warning about
  //   Usage of named or default arguments transformed this annotation constructor call into a block.
  //   The corresponding AnnotationInfo will contain references to local values and default getters
  //   instead of the actual argument trees
  // and `@deprecatedName(Symbol("<none>"), "2.12.0")` crashes scalac with
  //   scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving object Symbol
  // in run/repl-no-imports-no-predef-power.scala.
  /** @group implicit-classes-char */
  implicit final class SeqCharSequence(@deprecated("will be made private", "2.12.0") @deprecatedName(null, "2.12.0") val __sequenceOfChars: scala.collection.IndexedSeq[Char]) extends CharSequence {
    def length: Int                                     = __sequenceOfChars.length
    def charAt(index: Int): Char                        = __sequenceOfChars(index)
    def subSequence(start: Int, end: Int): CharSequence = new SeqCharSequence(__sequenceOfChars.slice(start, end))
    override def toString                               = __sequenceOfChars mkString ""
  }

  /** @group implicit-classes-char */
  implicit final class ArrayCharSequence(@deprecated("will be made private", "2.12.0") @deprecatedName(null, "2.12.0") val __arrayOfChars: Array[Char]) extends CharSequence {
    def length: Int                                     = __arrayOfChars.length
    def charAt(index: Int): Char                        = __arrayOfChars(index)
    def subSequence(start: Int, end: Int): CharSequence = new runtime.ArrayCharSequence(__arrayOfChars, start, end)
    override def toString                               = __arrayOfChars mkString ""
  }

  implicit val StringCanBuildFrom: CanBuildFrom[String, Char, String] = new CanBuildFrom[String, Char, String] {
    def apply(from: String) = apply()
    def apply()             = mutable.StringBuilder.newBuilder
  }

  /** @group conversions-string */
  @inline implicit def augmentString(x: String): StringOps = new StringOps(x)
  /** @group conversions-string */
  @inline implicit def unaugmentString(x: StringOps): String = x.repr

  // printing -----------------------------------------------------------

  /** Prints an object to `out` using its `toString` method.
   *
   *  @param x the object to print; may be null.
   *  @group console-output
   */
  def print(x: Any) = Console.print(x)

  /** Prints a newline character on the default output.
   *  @group console-output
   */
  def println() = Console.println()

  /** Prints out an object to the default output, followed by a newline character.
   *
   *  @param x the object to print.
   *  @group console-output
   */
  def println(x: Any) = Console.println(x)

  /** Prints its arguments as a formatted string to the default output,
   *  based on a string pattern (in a fashion similar to printf in C).
   *
   *  The interpretation of the formatting patterns is described in
   *  [[java.util.Formatter]].
   *
   *  Consider using the [[scala.StringContext.f f interpolator]] as more type safe and idiomatic.
   *
   *  @param text the pattern for formatting the arguments.
   *  @param args the arguments used to instantiating the pattern.
   *  @throws java.lang.IllegalArgumentException if there was a problem with the format string or arguments
   *
   *  @see [[scala.StringContext.f StringContext.f]]
   *  @group console-output
   */
  def printf(text: String, xs: Any*) = Console.print(text.format(xs: _*))

  // views --------------------------------------------------------------

  implicit def tuple2ToZippedOps[T1, T2](x: (T1, T2))                           = new runtime.Tuple2Zipped.Ops(x)
  implicit def tuple3ToZippedOps[T1, T2, T3](x: (T1, T2, T3))                   = new runtime.Tuple3Zipped.Ops(x)

  implicit def genericArrayOps[T](xs: Array[T]): ArrayOps[T] = (xs match {
    case x: Array[AnyRef]  => refArrayOps[AnyRef](x)
    case x: Array[Boolean] => booleanArrayOps(x)
    case x: Array[Byte]    => byteArrayOps(x)
    case x: Array[Char]    => charArrayOps(x)
    case x: Array[Double]  => doubleArrayOps(x)
    case x: Array[Float]   => floatArrayOps(x)
    case x: Array[Int]     => intArrayOps(x)
    case x: Array[Long]    => longArrayOps(x)
    case x: Array[Short]   => shortArrayOps(x)
    case x: Array[Unit]    => unitArrayOps(x)
    case null              => null
  }).asInstanceOf[ArrayOps[T]]

  implicit def booleanArrayOps(xs: Array[Boolean]): ArrayOps.ofBoolean   = new ArrayOps.ofBoolean(xs)
  implicit def byteArrayOps(xs: Array[Byte]): ArrayOps.ofByte            = new ArrayOps.ofByte(xs)
  implicit def charArrayOps(xs: Array[Char]): ArrayOps.ofChar            = new ArrayOps.ofChar(xs)
  implicit def doubleArrayOps(xs: Array[Double]): ArrayOps.ofDouble      = new ArrayOps.ofDouble(xs)
  implicit def floatArrayOps(xs: Array[Float]): ArrayOps.ofFloat         = new ArrayOps.ofFloat(xs)
  implicit def intArrayOps(xs: Array[Int]): ArrayOps.ofInt               = new ArrayOps.ofInt(xs)
  implicit def longArrayOps(xs: Array[Long]): ArrayOps.ofLong            = new ArrayOps.ofLong(xs)
  implicit def refArrayOps[T <: AnyRef](xs: Array[T]): ArrayOps.ofRef[T] = new ArrayOps.ofRef[T](xs)
  implicit def shortArrayOps(xs: Array[Short]): ArrayOps.ofShort         = new ArrayOps.ofShort(xs)
  implicit def unitArrayOps(xs: Array[Unit]): ArrayOps.ofUnit            = new ArrayOps.ofUnit(xs)

  // "Autoboxing" and "Autounboxing" ---------------------------------------------------

  /** @group conversions-anyval-to-java */
  implicit def byte2Byte(x: Byte): java.lang.Byte             = x.asInstanceOf[java.lang.Byte]
  /** @group conversions-anyval-to-java */
  implicit def short2Short(x: Short): java.lang.Short         = x.asInstanceOf[java.lang.Short]
  /** @group conversions-anyval-to-java */
  implicit def char2Character(x: Char): java.lang.Character   = x.asInstanceOf[java.lang.Character]
  /** @group conversions-anyval-to-java */
  implicit def int2Integer(x: Int): java.lang.Integer         = x.asInstanceOf[java.lang.Integer]
  /** @group conversions-anyval-to-java */
  implicit def long2Long(x: Long): java.lang.Long             = x.asInstanceOf[java.lang.Long]
  /** @group conversions-anyval-to-java */
  implicit def float2Float(x: Float): java.lang.Float         = x.asInstanceOf[java.lang.Float]
  /** @group conversions-anyval-to-java */
  implicit def double2Double(x: Double): java.lang.Double     = x.asInstanceOf[java.lang.Double]
  /** @group conversions-anyval-to-java */
  implicit def boolean2Boolean(x: Boolean): java.lang.Boolean = x.asInstanceOf[java.lang.Boolean]

  /** @group conversions-java-to-anyval */
  implicit def Byte2byte(x: java.lang.Byte): Byte             = x.asInstanceOf[Byte]
  /** @group conversions-java-to-anyval */
  implicit def Short2short(x: java.lang.Short): Short         = x.asInstanceOf[Short]
  /** @group conversions-java-to-anyval */
  implicit def Character2char(x: java.lang.Character): Char   = x.asInstanceOf[Char]
  /** @group conversions-java-to-anyval */
  implicit def Integer2int(x: java.lang.Integer): Int         = x.asInstanceOf[Int]
  /** @group conversions-java-to-anyval */
  implicit def Long2long(x: java.lang.Long): Long             = x.asInstanceOf[Long]
  /** @group conversions-java-to-anyval */
  implicit def Float2float(x: java.lang.Float): Float         = x.asInstanceOf[Float]
  /** @group conversions-java-to-anyval */
  implicit def Double2double(x: java.lang.Double): Double     = x.asInstanceOf[Double]
  /** @group conversions-java-to-anyval */
  implicit def Boolean2boolean(x: java.lang.Boolean): Boolean = x.asInstanceOf[Boolean]

  // Type Constraints --------------------------------------------------------------

  /**
   * An instance of `A <:< B` witnesses that `A` is a subtype of `B`.
   * Requiring an implicit argument of the type `A <:< B` encodes
   * the generalized constraint `A <: B`.
   *
   * @note we need a new type constructor `<:<` and evidence `conforms`,
   * as reusing `Function1` and `identity` leads to ambiguities in
   * case of type errors (`any2stringadd` is inferred)
   *
   * To constrain any abstract type T that's in scope in a method's
   * argument list (not just the method's own type parameters) simply
   * add an implicit argument of type `T <:< U`, where `U` is the required
   * upper bound; or for lower-bounds, use: `L <:< T`, where `L` is the
   * required lower bound.
   *
   * In part contributed by Jason Zaugg.
   * @group type-constraints
   */
  @implicitNotFound(msg = "Cannot prove that ${From} <:< ${To}.")
  sealed abstract class <:<[-From, +To] extends (From => To) with Serializable
  private[this] final val singleton_<:< = new <:<[Any,Any] { def apply(x: Any): Any = x }
  // The dollar prefix is to dodge accidental shadowing of this method
  // by a user-defined method of the same name (scala/bug#7788).
  // The collections rely on this method.
  /** @group type-constraints */
  implicit def $conforms[A]: A <:< A = singleton_<:<.asInstanceOf[A <:< A]

  @deprecated("use `implicitly[T <:< U]` or `identity` instead.", "2.11.0")
  def conforms[A]: A <:< A = $conforms[A]

  /** An instance of `A =:= B` witnesses that the types `A` and `B` are equal.
   *
   * @see `<:<` for expressing subtyping constraints
   * @group type-constraints
   */
  @implicitNotFound(msg = "Cannot prove that ${From} =:= ${To}.")
  sealed abstract class =:=[From, To] extends (From => To) with Serializable
  private[this] final val singleton_=:= = new =:=[Any,Any] { def apply(x: Any): Any = x }
  /** @group type-constraints */
  object =:= {
     implicit def tpEquals[A]: A =:= A = singleton_=:=.asInstanceOf[A =:= A]
  }

  /** A type for which there is always an implicit value.
   *  @see [[scala.Array$]], method `fallbackCanBuildFrom`
   */
  class DummyImplicit

  object DummyImplicit {

    /** An implicit value yielding a `DummyImplicit`.
     *   @see [[scala.Array$]], method `fallbackCanBuildFrom`
     */
    implicit def dummyImplicit: DummyImplicit = new DummyImplicit
  }
}

private[scala] trait DeprecatedPredef {
  self: Predef.type =>

  // Deprecated stubs for any who may have been calling these methods directly.
  @deprecated("use `ArrowAssoc`", "2.11.0") def any2ArrowAssoc[A](x: A): ArrowAssoc[A]                                      = new ArrowAssoc(x)
  @deprecated("use `Ensuring`", "2.11.0") def any2Ensuring[A](x: A): Ensuring[A]                                            = new Ensuring(x)
  @deprecated("use `StringFormat`", "2.11.0") def any2stringfmt(x: Any): StringFormat[Any]                                  = new StringFormat(x)
  @deprecated("use `Throwable` directly", "2.11.0") def exceptionWrapper(exc: Throwable)                                    = new RichException(exc)
  @deprecated("use `SeqCharSequence`", "2.11.0") def seqToCharSequence(xs: scala.collection.IndexedSeq[Char]): CharSequence = new SeqCharSequence(xs)
  @deprecated("use `ArrayCharSequence`", "2.11.0") def arrayToCharSequence(xs: Array[Char]): CharSequence                   = new ArrayCharSequence(xs)

  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readLine(): String                 = StdIn.readLine()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readLine(text: String, args: Any*) = StdIn.readLine(text, args: _*)
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readBoolean()                      = StdIn.readBoolean()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readByte()                         = StdIn.readByte()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readShort()                        = StdIn.readShort()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readChar()                         = StdIn.readChar()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readInt()                          = StdIn.readInt()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readLong()                         = StdIn.readLong()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readFloat()                        = StdIn.readFloat()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readDouble()                       = StdIn.readDouble()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readf(format: String)              = StdIn.readf(format)
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readf1(format: String)             = StdIn.readf1(format)
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readf2(format: String)             = StdIn.readf2(format)
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readf3(format: String)             = StdIn.readf3(format)
}

/** The `LowPriorityImplicits` class provides implicit values that
*  are valid in all Scala compilation units without explicit qualification,
*  but that are partially overridden by higher-priority conversions in object
*  `Predef`.
*
*  @author  Martin Odersky
*  @since 2.8
*/
// scala/bug#7335 Parents of Predef are defined in the same compilation unit to avoid
// cyclic reference errors compiling the standard library *without* a previously
// compiled copy on the classpath.
private[scala] abstract class LowPriorityImplicits {
  import mutable.WrappedArray
  import immutable.WrappedString

  /** We prefer the java.lang.* boxed types to these wrappers in
   *  any potential conflicts.  Conflicts do exist because the wrappers
   *  need to implement ScalaNumber in order to have a symmetric equals
   *  method, but that implies implementing java.lang.Number as well.
   *
   *  Note - these are inlined because they are value classes, but
   *  the call to xxxWrapper is not eliminated even though it does nothing.
   *  Even inlined, every call site does a no-op retrieval of Predef's MODULE$
   *  because maybe loading Predef has side effects!
   */
  @inline implicit def byteWrapper(x: Byte)       = new runtime.RichByte(x)
  @inline implicit def shortWrapper(x: Short)     = new runtime.RichShort(x)
  @inline implicit def intWrapper(x: Int)         = new runtime.RichInt(x)
  @inline implicit def charWrapper(c: Char)       = new runtime.RichChar(c)
  @inline implicit def longWrapper(x: Long)       = new runtime.RichLong(x)
  @inline implicit def floatWrapper(x: Float)     = new runtime.RichFloat(x)
  @inline implicit def doubleWrapper(x: Double)   = new runtime.RichDouble(x)
  @inline implicit def booleanWrapper(x: Boolean) = new runtime.RichBoolean(x)

  /** @group conversions-array-to-wrapped-array */
  implicit def genericWrapArray[T](xs: Array[T]): WrappedArray[T] =
    if (xs eq null) null
    else WrappedArray.make(xs)

  // Since the JVM thinks arrays are covariant, one 0-length Array[AnyRef]
  // is as good as another for all T <: AnyRef.  Instead of creating 100,000,000
  // unique ones by way of this implicit, let's share one.
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapRefArray[T <: AnyRef](xs: Array[T]): WrappedArray[T] = {
    if (xs eq null) null
    else if (xs.length == 0) WrappedArray.empty[T]
    else new WrappedArray.ofRef[T](xs)
  }

  /** @group conversions-array-to-wrapped-array */
  implicit def wrapIntArray(xs: Array[Int]): WrappedArray[Int] = if (xs ne null) new WrappedArray.ofInt(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapDoubleArray(xs: Array[Double]): WrappedArray[Double] = if (xs ne null) new WrappedArray.ofDouble(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapLongArray(xs: Array[Long]): WrappedArray[Long] = if (xs ne null) new WrappedArray.ofLong(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapFloatArray(xs: Array[Float]): WrappedArray[Float] = if (xs ne null) new WrappedArray.ofFloat(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapCharArray(xs: Array[Char]): WrappedArray[Char] = if (xs ne null) new WrappedArray.ofChar(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapByteArray(xs: Array[Byte]): WrappedArray[Byte] = if (xs ne null) new WrappedArray.ofByte(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapShortArray(xs: Array[Short]): WrappedArray[Short] = if (xs ne null) new WrappedArray.ofShort(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapBooleanArray(xs: Array[Boolean]): WrappedArray[Boolean] = if (xs ne null) new WrappedArray.ofBoolean(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapUnitArray(xs: Array[Unit]): WrappedArray[Unit] = if (xs ne null) new WrappedArray.ofUnit(xs) else null

  /** @group conversions-string */
  implicit def wrapString(s: String): WrappedString = if (s ne null) new WrappedString(s) else null
  /** @group conversions-string */
  implicit def unwrapString(ws: WrappedString): String = if (ws ne null) ws.self else null

  implicit def fallbackStringCanBuildFrom[T]: CanBuildFrom[String, T, immutable.IndexedSeq[T]] =
    new CanBuildFrom[String, T, immutable.IndexedSeq[T]] {
      def apply(from: String) = immutable.IndexedSeq.newBuilder[T]
      def apply() = immutable.IndexedSeq.newBuilder[T]
    }
}

[0m2021.03.03 10:09:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:09:45 INFO  time: compiled root in 0.69s[0m
Mar 03, 2021 10:12:04 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8826
[0m2021.03.03 10:12:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:12:15 INFO  time: compiled root in 0.69s[0m
[0m2021.03.03 10:12:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:12:23 INFO  time: compiled root in 0.62s[0m
[0m2021.03.03 10:13:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:13:18 INFO  time: compiled root in 0.69s[0m
[0m2021.03.03 10:21:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:21:06 INFO  time: compiled root in 0.84s[0m
[0m2021.03.03 10:23:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:23:11 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 10:23:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:23:29 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 10:25:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:25:18 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 10:25:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:25:33 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 10:25:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:25:43 INFO  time: compiled root in 0.65s[0m
Mar 03, 2021 10:26:05 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9145
[0m2021.03.03 10:27:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:27:01 INFO  time: compiled root in 0.14s[0m
Mar 03, 2021 10:27:18 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9210
[0m2021.03.03 10:28:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:28:36 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 10:29:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:29:40 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 10:29:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:29:44 INFO  time: compiled root in 0.66s[0m
[0m2021.03.03 10:29:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:29:47 INFO  time: compiled root in 0.62s[0m
Mar 03, 2021 10:31:43 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9367
Mar 03, 2021 10:33:23 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9420
Mar 03, 2021 10:35:38 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9487
Mar 03, 2021 10:36:07 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9509
[0m2021.03.03 10:36:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:36:43 INFO  time: compiled root in 0.87s[0m
[0m2021.03.03 10:37:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:37:33 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 10:39:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:39:42 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 10:39:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:39:53 INFO  time: compiled root in 0.64s[0m
[0m2021.03.03 10:39:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:39:58 INFO  time: compiled root in 0.69s[0m
[0m2021.03.03 10:43:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:43:20 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 10:43:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:43:49 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 10:43:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:43:52 INFO  time: compiled root in 0.69s[0m
[0m2021.03.03 10:44:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:44:50 INFO  time: compiled root in 0.61s[0m
[0m2021.03.03 10:45:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:45:31 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 10:45:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:45:48 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 10:45:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:45:53 INFO  time: compiled root in 0.62s[0m
[0m2021.03.03 10:46:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:46:59 INFO  time: compiled root in 0.14s[0m
Mar 03, 2021 10:47:31 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9976
[0m2021.03.03 10:48:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:49:15: stale bloop error: not found: value s3bucket
    for (e <- s3bucket) {
              ^^^^^^^^[0m
[0m2021.03.03 10:48:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:49:15: stale bloop error: not found: value s3bucket
    for (e <- s3bucket) {
              ^^^^^^^^[0m
[0m2021.03.03 10:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:49:15: stale bloop error: not found: value s3bucket
    for (e <- s3bucket) {
              ^^^^^^^^[0m
[0m2021.03.03 10:48:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:49:15: stale bloop error: not found: value s3bucket
    for (e <- s3bucket) {
              ^^^^^^^^[0m
[0m2021.03.03 10:48:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:48:51 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 10:49:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:49:40 INFO  time: compiled root in 0.71s[0m
[0m2021.03.03 10:50:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:50:13 INFO  time: compiled root in 0.65s[0m
[0m2021.03.03 10:50:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:50:29 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 10:50:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:50:45 INFO  time: compiled root in 0.18s[0m
[0m2021.03.03 10:50:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:50:49 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 10:50:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:50:55 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 10:51:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:51:06 INFO  time: compiled root in 0.7s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.util.{Locale, Properties}

import scala.collection.JavaConverters._

import com.fasterxml.jackson.databind.ObjectMapper
import com.univocity.parsers.csv.CsvParser

import org.apache.spark.Partition
import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.json.{CreateJacksonParser, JacksonParser, JSONOptions}
import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap
import org.apache.spark.sql.execution.command.DDLUtils
import org.apache.spark.sql.execution.datasources.{DataSource, FailureSafeParser}
import org.apache.spark.sql.execution.datasources.csv._
import org.apache.spark.sql.execution.datasources.jdbc._
import org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource
import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation
import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils
import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport}
import org.apache.spark.sql.types.{StringType, StructType}
import org.apache.spark.unsafe.types.UTF8String

/**
 * Interface used to load a [[Dataset]] from external storage systems (e.g. file systems,
 * key-value stores, etc). Use `SparkSession.read` to access this.
 *
 * @since 1.4.0
 */
@InterfaceStability.Stable
class DataFrameReader private[sql](sparkSession: SparkSession) extends Logging {

  /**
   * Specifies the input data source format.
   *
   * @since 1.4.0
   */
  def format(source: String): DataFrameReader = {
    this.source = source
    this
  }

  /**
   * Specifies the input schema. Some data sources (e.g. JSON) can infer the input schema
   * automatically from data. By specifying the schema here, the underlying data source can
   * skip the schema inference step, and thus speed up data loading.
   *
   * @since 1.4.0
   */
  def schema(schema: StructType): DataFrameReader = {
    this.userSpecifiedSchema = Option(schema)
    this
  }

  /**
   * Specifies the schema by using the input DDL-formatted string. Some data sources (e.g. JSON) can
   * infer the input schema automatically from data. By specifying the schema here, the underlying
   * data source can skip the schema inference step, and thus speed up data loading.
   *
   * {{{
   *   spark.read.schema("a INT, b STRING, c DOUBLE").csv("test.csv")
   * }}}
   *
   * @since 2.3.0
   */
  def schema(schemaString: String): DataFrameReader = {
    this.userSpecifiedSchema = Option(StructType.fromDDL(schemaString))
    this
  }

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def option(key: String, value: String): DataFrameReader = {
    this.extraOptions += (key -> value)
    this
  }

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Boolean): DataFrameReader = option(key, value.toString)

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Long): DataFrameReader = option(key, value.toString)

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Double): DataFrameReader = option(key, value.toString)

  /**
   * (Scala-specific) Adds input options for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def options(options: scala.collection.Map[String, String]): DataFrameReader = {
    this.extraOptions ++= options
    this
  }

  /**
   * Adds input options for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def options(options: java.util.Map[String, String]): DataFrameReader = {
    this.options(options.asScala)
    this
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that don't require a path (e.g. external
   * key-value stores).
   *
   * @since 1.4.0
   */
  def load(): DataFrame = {
    load(Seq.empty: _*) // force invocation of `load(...varargs...)`
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that require a path (e.g. data backed by
   * a local or distributed file system).
   *
   * @since 1.4.0
   */
  def load(path: String): DataFrame = {
    // force invocation of `load(...varargs...)`
    option(DataSourceOptions.PATH_KEY, path).load(Seq.empty: _*)
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that support multiple paths.
   * Only works if the source is a HadoopFsRelationProvider.
   *
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def load(paths: String*): DataFrame = {
    if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) {
      throw new AnalysisException("Hive data source can only be used with tables, you can not " +
        "read files of Hive data source directly.")
    }

    val cls = DataSource.lookupDataSource(source, sparkSession.sessionState.conf)
    if (classOf[DataSourceV2].isAssignableFrom(cls)) {
      val ds = cls.newInstance().asInstanceOf[DataSourceV2]
      if (ds.isInstanceOf[ReadSupport]) {
        val sessionOptions = DataSourceV2Utils.extractSessionConfigs(
          ds = ds, conf = sparkSession.sessionState.conf)
        val pathsOption = {
          val objectMapper = new ObjectMapper()
          DataSourceOptions.PATHS_KEY -> objectMapper.writeValueAsString(paths.toArray)
        }
        Dataset.ofRows(sparkSession, DataSourceV2Relation.create(
          ds, sessionOptions ++ extraOptions.toMap + pathsOption,
          userSpecifiedSchema = userSpecifiedSchema))
      } else {
        loadV1Source(paths: _*)
      }
    } else {
      loadV1Source(paths: _*)
    }
  }

  private def loadV1Source(paths: String*) = {
    // Code path for data source v1.
    sparkSession.baseRelationToDataFrame(
      DataSource.apply(
        sparkSession,
        paths = paths,
        userSpecifiedSchema = userSpecifiedSchema,
        className = source,
        options = extraOptions.toMap).resolveRelation())
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table and connection properties.
   *
   * @since 1.4.0
   */
  def jdbc(url: String, table: String, properties: Properties): DataFrame = {
    assertNoSpecifiedSchema("jdbc")
    // properties should override settings in extraOptions.
    this.extraOptions ++= properties.asScala
    // explicit url and dbtable should override all
    this.extraOptions ++= Seq(JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table)
    format("jdbc").load()
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table. Partitions of the table will be retrieved in parallel based on the parameters
   * passed to this function.
   *
   * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
   * your external database systems.
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`.
   * @param table Name of the table in the external database.
   * @param columnName the name of a column of numeric, date, or timestamp type
   *                   that will be used for partitioning.
   * @param lowerBound the minimum value of `columnName` used to decide partition stride.
   * @param upperBound the maximum value of `columnName` used to decide partition stride.
   * @param numPartitions the number of partitions. This, along with `lowerBound` (inclusive),
   *                      `upperBound` (exclusive), form partition strides for generated WHERE
   *                      clause expressions used to split the column `columnName` evenly. When
   *                      the input is less than 1, the number is set to 1.
   * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
   *                             tag/value. Normally at least a "user" and "password" property
   *                             should be included. "fetchsize" can be used to control the
   *                             number of rows per fetch and "queryTimeout" can be used to wait
   *                             for a Statement object to execute to the given number of seconds.
   * @since 1.4.0
   */
  def jdbc(
      url: String,
      table: String,
      columnName: String,
      lowerBound: Long,
      upperBound: Long,
      numPartitions: Int,
      connectionProperties: Properties): DataFrame = {
    // columnName, lowerBound, upperBound and numPartitions override settings in extraOptions.
    this.extraOptions ++= Map(
      JDBCOptions.JDBC_PARTITION_COLUMN -> columnName,
      JDBCOptions.JDBC_LOWER_BOUND -> lowerBound.toString,
      JDBCOptions.JDBC_UPPER_BOUND -> upperBound.toString,
      JDBCOptions.JDBC_NUM_PARTITIONS -> numPartitions.toString)
    jdbc(url, table, connectionProperties)
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table using connection properties. The `predicates` parameter gives a list
   * expressions suitable for inclusion in WHERE clauses; each one defines one partition
   * of the `DataFrame`.
   *
   * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
   * your external database systems.
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`
   * @param table Name of the table in the external database.
   * @param predicates Condition in the where clause for each partition.
   * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
   *                             tag/value. Normally at least a "user" and "password" property
   *                             should be included. "fetchsize" can be used to control the
   *                             number of rows per fetch.
   * @since 1.4.0
   */
  def jdbc(
      url: String,
      table: String,
      predicates: Array[String],
      connectionProperties: Properties): DataFrame = {
    assertNoSpecifiedSchema("jdbc")
    // connectionProperties should override settings in extraOptions.
    val params = extraOptions ++ connectionProperties.asScala
    val options = new JDBCOptions(url, table, params)
    val parts: Array[Partition] = predicates.zipWithIndex.map { case (part, i) =>
      JDBCPartition(part, i) : Partition
    }
    val relation = JDBCRelation(parts, options)(sparkSession)
    sparkSession.baseRelationToDataFrame(relation)
  }

  /**
   * Loads a JSON file and returns the results as a `DataFrame`.
   *
   * See the documentation on the overloaded `json()` method with varargs for more details.
   *
   * @since 1.4.0
   */
  def json(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    json(Seq(path): _*)
  }

  /**
   * Loads JSON files and returns the results as a `DataFrame`.
   *
   * <a href="http://jsonlines.org/">JSON Lines</a> (newline-delimited JSON) is supported by
   * default. For JSON (one record per file), set the `multiLine` option to true.
   *
   * This function goes through the input once to determine the input schema. If you know the
   * schema in advance, use the version that specifies the schema to avoid the extra scan.
   *
   * You can set the following JSON-specific options to deal with non-standard JSON files:
   * <ul>
   * <li>`primitivesAsString` (default `false`): infers all primitive values as a string type</li>
   * <li>`prefersDecimal` (default `false`): infers all floating-point values as a decimal
   * type. If the values do not fit in decimal, then it infers them as doubles.</li>
   * <li>`allowComments` (default `false`): ignores Java/C++ style comment in JSON records</li>
   * <li>`allowUnquotedFieldNames` (default `false`): allows unquoted JSON field names</li>
   * <li>`allowSingleQuotes` (default `true`): allows single quotes in addition to double quotes
   * </li>
   * <li>`allowNumericLeadingZeros` (default `false`): allows leading zeros in numbers
   * (e.g. 00012)</li>
   * <li>`allowBackslashEscapingAnyCharacter` (default `false`): allows accepting quoting of all
   * character using backslash quoting mechanism</li>
   * <li>`allowUnquotedControlChars` (default `false`): allows JSON Strings to contain unquoted
   * control characters (ASCII characters with value less than 32, including tab and line feed
   * characters) or not.</li>
   * <li>`mode` (default `PERMISSIVE`): allows a mode for dealing with corrupt records
   * during parsing.
   *   <ul>
   *     <li>`PERMISSIVE` : when it meets a corrupted record, puts the malformed string into a
   *     field configured by `columnNameOfCorruptRecord`, and sets other fields to `null`. To
   *     keep corrupt records, an user can set a string type field named
   *     `columnNameOfCorruptRecord` in an user-defined schema. If a schema does not have the
   *     field, it drops corrupt records during parsing. When inferring a schema, it implicitly
   *     adds a `columnNameOfCorruptRecord` field in an output schema.</li>
   *     <li>`DROPMALFORMED` : ignores the whole corrupted records.</li>
   *     <li>`FAILFAST` : throws an exception when it meets corrupted records.</li>
   *   </ul>
   * </li>
   * <li>`columnNameOfCorruptRecord` (default is the value specified in
   * `spark.sql.columnNameOfCorruptRecord`): allows renaming the new field having malformed string
   * created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.</li>
   * <li>`dateFormat` (default `yyyy-MM-dd`): sets the string that indicates a date format.
   * Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to
   * date type.</li>
   * <li>`timestampFormat` (default `yyyy-MM-dd'T'HH:mm:ss.SSSXXX`): sets the string that
   * indicates a timestamp format. Custom date formats follow the formats at
   * `java.text.SimpleDateFormat`. This applies to timestamp type.</li>
   * <li>`multiLine` (default `false`): parse one record, which may span multiple lines,
   * per file</li>
   * <li>`encoding` (by default it is not set): allows to forcibly set one of standard basic
   * or extended encoding for the JSON files. For example UTF-16BE, UTF-32LE. If the encoding
   * is not specified and `multiLine` is set to `true`, it will be detected automatically.</li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * <li>`samplingRatio` (default is 1.0): defines fraction of input JSON objects used
   * for schema inferring.</li>
   * <li>`dropFieldIfAllNull` (default `false`): whether to ignore column of all null values or
   * empty array/struct during schema inference.</li>
   * </ul>
   *
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def json(paths: String*): DataFrame = format("json").load(paths : _*)

  /**
   * Loads a `JavaRDD[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON
   * Lines text format or newline-delimited JSON</a>) and returns the result as
   * a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonRDD input RDD with one JSON object per record
   * @since 1.4.0
   */
  @deprecated("Use json(Dataset[String]) instead.", "2.2.0")
  def json(jsonRDD: JavaRDD[String]): DataFrame = json(jsonRDD.rdd)

  /**
   * Loads an `RDD[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON Lines
   * text format or newline-delimited JSON</a>) and returns the result as a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonRDD input RDD with one JSON object per record
   * @since 1.4.0
   */
  @deprecated("Use json(Dataset[String]) instead.", "2.2.0")
  def json(jsonRDD: RDD[String]): DataFrame = {
    json(sparkSession.createDataset(jsonRDD)(Encoders.STRING))
  }

  /**
   * Loads a `Dataset[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON Lines
   * text format or newline-delimited JSON</a>) and returns the result as a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonDataset input Dataset with one JSON object per record
   * @since 2.2.0
   */
  def json(jsonDataset: Dataset[String]): DataFrame = {
    val parsedOptions = new JSONOptions(
      extraOptions.toMap,
      sparkSession.sessionState.conf.sessionLocalTimeZone,
      sparkSession.sessionState.conf.columnNameOfCorruptRecord)

    val schema = userSpecifiedSchema.getOrElse {
      TextInputJsonDataSource.inferFromDataset(jsonDataset, parsedOptions)
    }

    verifyColumnNameOfCorruptRecord(schema, parsedOptions.columnNameOfCorruptRecord)
    val actualSchema =
      StructType(schema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord))

    val createParser = CreateJacksonParser.string _
    val parsed = jsonDataset.rdd.mapPartitions { iter =>
      val rawParser = new JacksonParser(actualSchema, parsedOptions)
      val parser = new FailureSafeParser[String](
        input => rawParser.parse(input, createParser, UTF8String.fromString),
        parsedOptions.parseMode,
        schema,
        parsedOptions.columnNameOfCorruptRecord)
      iter.flatMap(parser.parse)
    }
    sparkSession.internalCreateDataFrame(parsed, schema, isStreaming = jsonDataset.isStreaming)
  }

  /**
   * Loads a CSV file and returns the result as a `DataFrame`. See the documentation on the
   * other overloaded `csv()` method for more details.
   *
   * @since 2.0.0
   */
  def csv(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    csv(Seq(path): _*)
  }

  /**
   * Loads an `Dataset[String]` storing CSV rows and returns the result as a `DataFrame`.
   *
   * If the schema is not specified using `schema` function and `inferSchema` option is enabled,
   * this function goes through the input once to determine the input schema.
   *
   * If the schema is not specified using `schema` function and `inferSchema` option is disabled,
   * it determines the columns as string types and it reads only the first line to determine the
   * names and the number of fields.
   *
   * If the enforceSchema is set to `false`, only the CSV header in the first line is checked
   * to conform specified or inferred schema.
   *
   * @param csvDataset input Dataset with one CSV row per record
   * @since 2.2.0
   */
  def csv(csvDataset: Dataset[String]): DataFrame = {
    val parsedOptions: CSVOptions = new CSVOptions(
      extraOptions.toMap,
      sparkSession.sessionState.conf.csvColumnPruning,
      sparkSession.sessionState.conf.sessionLocalTimeZone)
    val filteredLines: Dataset[String] =
      CSVUtils.filterCommentAndEmpty(csvDataset, parsedOptions)
    val maybeFirstLine: Option[String] = filteredLines.take(1).headOption

    val schema = userSpecifiedSchema.getOrElse {
      TextInputCSVDataSource.inferFromDataset(
        sparkSession,
        csvDataset,
        maybeFirstLine,
        parsedOptions)
    }

    verifyColumnNameOfCorruptRecord(schema, parsedOptions.columnNameOfCorruptRecord)
    val actualSchema =
      StructType(schema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord))

    val linesWithoutHeader = if (parsedOptions.headerFlag && maybeFirstLine.isDefined) {
      val firstLine = maybeFirstLine.get
      val parser = new CsvParser(parsedOptions.asParserSettings)
      val columnNames = parser.parseLine(firstLine)
      CSVDataSource.checkHeaderColumnNames(
        actualSchema,
        columnNames,
        csvDataset.getClass.getCanonicalName,
        parsedOptions.enforceSchema,
        sparkSession.sessionState.conf.caseSensitiveAnalysis)
      filteredLines.rdd.mapPartitions(CSVUtils.filterHeaderLine(_, firstLine, parsedOptions))
    } else {
      filteredLines.rdd
    }

    val parsed = linesWithoutHeader.mapPartitions { iter =>
      val rawParser = new UnivocityParser(actualSchema, parsedOptions)
      val parser = new FailureSafeParser[String](
        input => Seq(rawParser.parse(input)),
        parsedOptions.parseMode,
        schema,
        parsedOptions.columnNameOfCorruptRecord)
      iter.flatMap(parser.parse)
    }
    sparkSession.internalCreateDataFrame(parsed, schema, isStreaming = csvDataset.isStreaming)
  }

  /**
   * Loads CSV files and returns the result as a `DataFrame`.
   *
   * This function will go through the input once to determine the input schema if `inferSchema`
   * is enabled. To avoid going through the entire data once, disable `inferSchema` option or
   * specify the schema explicitly using `schema`.
   *
   * You can set the following CSV-specific options to deal with CSV files:
   * <ul>
   * <li>`sep` (default `,`): sets a single character as a separator for each
   * field and value.</li>
   * <li>`encoding` (default `UTF-8`): decodes the CSV files by the given encoding
   * type.</li>
   * <li>`quote` (default `"`): sets a single character used for escaping quoted values where
   * the separator can be part of the value. If you would like to turn off quotations, you need to
   * set not `null` but an empty string. This behaviour is different from
   * `com.databricks.spark.csv`.</li>
   * <li>`escape` (default `\`): sets a single character used for escaping quotes inside
   * an already quoted value.</li>
   * <li>`charToEscapeQuoteEscaping` (default `escape` or `\0`): sets a single character used for
   * escaping the escape for the quote character. The default value is escape character when escape
   * and quote characters are different, `\0` otherwise.</li>
   * <li>`comment` (default empty string): sets a single character used for skipping lines
   * beginning with this character. By default, it is disabled.</li>
   * <li>`header` (default `false`): uses the first line as names of columns.</li>
   * <li>`enforceSchema` (default `true`): If it is set to `true`, the specified or inferred schema
   * will be forcibly applied to datasource files, and headers in CSV files will be ignored.
   * If the option is set to `false`, the schema will be validated against all headers in CSV files
   * in the case when the `header` option is set to `true`. Field names in the schema
   * and column names in CSV headers are checked by their positions taking into account
   * `spark.sql.caseSensitive`. Though the default value is true, it is recommended to disable
   * the `enforceSchema` option to avoid incorrect results.</li>
   * <li>`inferSchema` (default `false`): infers the input schema automatically from data. It
   * requires one extra pass over the data.</li>
   * <li>`samplingRatio` (default is 1.0): defines fraction of rows used for schema inferring.</li>
   * <li>`ignoreLeadingWhiteSpace` (default `false`): a flag indicating whether or not leading
   * whitespaces from values being read should be skipped.</li>
   * <li>`ignoreTrailingWhiteSpace` (default `false`): a flag indicating whether or not trailing
   * whitespaces from values being read should be skipped.</li>
   * <li>`nullValue` (default empty string): sets the string representation of a null value. Since
   * 2.0.1, this applies to all supported types including the string type.</li>
   * <li>`emptyValue` (default empty string): sets the string representation of an empty value.</li>
   * <li>`nanValue` (default `NaN`): sets the string representation of a non-number" value.</li>
   * <li>`positiveInf` (default `Inf`): sets the string representation of a positive infinity
   * value.</li>
   * <li>`negativeInf` (default `-Inf`): sets the string representation of a negative infinity
   * value.</li>
   * <li>`dateFormat` (default `yyyy-MM-dd`): sets the string that indicates a date format.
   * Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to
   * date type.</li>
   * <li>`timestampFormat` (default `yyyy-MM-dd'T'HH:mm:ss.SSSXXX`): sets the string that
   * indicates a timestamp format. Custom date formats follow the formats at
   * `java.text.SimpleDateFormat`. This applies to timestamp type.</li>
   * <li>`maxColumns` (default `20480`): defines a hard limit of how many columns
   * a record can have.</li>
   * <li>`maxCharsPerColumn` (default `-1`): defines the maximum number of characters allowed
   * for any given value being read. By default, it is -1 meaning unlimited length</li>
   * <li>`mode` (default `PERMISSIVE`): allows a mode for dealing with corrupt records
   *    during parsing. It supports the following case-insensitive modes. Note that Spark tries
   *    to parse only required columns in CSV under column pruning. Therefore, corrupt records
   *    can be different based on required set of fields. This behavior can be controlled by
   *    `spark.sql.csv.parser.columnPruning.enabled` (enabled by default).
   *   <ul>
   *     <li>`PERMISSIVE` : when it meets a corrupted record, puts the malformed string into a
   *     field configured by `columnNameOfCorruptRecord`, and sets other fields to `null`. To keep
   *     corrupt records, an user can set a string type field named `columnNameOfCorruptRecord`
   *     in an user-defined schema. If a schema does not have the field, it drops corrupt records
   *     during parsing. A record with less/more tokens than schema is not a corrupted record to
   *     CSV. When it meets a record having fewer tokens than the length of the schema, sets
   *     `null` to extra fields. When the record has more tokens than the length of the schema,
   *     it drops extra tokens.</li>
   *     <li>`DROPMALFORMED` : ignores the whole corrupted records.</li>
   *     <li>`FAILFAST` : throws an exception when it meets corrupted records.</li>
   *   </ul>
   * </li>
   * <li>`columnNameOfCorruptRecord` (default is the value specified in
   * `spark.sql.columnNameOfCorruptRecord`): allows renaming the new field having malformed string
   * created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.</li>
   * <li>`multiLine` (default `false`): parse one record, which may span multiple lines.</li>
   * </ul>
   *
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def csv(paths: String*): DataFrame = format("csv").load(paths : _*)

  /**
   * Loads a Parquet file, returning the result as a `DataFrame`. See the documentation
   * on the other overloaded `parquet()` method for more details.
   *
   * @since 2.0.0
   */
  def parquet(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    parquet(Seq(path): _*)
  }

  /**
   * Loads a Parquet file, returning the result as a `DataFrame`.
   *
   * You can set the following Parquet-specific option(s) for reading Parquet files:
   * <ul>
   * <li>`mergeSchema` (default is the value specified in `spark.sql.parquet.mergeSchema`): sets
   * whether we should merge schemas collected from all Parquet part-files. This will override
   * `spark.sql.parquet.mergeSchema`.</li>
   * </ul>
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def parquet(paths: String*): DataFrame = {
    format("parquet").load(paths: _*)
  }

  /**
   * Loads an ORC file and returns the result as a `DataFrame`.
   *
   * @param path input path
   * @since 1.5.0
   */
  def orc(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    orc(Seq(path): _*)
  }

  /**
   * Loads ORC files and returns the result as a `DataFrame`.
   *
   * @param paths input paths
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orc(paths: String*): DataFrame = format("orc").load(paths: _*)

  /**
   * Returns the specified table as a `DataFrame`.
   *
   * @since 1.4.0
   */
  def table(tableName: String): DataFrame = {
    assertNoSpecifiedSchema("table")
    sparkSession.table(tableName)
  }

  /**
   * Loads text files and returns a `DataFrame` whose schema starts with a string column named
   * "value", and followed by partitioned columns if there are any. See the documentation on
   * the other overloaded `text()` method for more details.
   *
   * @since 2.0.0
   */
  def text(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    text(Seq(path): _*)
  }

  /**
   * Loads text files and returns a `DataFrame` whose schema starts with a string column named
   * "value", and followed by partitioned columns if there are any.
   *
   * By default, each line in the text files is a new row in the resulting DataFrame. For example:
   * {{{
   *   // Scala:
   *   spark.read.text("/path/to/spark/README.md")
   *
   *   // Java:
   *   spark.read().text("/path/to/spark/README.md")
   * }}}
   *
   * You can set the following text-specific option(s) for reading text files:
   * <ul>
   * <li>`wholetext` (default `false`): If true, read a file as a single row and not split by "\n".
   * </li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * </ul>
   *
   * @param paths input paths
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def text(paths: String*): DataFrame = format("text").load(paths : _*)

  /**
   * Loads text files and returns a [[Dataset]] of String. See the documentation on the
   * other overloaded `textFile()` method for more details.
   * @since 2.0.0
   */
  def textFile(path: String): Dataset[String] = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    textFile(Seq(path): _*)
  }

  /**
   * Loads text files and returns a [[Dataset]] of String. The underlying schema of the Dataset
   * contains a single string column named "value".
   *
   * If the directory structure of the text files contains partitioning information, those are
   * ignored in the resulting Dataset. To include partitioning information as columns, use `text`.
   *
   * By default, each line in the text files is a new row in the resulting DataFrame. For example:
   * {{{
   *   // Scala:
   *   spark.read.textFile("/path/to/spark/README.md")
   *
   *   // Java:
   *   spark.read().textFile("/path/to/spark/README.md")
   * }}}
   *
   * You can set the following textFile-specific option(s) for reading text files:
   * <ul>
   * <li>`wholetext` (default `false`): If true, read a file as a single row and not split by "\n".
   * </li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * </ul>
   *
   * @param paths input path
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def textFile(paths: String*): Dataset[String] = {
    assertNoSpecifiedSchema("textFile")
    text(paths : _*).select("value").as[String](sparkSession.implicits.newStringEncoder)
  }

  /**
   * A convenient function for schema validation in APIs.
   */
  private def assertNoSpecifiedSchema(operation: String): Unit = {
    if (userSpecifiedSchema.nonEmpty) {
      throw new AnalysisException(s"User specified schema not supported with `$operation`")
    }
  }

  /**
   * A convenient function for schema validation in datasources supporting
   * `columnNameOfCorruptRecord` as an option.
   */
  private def verifyColumnNameOfCorruptRecord(
      schema: StructType,
      columnNameOfCorruptRecord: String): Unit = {
    schema.getFieldIndex(columnNameOfCorruptRecord).foreach { corruptFieldIndex =>
      val f = schema(corruptFieldIndex)
      if (f.dataType != StringType || !f.nullable) {
        throw new AnalysisException(
          "The field for corrupt records must be string type and nullable")
      }
    }
  }

  ///////////////////////////////////////////////////////////////////////////////////////
  // Builder pattern config options
  ///////////////////////////////////////////////////////////////////////////////////////

  private var source: String = sparkSession.sessionState.conf.defaultDataSourceName

  private var userSpecifiedSchema: Option[StructType] = None

  private var extraOptions = CaseInsensitiveMap[String](Map.empty)

}

[0m2021.03.03 10:52:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:52:04 INFO  time: compiled root in 0.66s[0m
[0m2021.03.03 10:57:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:57:48 INFO  time: compiled root in 0.79s[0m
[0m2021.03.03 10:58:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:58:12 INFO  time: compiled root in 0.74s[0m
[0m2021.03.03 10:59:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:59:43 INFO  time: compiled root in 1.14s[0m
[0m2021.03.03 11:00:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:00:26 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 11:00:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:00:41 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 11:00:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:00:46 INFO  time: compiled root in 0.74s[0m
[0m2021.03.03 11:02:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:02:39 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 11:03:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:03:24 INFO  time: compiled root in 0.71s[0m
[0m2021.03.03 11:05:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:05:23 INFO  time: compiled root in 0.73s[0m
[0m2021.03.03 11:06:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:06:03 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 11:08:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:08:00 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 11:08:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:08:35 INFO  time: compiled root in 0.7s[0m
[0m2021.03.03 11:08:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:08:57 INFO  time: compiled root in 0.76s[0m
[0m2021.03.03 11:09:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:09:33 INFO  time: compiled root in 0.68s[0m
[0m2021.03.03 11:11:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:11:47 INFO  time: compiled root in 0.7s[0m
[0m2021.03.03 11:14:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:14:24 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 11:14:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:14:47 INFO  time: compiled root in 0.62s[0m
[0m2021.03.03 11:16:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:16:27 INFO  time: compiled root in 0.8s[0m
[0m2021.03.03 11:23:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:23:05 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 11:23:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:23:11 INFO  time: compiled root in 0.63s[0m
[0m2021.03.03 11:25:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:25:21 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 11:25:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:25:42 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 11:26:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:26:30 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 11:26:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:26:33 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 11:26:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:26:39 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 11:27:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:27:17 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 11:27:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:27:30 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 11:28:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:28:14 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 11:28:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:28:28 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 11:29:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:29:01 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 11:29:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:29:45 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 11:30:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:30:43 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 11:31:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:31:06 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 11:31:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:31:08 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 11:31:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:31:25 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 11:31:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:31:30 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 11:32:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:32:05 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 11:33:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:33:33 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 11:34:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:34:50 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 11:35:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:35:44 INFO  time: compiled root in 0.62s[0m
[0m2021.03.03 11:36:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:36:34 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 11:36:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:36:46 INFO  time: compiled root in 0.63s[0m
Mar 03, 2021 11:37:55 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12461
[0m2021.03.03 11:37:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:37:59 INFO  time: compiled root in 0.7s[0m
[0m2021.03.03 11:38:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:38:38 INFO  time: compiled root in 0.77s[0m
[0m2021.03.03 11:38:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:38:57 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 11:39:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:39:04 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 11:39:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:39:26 INFO  time: compiled root in 0.18s[0m
[0m2021.03.03 11:39:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:39:38 INFO  time: compiled root in 0.75s[0m
Mar 03, 2021 11:42:32 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12747
Mar 03, 2021 11:43:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12753
[0m2021.03.03 11:44:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:44:19 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 11:44:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:44:57 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 11:45:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:45:02 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 11:45:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:45:33 INFO  time: compiled root in 0.89s[0m
[0m2021.03.03 11:46:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:46:25 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 11:50:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:50:22 INFO  time: compiled root in 0.78s[0m
[0m2021.03.03 11:52:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:52:47 INFO  time: compiled root in 0.83s[0m
[0m2021.03.03 11:53:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:53:35 INFO  time: compiled root in 0.89s[0m
[0m2021.03.03 11:54:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:54:46 INFO  time: compiled root in 0.9s[0m
[0m2021.03.03 11:56:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:56:01 INFO  time: compiled root in 0.87s[0m
[0m2021.03.03 11:57:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:57:23 INFO  time: compiled root in 0.84s[0m
[0m2021.03.03 11:59:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:59:09 INFO  time: compiled root in 0.81s[0m
[0m2021.03.03 12:01:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:01:06 INFO  time: compiled root in 0.77s[0m
[0m2021.03.03 12:01:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:01:20 INFO  time: compiled root in 0.89s[0m
[0m2021.03.03 12:08:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:08:54 INFO  time: compiled root in 0.87s[0m
[0m2021.03.03 12:19:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:19:45 INFO  time: compiled root in 0.94s[0m
[0m2021.03.03 12:30:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:30:08 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 12:31:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:31:50 INFO  time: compiled root in 0.18s[0m
[0m2021.03.03 12:32:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:71:14: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .flatMap(split("</*>"))
             ^^^^^^^^^^^^^[0m
[0m2021.03.03 12:32:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:71:14: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .flatMap(split("</*>"))
             ^^^^^^^^^^^^^[0m
[0m2021.03.03 12:32:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:71:14: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .flatMap(split("</*>"))
             ^^^^^^^^^^^^^[0m
[0m2021.03.03 12:32:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:71:14: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .flatMap(split("</*>"))
             ^^^^^^^^^^^^^[0m
[0m2021.03.03 12:32:05 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:71:14: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .flatMap(split("</*>"))
             ^^^^^^^^^^^^^[0m
[0m2021.03.03 12:32:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:71:14: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .flatMap(split("</*>"))
             ^^^^^^^^^^^^^[0m
[0m2021.03.03 12:32:07 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:71:14: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .flatMap(split("</*>"))
             ^^^^^^^^^^^^^[0m
[0m2021.03.03 12:32:07 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:71:14: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .flatMap(split("</*>"))
             ^^^^^^^^^^^^^[0m
[0m2021.03.03 12:32:07 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:71:14: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .flatMap(split("</*>"))
             ^^^^^^^^^^^^^[0m
[0m2021.03.03 12:32:07 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:71:14: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
    .flatMap(split("</*>"))
             ^^^^^^^^^^^^^[0m
[0m2021.03.03 12:32:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:32:11 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 12:32:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:32:14 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 12:32:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:32:28 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 12:32:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:32:36 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 12:32:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:32:43 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 12:34:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:34:38 INFO  time: compiled root in 0.81s[0m
[0m2021.03.03 12:35:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:35:26 INFO  time: compiled root in 0.87s[0m
[0m2021.03.03 12:38:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:38:24 INFO  time: compiled root in 0.98s[0m
[0m2021.03.03 12:38:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:38:55 INFO  time: compiled root in 0.85s[0m
[0m2021.03.03 12:41:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:41:36 INFO  time: compiled root in 1.19s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.Map
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder

/**
 * A collection of implicit methods for converting common Scala objects into [[Dataset]]s.
 *
 * @since 1.6.0
 */
@InterfaceStability.Evolving
abstract class SQLImplicits extends LowPrioritySQLImplicits {

  protected def _sqlContext: SQLContext

  /**
   * Converts $"col name" into a [[Column]].
   *
   * @since 2.0.0
   */
  implicit class StringToColumn(val sc: StringContext) {
    def $(args: Any*): ColumnName = {
      new ColumnName(sc.s(args: _*))
    }
  }

  // Primitives

  /** @since 1.6.0 */
  implicit def newIntEncoder: Encoder[Int] = Encoders.scalaInt

  /** @since 1.6.0 */
  implicit def newLongEncoder: Encoder[Long] = Encoders.scalaLong

  /** @since 1.6.0 */
  implicit def newDoubleEncoder: Encoder[Double] = Encoders.scalaDouble

  /** @since 1.6.0 */
  implicit def newFloatEncoder: Encoder[Float] = Encoders.scalaFloat

  /** @since 1.6.0 */
  implicit def newByteEncoder: Encoder[Byte] = Encoders.scalaByte

  /** @since 1.6.0 */
  implicit def newShortEncoder: Encoder[Short] = Encoders.scalaShort

  /** @since 1.6.0 */
  implicit def newBooleanEncoder: Encoder[Boolean] = Encoders.scalaBoolean

  /** @since 1.6.0 */
  implicit def newStringEncoder: Encoder[String] = Encoders.STRING

  /** @since 2.2.0 */
  implicit def newJavaDecimalEncoder: Encoder[java.math.BigDecimal] = Encoders.DECIMAL

  /** @since 2.2.0 */
  implicit def newScalaDecimalEncoder: Encoder[scala.math.BigDecimal] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newDateEncoder: Encoder[java.sql.Date] = Encoders.DATE

  /** @since 2.2.0 */
  implicit def newTimeStampEncoder: Encoder[java.sql.Timestamp] = Encoders.TIMESTAMP


  // Boxed primitives

  /** @since 2.0.0 */
  implicit def newBoxedIntEncoder: Encoder[java.lang.Integer] = Encoders.INT

  /** @since 2.0.0 */
  implicit def newBoxedLongEncoder: Encoder[java.lang.Long] = Encoders.LONG

  /** @since 2.0.0 */
  implicit def newBoxedDoubleEncoder: Encoder[java.lang.Double] = Encoders.DOUBLE

  /** @since 2.0.0 */
  implicit def newBoxedFloatEncoder: Encoder[java.lang.Float] = Encoders.FLOAT

  /** @since 2.0.0 */
  implicit def newBoxedByteEncoder: Encoder[java.lang.Byte] = Encoders.BYTE

  /** @since 2.0.0 */
  implicit def newBoxedShortEncoder: Encoder[java.lang.Short] = Encoders.SHORT

  /** @since 2.0.0 */
  implicit def newBoxedBooleanEncoder: Encoder[java.lang.Boolean] = Encoders.BOOLEAN

  // Seqs

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newIntSeqEncoder: Encoder[Seq[Int]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newLongSeqEncoder: Encoder[Seq[Long]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newDoubleSeqEncoder: Encoder[Seq[Double]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newFloatSeqEncoder: Encoder[Seq[Float]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newByteSeqEncoder: Encoder[Seq[Byte]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newShortSeqEncoder: Encoder[Seq[Short]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newBooleanSeqEncoder: Encoder[Seq[Boolean]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newStringSeqEncoder: Encoder[Seq[String]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newProductSeqEncoder[A <: Product : TypeTag]: Encoder[Seq[A]] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newSequenceEncoder[T <: Seq[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Maps
  /** @since 2.3.0 */
  implicit def newMapEncoder[T <: Map[_, _] : TypeTag]: Encoder[T] = ExpressionEncoder()

  /**
   * Notice that we serialize `Set` to Catalyst array. The set property is only kept when
   * manipulating the domain objects. The serialization format doesn't keep the set property.
   * When we have a Catalyst array which contains duplicated elements and convert it to
   * `Dataset[Set[T]]` by using the encoder, the elements will be de-duplicated.
   *
   * @since 2.3.0
   */
  implicit def newSetEncoder[T <: Set[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Arrays

  /** @since 1.6.1 */
  implicit def newIntArrayEncoder: Encoder[Array[Int]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newLongArrayEncoder: Encoder[Array[Long]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newDoubleArrayEncoder: Encoder[Array[Double]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newFloatArrayEncoder: Encoder[Array[Float]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newByteArrayEncoder: Encoder[Array[Byte]] = Encoders.BINARY

  /** @since 1.6.1 */
  implicit def newShortArrayEncoder: Encoder[Array[Short]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newBooleanArrayEncoder: Encoder[Array[Boolean]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newStringArrayEncoder: Encoder[Array[String]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newProductArrayEncoder[A <: Product : TypeTag]: Encoder[Array[A]] =
    ExpressionEncoder()

  /**
   * Creates a [[Dataset]] from an RDD.
   *
   * @since 1.6.0
   */
  implicit def rddToDatasetHolder[T : Encoder](rdd: RDD[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(rdd))
  }

  /**
   * Creates a [[Dataset]] from a local Seq.
   * @since 1.6.0
   */
  implicit def localSeqToDatasetHolder[T : Encoder](s: Seq[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(s))
  }

  /**
   * An implicit conversion that turns a Scala `Symbol` into a [[Column]].
   * @since 1.3.0
   */
  implicit def symbolToColumn(s: Symbol): ColumnName = new ColumnName(s.name)

}

/**
 * Lower priority implicit methods for converting Scala objects into [[Dataset]]s.
 * Conflicting implicits are placed here to disambiguate resolution.
 *
 * Reasons for including specific implicits:
 * newProductEncoder - to disambiguate for `List`s which are both `Seq` and `Product`
 */
trait LowPrioritySQLImplicits {
  /** @since 1.6.0 */
  implicit def newProductEncoder[T <: Product : TypeTag]: Encoder[T] = Encoders.product[T]

}

[0m2021.03.03 12:48:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:48:04 INFO  time: compiled root in 0.24s[0m
[0m2021.03.03 12:48:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:48:10 INFO  time: compiled root in 1.21s[0m
[0m2021.03.03 12:49:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:49:26 INFO  time: compiled root in 0.83s[0m
[0m2021.03.03 12:49:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:49:47 INFO  time: compiled root in 0.83s[0m
[0m2021.03.03 12:50:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:50:43 INFO  time: compiled root in 0.78s[0m
[0m2021.03.03 12:51:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:51:34 INFO  time: compiled root in 0.83s[0m
[0m2021.03.03 12:52:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:52:24 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 12:52:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:52:31 INFO  time: compiled root in 0.86s[0m
[0m2021.03.03 12:52:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:52:54 INFO  time: compiled root in 0.26s[0m
[0m2021.03.03 12:53:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:53:34 INFO  time: compiled root in 0.79s[0m
[0m2021.03.03 12:53:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:53:35 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 12:54:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:54:20 INFO  time: compiled root in 0.76s[0m
[0m2021.03.03 14:10:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:10:55 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 14:10:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:10:59 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 14:11:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:11:04 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 14:11:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:11:19 INFO  time: compiled root in 0.7s[0m
[0m2021.03.03 14:11:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:11:58 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 14:12:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:12:27 INFO  time: compiled root in 0.81s[0m
[0m2021.03.03 14:12:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:12:34 INFO  time: compiled root in 0.7s[0m
[0m2021.03.03 14:15:00 WARN  no build target for: /home/amburkee/tmp/210104-usf-bigdata/week7/s3example/src/main/scala/com/revature/s3example/Runner.scala[0m
package com.revature.s3example

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    //Let's write a spark application to read from S3, do some processing, and write to S3
    // This is meant to run on EMR, so it will *not* work locally.
    // The s3 paths that we use require deps we're not including
    //If we wanted to use S3 files locally, we'd use s3a + configure access to s3 from our machine

    val spark = SparkSession.builder()
      .appName("S3 example")
      .getOrCreate()

    import spark.implicits._

    //create a dataframe from file, but the file path is on S3
    spark.read.json("s3://bigdata-pj2-teamadam/data/tweets.json")
      .select($"data.text")
      .write
      .text("s3://bigdata-pj2-teamadam/data/tweets-content.txt")
      //we may need to fiddle with permissions to make this work ^
      // We can give the machines in our EMR cluster "roles" in AWS
      // Writing to s3 is a permission associated with some roles.  We can
      // provide that permission if it isn't already present

  }
}
package com.revature.s3example

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    //Let's write a spark application to read from S3, do some processing, and write to S3
    // This is meant to run on EMR, so it will *not* work locally.
    // The s3 paths that we use require deps we're not including
    //If we wanted to use S3 files locally, we'd use s3a + configure access to s3 from our machine

    val spark = SparkSession.builder()
      .appName("S3 example")
      .getOrCreate()

    import spark.implicits._

    //create a dataframe from file, but the file path is on S3
    spark.read.json("s3://bigdata-pj2-teamadam/data/tweets.json")
      .select($"data.text")
      .write
      .text("s3://bigdata-pj2-teamadam/data/tweets-content.txt")
      //we may need to fiddle with permissions to make this work ^
      // We can give the machines in our EMR cluster "roles" in AWS
      // Writing to s3 is a permission associated with some roles.  We can
      // provide that permission if it isn't already present

  }
}
package com.revature.s3example

import org.apache.spark.sql.SparkSession

object Runner {
  def main(args: Array[String]): Unit = {
    //Let's write a spark application to read from S3, do some processing, and write to S3
    // This is meant to run on EMR, so it will *not* work locally.
    // The s3 paths that we use require deps we're not including
    //If we wanted to use S3 files locally, we'd use s3a + configure access to s3 from our machine

    val spark = SparkSession.builder()
      .appName("S3 example")
      .getOrCreate()

    import spark.implicits._

    //create a dataframe from file, but the file path is on S3
    spark.read.json("s3://bigdata-pj2-teamadam/data/tweets.json")
      .select($"data.text")
      .write
      .text("s3://bigdata-pj2-teamadam/data/tweets-content.txt")
      //we may need to fiddle with permissions to make this work ^
      // We can give the machines in our EMR cluster "roles" in AWS
      // Writing to s3 is a permission associated with some roles.  We can
      // provide that permission if it isn't already present

  }
}
Mar 03, 2021 2:15:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 14864
[0m2021.03.03 14:15:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:15:37 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 14:15:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:15:48 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 14:16:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:16:14 INFO  time: compiled root in 0.73s[0m
[0m2021.03.03 14:16:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:16:31 INFO  time: compiled root in 0.58s[0m
[0m2021.03.03 14:17:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:17:07 INFO  time: compiled root in 0.86s[0m
[0m2021.03.03 14:17:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:17:15 INFO  time: compiled root in 0.77s[0m
Mar 03, 2021 2:25:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 15071
[0m2021.03.03 14:26:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:26:13 INFO  time: compiled root in 1s[0m
[0m2021.03.03 14:29:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:29:42 INFO  time: compiled root in 0.93s[0m
[0m2021.03.03 14:30:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:30:03 INFO  time: compiled root in 0.86s[0m
Mar 03, 2021 2:31:43 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:31:44 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: bad position: [2966:2950]
java.lang.AssertionError: assertion failed: bad position: [2966:2950]
	at scala.reflect.internal.util.Position$.validate(Position.scala:34)
	at scala.reflect.internal.util.Position$.range(Position.scala:51)
	at scala.reflect.internal.util.InternalPositionImpl$class.copyRange(Position.scala:205)
	at scala.reflect.internal.util.InternalPositionImpl$class.withStart(Position.scala:124)
	at scala.reflect.internal.util.Position.withStart(Position.scala:12)
	at scala.meta.internal.pc.CompletionProvider.editRange$lzycompute$1(CompletionProvider.scala:366)
	at scala.meta.internal.pc.CompletionProvider.editRange$2(CompletionProvider.scala:365)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:444)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)

Mar 03, 2021 2:31:44 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:31:44 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: bad position: [2967:2951]
java.lang.AssertionError: assertion failed: bad position: [2967:2951]
	at scala.reflect.internal.util.Position$.validate(Position.scala:34)
	at scala.reflect.internal.util.Position$.range(Position.scala:51)
	at scala.reflect.internal.util.InternalPositionImpl$class.copyRange(Position.scala:205)
	at scala.reflect.internal.util.InternalPositionImpl$class.withStart(Position.scala:124)
	at scala.reflect.internal.util.Position.withStart(Position.scala:12)
	at scala.meta.internal.pc.CompletionProvider.editRange$lzycompute$1(CompletionProvider.scala:366)
	at scala.meta.internal.pc.CompletionProvider.editRange$2(CompletionProvider.scala:365)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:444)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)

Mar 03, 2021 2:31:48 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:31:48 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: bad position: [2970:2954]
java.lang.AssertionError: assertion failed: bad position: [2970:2954]
	at scala.reflect.internal.util.Position$.validate(Position.scala:34)
	at scala.reflect.internal.util.Position$.range(Position.scala:51)
	at scala.reflect.internal.util.InternalPositionImpl$class.copyRange(Position.scala:205)
	at scala.reflect.internal.util.InternalPositionImpl$class.withStart(Position.scala:124)
	at scala.reflect.internal.util.Position.withStart(Position.scala:12)
	at scala.meta.internal.pc.CompletionProvider.editRange$lzycompute$1(CompletionProvider.scala:366)
	at scala.meta.internal.pc.CompletionProvider.editRange$2(CompletionProvider.scala:365)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:444)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)

Mar 03, 2021 2:31:48 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:31:49 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: bad position: [2971:2955]
java.lang.AssertionError: assertion failed: bad position: [2971:2955]
	at scala.reflect.internal.util.Position$.validate(Position.scala:34)
	at scala.reflect.internal.util.Position$.range(Position.scala:51)
	at scala.reflect.internal.util.InternalPositionImpl$class.copyRange(Position.scala:205)
	at scala.reflect.internal.util.InternalPositionImpl$class.withStart(Position.scala:124)
	at scala.reflect.internal.util.Position.withStart(Position.scala:12)
	at scala.meta.internal.pc.CompletionProvider.editRange$lzycompute$1(CompletionProvider.scala:366)
	at scala.meta.internal.pc.CompletionProvider.editRange$2(CompletionProvider.scala:365)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:444)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)

Mar 03, 2021 2:32:13 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:32:13 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:32:14 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:32:14 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: bad position: [2992:2976]
java.lang.AssertionError: assertion failed: bad position: [2992:2976]
	at scala.reflect.internal.util.Position$.validate(Position.scala:34)
	at scala.reflect.internal.util.Position$.range(Position.scala:51)
	at scala.reflect.internal.util.InternalPositionImpl$class.copyRange(Position.scala:205)
	at scala.reflect.internal.util.InternalPositionImpl$class.withStart(Position.scala:124)
	at scala.reflect.internal.util.Position.withStart(Position.scala:12)
	at scala.meta.internal.pc.CompletionProvider.editRange$lzycompute$1(CompletionProvider.scala:366)
	at scala.meta.internal.pc.CompletionProvider.editRange$2(CompletionProvider.scala:365)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:444)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)

Mar 03, 2021 2:32:14 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:32:14 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:32:14 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:32:15 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: bad position: [2996:2980]
java.lang.AssertionError: assertion failed: bad position: [2996:2980]
	at scala.reflect.internal.util.Position$.validate(Position.scala:34)
	at scala.reflect.internal.util.Position$.range(Position.scala:51)
	at scala.reflect.internal.util.InternalPositionImpl$class.copyRange(Position.scala:205)
	at scala.reflect.internal.util.InternalPositionImpl$class.withStart(Position.scala:124)
	at scala.reflect.internal.util.Position.withStart(Position.scala:12)
	at scala.meta.internal.pc.CompletionProvider.editRange$lzycompute$1(CompletionProvider.scala:366)
	at scala.meta.internal.pc.CompletionProvider.editRange$2(CompletionProvider.scala:365)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:444)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)

[0m2021.03.03 14:32:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:32:28 INFO  time: compiled root in 0.86s[0m
Mar 03, 2021 2:33:34 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:33:34 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:33:34 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:33:35 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:33:35 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:33:36 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:33:36 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: bad position: [3025:3009]
java.lang.AssertionError: assertion failed: bad position: [3025:3009]
	at scala.reflect.internal.util.Position$.validate(Position.scala:34)
	at scala.reflect.internal.util.Position$.range(Position.scala:51)
	at scala.reflect.internal.util.InternalPositionImpl$class.copyRange(Position.scala:205)
	at scala.reflect.internal.util.InternalPositionImpl$class.withStart(Position.scala:124)
	at scala.reflect.internal.util.Position.withStart(Position.scala:12)
	at scala.meta.internal.pc.CompletionProvider.editRange$lzycompute$1(CompletionProvider.scala:366)
	at scala.meta.internal.pc.CompletionProvider.editRange$2(CompletionProvider.scala:365)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:444)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)

[0m2021.03.03 14:33:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:33:50 INFO  time: compiled root in 0.88s[0m
[0m2021.03.03 14:34:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:34:40 INFO  time: compiled root in 0.85s[0m
Mar 03, 2021 2:36:46 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:36:46 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: bad position: [3007:2951]
java.lang.AssertionError: assertion failed: bad position: [3007:2951]
	at scala.reflect.internal.util.Position$.validate(Position.scala:34)
	at scala.reflect.internal.util.Position$.range(Position.scala:51)
	at scala.reflect.internal.util.InternalPositionImpl$class.copyRange(Position.scala:205)
	at scala.reflect.internal.util.InternalPositionImpl$class.withStart(Position.scala:124)
	at scala.reflect.internal.util.Position.withStart(Position.scala:12)
	at scala.meta.internal.pc.CompletionProvider.editRange$lzycompute$1(CompletionProvider.scala:366)
	at scala.meta.internal.pc.CompletionProvider.editRange$2(CompletionProvider.scala:365)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:444)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)

Mar 03, 2021 2:36:47 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 03, 2021 2:36:47 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: bad position: [3008:2952]
java.lang.AssertionError: assertion failed: bad position: [3008:2952]
	at scala.reflect.internal.util.Position$.validate(Position.scala:34)
	at scala.reflect.internal.util.Position$.range(Position.scala:51)
	at scala.reflect.internal.util.InternalPositionImpl$class.copyRange(Position.scala:205)
	at scala.reflect.internal.util.InternalPositionImpl$class.withStart(Position.scala:124)
	at scala.reflect.internal.util.Position.withStart(Position.scala:12)
	at scala.meta.internal.pc.CompletionProvider.editRange$lzycompute$1(CompletionProvider.scala:366)
	at scala.meta.internal.pc.CompletionProvider.editRange$2(CompletionProvider.scala:365)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:444)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)

[0m2021.03.03 14:38:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:38:33 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 14:45:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:45:02 INFO  time: compiled root in 0.77s[0m
[0m2021.03.03 14:48:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:48:12 INFO  time: compiled root in 0.77s[0m
[0m2021.03.03 14:48:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:48:22 INFO  time: compiled root in 0.74s[0m
[0m2021.03.03 14:49:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:49:03 INFO  time: compiled root in 0.92s[0m
[0m2021.03.03 14:49:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:49:12 INFO  time: compiled root in 0.78s[0m
[0m2021.03.03 14:52:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:52:56 INFO  time: compiled root in 0.77s[0m
[0m2021.03.03 14:54:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:54:10 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 14:55:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:55:49 INFO  time: compiled root in 0.94s[0m
[0m2021.03.03 14:57:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:57:29 INFO  time: compiled root in 0.96s[0m
[0m2021.03.03 14:59:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:59:27 INFO  time: compiled root in 0.97s[0m
[0m2021.03.03 15:05:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:05:58 INFO  time: compiled root in 0.92s[0m
Mar 03, 2021 3:07:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 16283
[0m2021.03.03 15:07:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:07:19 INFO  time: compiled root in 0.94s[0m
Mar 03, 2021 3:08:31 PM scala.meta.internal.pc.CompletionProvider expected$1
WARNING: String index out of range: -1
[0m2021.03.03 15:08:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:08:44 INFO  time: compiled root in 0.84s[0m
[0m2021.03.03 15:09:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:09:26 INFO  time: compiled root in 1.94s[0m
[0m2021.03.03 15:11:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:11:44 INFO  time: compiled root in 1.78s[0m
[0m2021.03.03 15:20:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:20:48 INFO  time: compiled root in 0.79s[0m
[0m2021.03.03 15:21:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:21:38 INFO  time: compiled root in 0.82s[0m
[0m2021.03.03 15:32:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:32:36 INFO  time: compiled root in 0.8s[0m
[0m2021.03.03 15:34:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:34:04 INFO  time: compiled root in 0.83s[0m
[0m2021.03.03 15:34:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:34:47 INFO  time: compiled root in 0.77s[0m
[0m2021.03.03 15:37:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:37:17 INFO  time: compiled root in 0.91s[0m
Mar 03, 2021 3:38:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 16874
[0m2021.03.03 15:38:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:38:25 INFO  time: compiled root in 0.82s[0m
[0m2021.03.03 15:46:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:46:20 INFO  time: compiled root in 0.81s[0m
[0m2021.03.03 15:47:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:47:28 INFO  time: compiled root in 0.78s[0m
[0m2021.03.03 15:47:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:47:34 INFO  time: compiled root in 0.87s[0m
[0m2021.03.03 15:51:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:51:57 INFO  time: compiled root in 0.82s[0m
[0m2021.03.03 15:52:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:52:28 INFO  time: compiled root in 0.77s[0m
[0m2021.03.03 15:54:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:54:51 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 15:55:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:55:04 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:55:31 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:35: stale bloop error: ')' expected but string literal found.
      .filter(!($"value" rlike (?)".*sex.*" or($"value" rlike (?)".*blowjob.*")))
                                  ^[0m
[0m2021.03.03 15:55:31 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:81: stale bloop error: ';' expected but ')' found.
      .filter(!($"value" rlike (?)".*sex.*" or($"value" rlike (?)".*blowjob.*")))
                                                                                ^[0m
[0m2021.03.03 15:55:31 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:35: stale bloop error: ')' expected but string literal found.
      .filter(!($"value" rlike (?)".*sex.*" or($"value" rlike (?)".*blowjob.*")))
                                  ^[0m
[0m2021.03.03 15:55:31 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:81: stale bloop error: ';' expected but ')' found.
      .filter(!($"value" rlike (?)".*sex.*" or($"value" rlike (?)".*blowjob.*")))
                                                                                ^[0m
[0m2021.03.03 15:55:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:55:35 INFO  time: compiled root in 0.91s[0m
[0m2021.03.03 15:59:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:14 INFO  time: compiled root in 0.76s[0m
[0m2021.03.03 16:05:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:05:54 INFO  time: compiled root in 0.79s[0m
[0m2021.03.03 16:11:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:11:08 INFO  time: compiled root in 0.86s[0m
[0m2021.03.03 16:14:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:14:02 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:66:92: stale bloop error: invalid escape character
      .filter($"value" rlike "<a\\shref=.*career.*>.*</a>" or($"value" rlike "<a\\shref=.*\job.*>.*</a>") or($"value" rlike "<a\\shref=.*employment.*>.*</a>"))
                                                                                           ^[0m
[0m2021.03.03 16:14:02 INFO  time: compiled root in 92ms[0m
[0m2021.03.03 16:14:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:66:92: stale bloop error: invalid escape character
      .filter($"value" rlike "<a\\shref=.*career.*>.*</a>" or($"value" rlike "<a\\shref=.*\job.*>.*</a>") or($"value" rlike "<a\\shref=.*employment.*>.*</a>"))
                                                                                           ^[0m
[0m2021.03.03 16:14:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:66:92: stale bloop error: invalid escape character
      .filter($"value" rlike "<a\\shref=.*career.*>.*</a>" or($"value" rlike "<a\\shref=.*\job.*>.*</a>") or($"value" rlike "<a\\shref=.*employment.*>.*</a>"))
                                                                                           ^[0m
[0m2021.03.03 16:14:04 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:66:92: stale bloop error: invalid escape character
      .filter($"value" rlike "<a\\shref=.*career.*>.*</a>" or($"value" rlike "<a\\shref=.*\job.*>.*</a>") or($"value" rlike "<a\\shref=.*employment.*>.*</a>"))
                                                                                           ^[0m
[0m2021.03.03 16:14:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:14:55 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 16:14:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:14:59 INFO  time: compiled root in 0.81s[0m
[0m2021.03.03 16:15:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:15:25 INFO  time: compiled root in 0.76s[0m
[0m2021.03.03 16:17:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:17:19 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 16:17:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:17:25 INFO  time: compiled root in 0.76s[0m
Mar 03, 2021 4:17:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 17607
[0m2021.03.03 16:17:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:17:39 INFO  time: compiled root in 0.78s[0m
[0m2021.03.03 16:18:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:18:28 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 16:18:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:18:33 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 16:18:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:18:40 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 16:18:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:18:45 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 16:19:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:19:30 INFO  time: compiled root in 0.74s[0m
[0m2021.03.03 16:20:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:20:09 INFO  time: compiled root in 0.78s[0m
[0m2021.03.03 16:20:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:20:16 INFO  time: compiled root in 0.76s[0m
[0m2021.03.03 16:20:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:20:50 INFO  time: compiled root in 0.79s[0m
[0m2021.03.03 16:20:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:20:57 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 16:21:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:21:06 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 16:21:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:21:29 INFO  time: compiled root in 0.18s[0m
[0m2021.03.03 16:21:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:21:32 INFO  time: compiled root in 0.8s[0m
[0m2021.03.03 16:28:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:28:54 INFO  time: compiled root in 0.83s[0m
[0m2021.03.03 16:29:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:29:25 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 16:29:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:29:36 INFO  time: compiled root in 0.8s[0m
[0m2021.03.03 16:30:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:30:06 INFO  time: compiled root in 0.83s[0m
[0m2021.03.03 16:30:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:30:23 INFO  time: compiled root in 0.78s[0m
[0m2021.03.03 16:30:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:30:29 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 16:30:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:30:32 INFO  time: compiled root in 0.83s[0m
[0m2021.03.03 16:31:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:31:08 INFO  time: compiled root in 0.81s[0m
[0m2021.03.03 16:32:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:32:04 INFO  time: compiled root in 0.85s[0m
[0m2021.03.03 16:35:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:35:03 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 16:38:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:38:59 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 16:39:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:39:11 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 16:40:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:40:37 INFO  time: compiled root in 1s[0m
[0m2021.03.03 16:41:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:41:21 INFO  time: compiled root in 1s[0m
[0m2021.03.03 16:41:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:41:24 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 16:45:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:10 INFO  time: compiled root in 0.22s[0m
[0m2021.03.03 16:46:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:46:06 INFO  time: compiled root in 1.41s[0m
[0m2021.03.03 16:46:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:46:59 INFO  time: compiled root in 1.41s[0m
[0m2021.03.03 16:47:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:47:04 INFO  time: compiled root in 0.72s[0m
[0m2021.03.03 16:47:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:47:49 INFO  time: compiled root in 0.85s[0m
[0m2021.03.03 16:51:54 INFO  shutting down Metals[0m
[0m2021.03.03 16:51:54 INFO  Shut down connection with build server.[0m
[0m2021.03.03 16:51:54 INFO  Shut down connection with build server.[0m
[0m2021.03.03 16:51:54 INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.04 08:18:13 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.53.2.[0m
[0m2021.03.04 08:18:13 INFO  time: initialize in 0.39s[0m
[0m2021.03.04 08:18:13 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2031789062571604653/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.04 08:18:14 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
[0m2021.03.04 08:18:14 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.04 08:18:16 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.DataFrame
import java.io.File
import scala.io.Source

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?

//warc file paths
    //"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/warc.paths.gz"
//wet file paths
    //"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz"

    //"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00000.warc.gz"
    //"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00000.warc.wet.gz"
    // val bucket = spark.read.textFile(
    //   "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/warc.paths.gz"
    // ).write.csv("s3")
    // s3bucket.printSchema()
    // s3bucket.show(false)

    val s3bucket = Source.fromFile(
      "s3/part-00000-da9ce71c-e820-412f-b874-1e4f6d091a4d-c000.csv"
    )

    for (line <- s3bucket.getLines()) {
      val col = line.split("\n").map(_.trim())
      val path = "s3a://commoncrawl/".concat(line)
      val ds = spark.read.textFile(path)
      val df = ds.toDF()
      htmlFilter(df, spark)
    }
  }

  def htmlFilter(df: DataFrame, spark: SparkSession) = {
    import spark.implicits._
    val job = df
      .filter(
        $"value" rlike "<a\\shref=.*career.*>.*</a>" or ($"value" rlike "<a\\shref=.*/job.*>.*</a>") or ($"value" rlike "<a\\shref=.*employment.*>.*</a>")
      )
      .filter(!($"value" rlike ".*sex.*" or ($"value" rlike ".*blowjob.*")))
      .select(split($"value", "</.*>"))
      // .flatMap(line => line.toString())
      // .select(trim($"value") as ("HTML Data"))
    job.show(false)

    val techJob = job
      .filter(
        $"value" rlike ".*(Frontend.*job).*" or ($"value" rlike ".*(Backendend.*job).*") or ($"value" rlike ".*(Fullstack.*job).*")
          or ($"value" rlike ".*(Cybersecurity.*job).*") or ($"value" rlike ".*(Software.*job).*") or ($"value" rlike ".*(Computer.*job).*")
      )
      //".*(Frontend.*job).*" || ".*(Backend.*job).*" || ".*(Fullstack.*job).*" ||
      //".*(Cybersecurity.*job).*" || ".*(Software.*job).*" || ".*(Computer.*job).*"

      //   // <.*>.*</.*>
      //   //<p> ........ </p>
      // .show(false)
  }
}

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2031789062571604653/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2031789062571604653/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 08:18:18 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 08:18:18 INFO  time: code lens generation in 3.55s[0m
[0m2021.03.04 08:18:18 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.04 08:18:18 INFO  Attempting to connect to the build server...[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1720491964563529497/bsp.socket'...
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6184089436209409440/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1720491964563529497/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1720491964563529497/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6184089436209409440/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6184089436209409440/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 08:18:18 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 08:18:18 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 08:18:18 INFO  time: Connected to build server in 4.15s[0m
[0m2021.03.04 08:18:18 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.04 08:18:18 INFO  time: Imported build in 0.21s[0m
[0m2021.03.04 08:18:20 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.04 08:18:20 INFO  time: indexed workspace in 1.95s[0m
[0m2021.03.04 10:18:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:18:45 INFO  time: compiled root in 4s[0m
[0m2021.03.04 10:19:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:19:56 INFO  time: compiled root in 1.52s[0m
[0m2021.03.04 10:20:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:20:24 INFO  time: compiled root in 1.48s[0m
[0m2021.03.04 10:24:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:24:27 INFO  time: compiled root in 1.17s[0m
[0m2021.03.04 10:24:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:24:33 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 10:24:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:24:51 INFO  time: compiled root in 0.89s[0m
[0m2021.03.04 10:25:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:25:52 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 10:26:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:26:11 INFO  time: compiled root in 0.73s[0m
[0m2021.03.04 10:26:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:26:52 INFO  time: compiled root in 0.63s[0m
[0m2021.03.04 10:27:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:27:26 INFO  time: compiled root in 0.76s[0m
[0m2021.03.04 10:27:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:27:40 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 10:27:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:27:50 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 10:27:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:27:59 INFO  time: compiled root in 0.34s[0m
[0m2021.03.04 10:28:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:28:33 INFO  time: compiled root in 0.97s[0m
[0m2021.03.04 10:28:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:28:59 INFO  time: compiled root in 0.84s[0m
[0m2021.03.04 10:30:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:30:28 INFO  time: compiled root in 1.03s[0m
[0m2021.03.04 10:30:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:30:33 INFO  time: compiled root in 0.79s[0m
Mar 04, 2021 10:32:23 AM scala.meta.internal.pc.CompletionProvider expected$1
WARNING: String index out of range: -1
[0m2021.03.04 10:32:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:32:30 INFO  time: compiled root in 0.1s[0m
[0m2021.03.04 10:32:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:32:35 INFO  time: compiled root in 0.13s[0m
[0m2021.03.04 10:32:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:24: stale bloop error: identifier expected but string literal found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                       ^[0m
[0m2021.03.04 10:32:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:122: stale bloop error: ';' expected but ')' found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                                                                                                                         ^[0m
[0m2021.03.04 10:32:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:24: stale bloop error: identifier expected but string literal found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                       ^[0m
[0m2021.03.04 10:32:42 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:122: stale bloop error: ';' expected but ')' found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                                                                                                                         ^[0m
[0m2021.03.04 10:32:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:24: stale bloop error: identifier expected but string literal found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                       ^[0m
[0m2021.03.04 10:32:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:122: stale bloop error: ';' expected but ')' found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                                                                                                                         ^[0m
[0m2021.03.04 10:32:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:24: stale bloop error: identifier expected but string literal found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                       ^[0m
[0m2021.03.04 10:32:44 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:122: stale bloop error: ';' expected but ')' found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                                                                                                                         ^[0m
[0m2021.03.04 10:32:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:24: stale bloop error: identifier expected but string literal found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                       ^[0m
[0m2021.03.04 10:32:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:122: stale bloop error: ';' expected but ')' found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                                                                                                                         ^[0m
[0m2021.03.04 10:32:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:24: stale bloop error: identifier expected but string literal found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                       ^[0m
[0m2021.03.04 10:32:47 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:122: stale bloop error: ';' expected but ')' found.
    .select($"Envelop".$"WARC-Header-Metadata"."WARC-Target-URI", $"Envelop"($"WARC-Header-Metadata"($"WARC-IP-Address")))
                                                                                                                         ^[0m
[0m2021.03.04 10:32:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:32:49 INFO  time: compiled root in 0.79s[0m
[0m2021.03.04 10:33:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:33:00 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 10:33:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:33:16 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 10:34:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:34:20 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 10:34:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:34:50 INFO  time: compiled root in 0.9s[0m
[0m2021.03.04 10:36:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:36:41 WARN  there were two deprecation warnings; re-run with -deprecation for details[0m
[0m2021.03.04 10:36:41 INFO  time: compiled root in 0.88s[0m
[0m2021.03.04 10:37:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:37:11 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.03.04 10:37:11 INFO  time: compiled root in 0.89s[0m
[0m2021.03.04 10:38:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:38:32 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.03.04 10:38:32 INFO  time: compiled root in 0.86s[0m
[0m2021.03.04 10:45:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:45:07 INFO  time: compiled root in 0.63s[0m
[0m2021.03.04 10:45:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:45:45 INFO  time: compiled root in 0.82s[0m
[0m2021.03.04 10:45:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:45:58 INFO  time: compiled root in 0.91s[0m
[0m2021.03.04 10:47:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:47:08 INFO  time: compiled root in 0.84s[0m
[0m2021.03.04 10:59:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:59:48 INFO  time: compiled root in 0.84s[0m
[0m2021.03.04 11:00:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:00:25 INFO  time: compiled root in 1.03s[0m
[0m2021.03.04 11:00:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:00:44 INFO  time: compiled root in 0.84s[0m
[0m2021.03.04 11:01:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:01:23 INFO  time: compiled root in 0.8s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.Map
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder

/**
 * A collection of implicit methods for converting common Scala objects into [[Dataset]]s.
 *
 * @since 1.6.0
 */
@InterfaceStability.Evolving
abstract class SQLImplicits extends LowPrioritySQLImplicits {

  protected def _sqlContext: SQLContext

  /**
   * Converts $"col name" into a [[Column]].
   *
   * @since 2.0.0
   */
  implicit class StringToColumn(val sc: StringContext) {
    def $(args: Any*): ColumnName = {
      new ColumnName(sc.s(args: _*))
    }
  }

  // Primitives

  /** @since 1.6.0 */
  implicit def newIntEncoder: Encoder[Int] = Encoders.scalaInt

  /** @since 1.6.0 */
  implicit def newLongEncoder: Encoder[Long] = Encoders.scalaLong

  /** @since 1.6.0 */
  implicit def newDoubleEncoder: Encoder[Double] = Encoders.scalaDouble

  /** @since 1.6.0 */
  implicit def newFloatEncoder: Encoder[Float] = Encoders.scalaFloat

  /** @since 1.6.0 */
  implicit def newByteEncoder: Encoder[Byte] = Encoders.scalaByte

  /** @since 1.6.0 */
  implicit def newShortEncoder: Encoder[Short] = Encoders.scalaShort

  /** @since 1.6.0 */
  implicit def newBooleanEncoder: Encoder[Boolean] = Encoders.scalaBoolean

  /** @since 1.6.0 */
  implicit def newStringEncoder: Encoder[String] = Encoders.STRING

  /** @since 2.2.0 */
  implicit def newJavaDecimalEncoder: Encoder[java.math.BigDecimal] = Encoders.DECIMAL

  /** @since 2.2.0 */
  implicit def newScalaDecimalEncoder: Encoder[scala.math.BigDecimal] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newDateEncoder: Encoder[java.sql.Date] = Encoders.DATE

  /** @since 2.2.0 */
  implicit def newTimeStampEncoder: Encoder[java.sql.Timestamp] = Encoders.TIMESTAMP


  // Boxed primitives

  /** @since 2.0.0 */
  implicit def newBoxedIntEncoder: Encoder[java.lang.Integer] = Encoders.INT

  /** @since 2.0.0 */
  implicit def newBoxedLongEncoder: Encoder[java.lang.Long] = Encoders.LONG

  /** @since 2.0.0 */
  implicit def newBoxedDoubleEncoder: Encoder[java.lang.Double] = Encoders.DOUBLE

  /** @since 2.0.0 */
  implicit def newBoxedFloatEncoder: Encoder[java.lang.Float] = Encoders.FLOAT

  /** @since 2.0.0 */
  implicit def newBoxedByteEncoder: Encoder[java.lang.Byte] = Encoders.BYTE

  /** @since 2.0.0 */
  implicit def newBoxedShortEncoder: Encoder[java.lang.Short] = Encoders.SHORT

  /** @since 2.0.0 */
  implicit def newBoxedBooleanEncoder: Encoder[java.lang.Boolean] = Encoders.BOOLEAN

  // Seqs

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newIntSeqEncoder: Encoder[Seq[Int]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newLongSeqEncoder: Encoder[Seq[Long]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newDoubleSeqEncoder: Encoder[Seq[Double]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newFloatSeqEncoder: Encoder[Seq[Float]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newByteSeqEncoder: Encoder[Seq[Byte]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newShortSeqEncoder: Encoder[Seq[Short]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newBooleanSeqEncoder: Encoder[Seq[Boolean]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newStringSeqEncoder: Encoder[Seq[String]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newProductSeqEncoder[A <: Product : TypeTag]: Encoder[Seq[A]] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newSequenceEncoder[T <: Seq[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Maps
  /** @since 2.3.0 */
  implicit def newMapEncoder[T <: Map[_, _] : TypeTag]: Encoder[T] = ExpressionEncoder()

  /**
   * Notice that we serialize `Set` to Catalyst array. The set property is only kept when
   * manipulating the domain objects. The serialization format doesn't keep the set property.
   * When we have a Catalyst array which contains duplicated elements and convert it to
   * `Dataset[Set[T]]` by using the encoder, the elements will be de-duplicated.
   *
   * @since 2.3.0
   */
  implicit def newSetEncoder[T <: Set[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Arrays

  /** @since 1.6.1 */
  implicit def newIntArrayEncoder: Encoder[Array[Int]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newLongArrayEncoder: Encoder[Array[Long]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newDoubleArrayEncoder: Encoder[Array[Double]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newFloatArrayEncoder: Encoder[Array[Float]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newByteArrayEncoder: Encoder[Array[Byte]] = Encoders.BINARY

  /** @since 1.6.1 */
  implicit def newShortArrayEncoder: Encoder[Array[Short]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newBooleanArrayEncoder: Encoder[Array[Boolean]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newStringArrayEncoder: Encoder[Array[String]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newProductArrayEncoder[A <: Product : TypeTag]: Encoder[Array[A]] =
    ExpressionEncoder()

  /**
   * Creates a [[Dataset]] from an RDD.
   *
   * @since 1.6.0
   */
  implicit def rddToDatasetHolder[T : Encoder](rdd: RDD[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(rdd))
  }

  /**
   * Creates a [[Dataset]] from a local Seq.
   * @since 1.6.0
   */
  implicit def localSeqToDatasetHolder[T : Encoder](s: Seq[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(s))
  }

  /**
   * An implicit conversion that turns a Scala `Symbol` into a [[Column]].
   * @since 1.3.0
   */
  implicit def symbolToColumn(s: Symbol): ColumnName = new ColumnName(s.name)

}

/**
 * Lower priority implicit methods for converting Scala objects into [[Dataset]]s.
 * Conflicting implicits are placed here to disambiguate resolution.
 *
 * Reasons for including specific implicits:
 * newProductEncoder - to disambiguate for `List`s which are both `Seq` and `Product`
 */
trait LowPrioritySQLImplicits {
  /** @since 1.6.0 */
  implicit def newProductEncoder[T <: Product : TypeTag]: Encoder[T] = Encoders.product[T]

}

[0m2021.03.04 11:03:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:03:20 INFO  time: compiled root in 0.23s[0m
[0m2021.03.04 11:03:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:03:33 INFO  time: compiled root in 0.77s[0m
[0m2021.03.04 11:04:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:04:39 INFO  time: compiled root in 0.23s[0m
[0m2021.03.04 11:04:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:04:52 INFO  time: compiled root in 0.8s[0m
[0m2021.03.04 11:04:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:04:57 INFO  time: compiled root in 0.2s[0m
[0m2021.03.04 11:05:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:05:36 INFO  time: compiled root in 0.18s[0m
[0m2021.03.04 11:05:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:05:52 INFO  time: compiled root in 0.73s[0m
[0m2021.03.04 11:06:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:06:06 INFO  time: compiled root in 0.76s[0m
[0m2021.03.04 11:07:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:07:20 INFO  time: compiled root in 0.78s[0m
[0m2021.03.04 11:09:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:09:18 INFO  time: compiled root in 0.23s[0m
[0m2021.03.04 11:09:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:09:42 INFO  time: compiled root in 0.12s[0m
[0m2021.03.04 11:09:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:51: stale bloop error: ')' expected but string literal found.
    .withColumn("URL Address", col($"value"(rlike "WARC-Target-URI:.*")))
                                                  ^[0m
[0m2021.03.04 11:09:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:88:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 11:09:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:51: stale bloop error: ')' expected but string literal found.
    .withColumn("URL Address", col($"value"(rlike "WARC-Target-URI:.*")))
                                                  ^[0m
[0m2021.03.04 11:09:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:88:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 11:09:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:51: stale bloop error: ')' expected but string literal found.
    .withColumn("URL Address", col($"value"(rlike "WARC-Target-URI:.*")))
                                                  ^[0m
[0m2021.03.04 11:09:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:88:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 11:09:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:51: stale bloop error: ')' expected but string literal found.
    .withColumn("URL Address", col($"value"(rlike "WARC-Target-URI:.*")))
                                                  ^[0m
[0m2021.03.04 11:09:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:88:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 11:09:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:51: stale bloop error: ')' expected but string literal found.
    .withColumn("URL Address", col($"value"(rlike "WARC-Target-URI:.*")))
                                                  ^[0m
[0m2021.03.04 11:09:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:88:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 11:09:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:67:51: stale bloop error: ')' expected but string literal found.
    .withColumn("URL Address", col($"value"(rlike "WARC-Target-URI:.*")))
                                                  ^[0m
[0m2021.03.04 11:09:51 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:88:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 11:09:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:09:55 INFO  time: compiled root in 0.18s[0m
[0m2021.03.04 11:10:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:10:05 INFO  time: compiled root in 0.2s[0m
[0m2021.03.04 11:10:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:10:18 INFO  time: compiled root in 0.78s[0m
[0m2021.03.04 11:11:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:11:39 INFO  time: compiled root in 0.72s[0m
[0m2021.03.04 11:12:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:12:18 INFO  time: compiled root in 0.73s[0m
[0m2021.03.04 11:12:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:12:20 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 11:13:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:13:08 INFO  time: compiled root in 0.73s[0m
[0m2021.03.04 11:13:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:13:14 INFO  time: compiled root in 0.71s[0m
[0m2021.03.04 11:13:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:13:48 INFO  time: compiled root in 0.8s[0m
[0m2021.03.04 11:16:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:16:22 INFO  time: compiled root in 0.19s[0m
[0m2021.03.04 11:16:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:16:29 INFO  time: compiled root in 0.93s[0m
Mar 04, 2021 11:17:27 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3040
[0m2021.03.04 11:18:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:18:07 INFO  time: compiled root in 0.16s[0m
[0m2021.03.04 11:18:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:18:13 INFO  time: compiled root in 0.77s[0m
[0m2021.03.04 11:19:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:19:16 INFO  time: compiled root in 0.78s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.encoders.{encoderFor, ExpressionEncoder}
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
import org.apache.spark.sql.catalyst.util.toPrettySQL
import org.apache.spark.sql.execution.aggregate.TypedAggregateExpression
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.types._

private[sql] object Column {

  def apply(colName: String): Column = new Column(colName)

  def apply(expr: Expression): Column = new Column(expr)

  def unapply(col: Column): Option[Expression] = Some(col.expr)

  private[sql] def generateAlias(e: Expression): String = {
    e match {
      case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
        a.aggregateFunction.toString
      case expr => toPrettySQL(expr)
    }
  }
}

/**
 * A [[Column]] where an [[Encoder]] has been given for the expected input and return type.
 * To create a [[TypedColumn]], use the `as` function on a [[Column]].
 *
 * @tparam T The input type expected for this expression.  Can be `Any` if the expression is type
 *           checked by the analyzer instead of the compiler (i.e. `expr("sum(...)")`).
 * @tparam U The output type of this column.
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class TypedColumn[-T, U](
    expr: Expression,
    private[sql] val encoder: ExpressionEncoder[U])
  extends Column(expr) {

  /**
   * Inserts the specific input type and schema into any expressions that are expected to operate
   * on a decoded object.
   */
  private[sql] def withInputType(
      inputEncoder: ExpressionEncoder[_],
      inputAttributes: Seq[Attribute]): TypedColumn[T, U] = {
    val unresolvedDeserializer = UnresolvedDeserializer(inputEncoder.deserializer, inputAttributes)
    val newExpr = expr transform {
      case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty =>
        ta.withInputInfo(
          deser = unresolvedDeserializer,
          cls = inputEncoder.clsTag.runtimeClass,
          schema = inputEncoder.schema)
    }
    new TypedColumn[T, U](newExpr, encoder)
  }

  /**
   * Gives the [[TypedColumn]] a name (alias).
   * If the current `TypedColumn` has metadata associated with it, this metadata will be propagated
   * to the new column.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  override def name(alias: String): TypedColumn[T, U] =
    new TypedColumn[T, U](super.name(alias).expr, encoder)

}

/**
 * A column that will be computed based on the data in a `DataFrame`.
 *
 * A new column can be constructed based on the input columns present in a DataFrame:
 *
 * {{{
 *   df("columnName")            // On a specific `df` DataFrame.
 *   col("columnName")           // A generic column not yet associated with a DataFrame.
 *   col("columnName.field")     // Extracting a struct field
 *   col("`a.column.with.dots`") // Escape `.` in column names.
 *   $"columnName"               // Scala short hand for a named column.
 * }}}
 *
 * [[Column]] objects can be composed to form complex expressions:
 *
 * {{{
 *   $"a" + 1
 *   $"a" === $"b"
 * }}}
 *
 * @note The internal Catalyst expression can be accessed via [[expr]], but this method is for
 * debugging purposes only and can change in any future Spark releases.
 *
 * @groupname java_expr_ops Java-specific expression operators
 * @groupname expr_ops Expression operators
 * @groupname df_ops DataFrame functions
 * @groupname Ungrouped Support functions for DataFrames
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class Column(val expr: Expression) extends Logging {

  def this(name: String) = this(name match {
    case "*" => UnresolvedStar(None)
    case _ if name.endsWith(".*") =>
      val parts = UnresolvedAttribute.parseAttributeName(name.substring(0, name.length - 2))
      UnresolvedStar(Some(parts))
    case _ => UnresolvedAttribute.quotedString(name)
  })

  override def toString: String = toPrettySQL(expr)

  override def equals(that: Any): Boolean = that match {
    case that: Column => that.expr.equals(this.expr)
    case _ => false
  }

  override def hashCode: Int = this.expr.hashCode()

  /** Creates a column based on the given expression. */
  private def withExpr(newExpr: Expression): Column = new Column(newExpr)

  /**
   * Returns the expression for this column either with an existing or auto assigned name.
   */
  private[sql] def named: NamedExpression = expr match {
    // Wrap UnresolvedAttribute with UnresolvedAlias, as when we resolve UnresolvedAttribute, we
    // will remove intermediate Alias for ExtractValue chain, and we need to alias it again to
    // make it a NamedExpression.
    case u: UnresolvedAttribute => UnresolvedAlias(u)

    case u: UnresolvedExtractValue => UnresolvedAlias(u)

    case expr: NamedExpression => expr

    // Leave an unaliased generator with an empty list of names since the analyzer will generate
    // the correct defaults after the nested expression's type has been resolved.
    case g: Generator => MultiAlias(g, Nil)

    case func: UnresolvedFunction => UnresolvedAlias(func, Some(Column.generateAlias))

    // If we have a top level Cast, there is a chance to give it a better alias, if there is a
    // NamedExpression under this Cast.
    case c: Cast =>
      c.transformUp {
        case c @ Cast(_: NamedExpression, _, _) => UnresolvedAlias(c)
      } match {
        case ne: NamedExpression => ne
        case _ => Alias(expr, toPrettySQL(expr))()
      }

    case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
      UnresolvedAlias(a, Some(Column.generateAlias))

    // Wait until the struct is resolved. This will generate a nicer looking alias.
    case struct: CreateNamedStructLike => UnresolvedAlias(struct)

    case expr: Expression => Alias(expr, toPrettySQL(expr))()
  }

  /**
   * Provides a type hint about the expected return value of this column.  This information can
   * be used by operations such as `select` on a [[Dataset]] to automatically convert the
   * results into the correct JVM types.
   * @since 1.6.0
   */
  def as[U : Encoder]: TypedColumn[Any, U] = new TypedColumn[Any, U](expr, encoderFor[U])

  /**
   * Extracts a value or values from a complex type.
   * The following types of extraction are supported:
   * <ul>
   * <li>Given an Array, an integer ordinal can be used to retrieve a single value.</li>
   * <li>Given a Map, a key of the correct type can be used to retrieve an individual value.</li>
   * <li>Given a Struct, a string fieldName can be used to extract that field.</li>
   * <li>Given an Array of Structs, a string fieldName can be used to extract filed
   *    of every struct in that array, and return an Array of fields.</li>
   * </ul>
   * @group expr_ops
   * @since 1.4.0
   */
  def apply(extraction: Any): Column = withExpr {
    UnresolvedExtractValue(expr, lit(extraction).expr)
  }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Scala: select the amount column and negates all values.
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.select( negate(col("amount") );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_- : Column = withExpr { UnaryMinus(expr) }

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_! : Column = withExpr { Not(expr) }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def === (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} = $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualTo(expr, right)
  }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def equalTo(other: Any): Column = this === other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") =!= df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
    */
  def =!= (other: Any): Column = withExpr{ Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
    */
  @deprecated("!== does not have the same precedence as ===, use =!= instead", "2.0.0")
  def !== (other: Any): Column = this =!= other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def notEqual(other: Any): Column = withExpr { Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > 21 )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def > (other: Any): Column = withExpr { GreaterThan(expr, lit(other).expr) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > lit(21) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def gt(other: Any): Column = this > other

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def < (other: Any): Column = withExpr { LessThan(expr, lit(other).expr) }

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def lt(other: Any): Column = this < other

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <= (other: Any): Column = withExpr { LessThanOrEqual(expr, lit(other).expr) }

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def leq(other: Any): Column = this <= other

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def >= (other: Any): Column = withExpr { GreaterThanOrEqual(expr, lit(other).expr) }

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def geq(other: Any): Column = this >= other

  /**
   * Equality test that is safe for null values.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <=> (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} <=> $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualNullSafe(expr, right)
  }

  /**
   * Equality test that is safe for null values.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def eqNullSafe(other: Any): Column = this <=> other

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches :+ ((condition.expr, lit(value).expr))) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "when() cannot be applied once otherwise() is applied")
    case _ =>
      throw new IllegalArgumentException(
        "when() can only be applied on a Column previously generated by when() function")
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def otherwise(value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches, Option(lit(value).expr)) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied once on a Column previously generated by when()")
    case _ =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied on a Column previously generated by when()")
  }

  /**
   * True if the current column is between the lower bound and upper bound, inclusive.
   *
   * @group java_expr_ops
   * @since 1.4.0
   */
  def between(lowerBound: Any, upperBound: Any): Column = {
    (this >= lowerBound) && (this <= upperBound)
  }

  /**
   * True if the current expression is NaN.
   *
   * @group expr_ops
   * @since 1.5.0
   */
  def isNaN: Column = withExpr { IsNaN(expr) }

  /**
   * True if the current expression is null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNull: Column = withExpr { IsNull(expr) }

  /**
   * True if the current expression is NOT null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNotNull: Column = withExpr { IsNotNull(expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def || (other: Any): Column = withExpr { Or(expr, lit(other).expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def or(other: Column): Column = this || other

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def && (other: Any): Column = withExpr { And(expr, lit(other).expr) }

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def and(other: Column): Column = this && other

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def + (other: Any): Column = withExpr { Add(expr, lit(other).expr) }

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def plus(other: Any): Column = this + other

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def - (other: Any): Column = withExpr { Subtract(expr, lit(other).expr) }

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def minus(other: Any): Column = this - other

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def * (other: Any): Column = withExpr { Multiply(expr, lit(other).expr) }

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def multiply(other: Any): Column = this * other

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def / (other: Any): Column = withExpr { Divide(expr, lit(other).expr) }

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def divide(other: Any): Column = this / other

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def % (other: Any): Column = withExpr { Remainder(expr, lit(other).expr) }

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def mod(other: Any): Column = this % other

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the evaluated values of the arguments.
   *
   * Note: Since the type of the elements in the list are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def isin(list: Any*): Column = withExpr { In(expr, list.map(lit(_).expr)) }

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: scala.collection.Iterable[_]): Column = isin(values.toSeq: _*)

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group java_expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: java.lang.Iterable[_]): Column = isInCollection(values.asScala)

  /**
   * SQL like expression. Returns a boolean column based on a SQL LIKE match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def like(literal: String): Column = withExpr { Like(expr, lit(literal).expr) }

  /**
   * SQL RLIKE expression (LIKE with Regex). Returns a boolean column based on a regex
   * match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def rlike(literal: String): Column = withExpr { RLike(expr, lit(literal).expr) }

  /**
   * An expression that gets an item at position `ordinal` out of an array,
   * or gets a value by key `key` in a `MapType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getItem(key: Any): Column = withExpr { UnresolvedExtractValue(expr, Literal(key)) }

  /**
   * An expression that gets a field by name in a `StructType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getField(fieldName: String): Column = withExpr {
    UnresolvedExtractValue(expr, Literal(fieldName))
  }

  /**
   * An expression that returns a substring.
   * @param startPos expression for the starting position.
   * @param len expression for the length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Column, len: Column): Column = withExpr {
    Substring(expr, startPos.expr, len.expr)
  }

  /**
   * An expression that returns a substring.
   * @param startPos starting position.
   * @param len length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Int, len: Int): Column = withExpr {
    Substring(expr, lit(startPos).expr, lit(len).expr)
  }

  /**
   * Contains the other element. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def contains(other: Any): Column = withExpr { Contains(expr, lit(other).expr) }

  /**
   * String starts with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(other: Column): Column = withExpr { StartsWith(expr, lit(other).expr) }

  /**
   * String starts with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(literal: String): Column = this.startsWith(lit(literal))

  /**
   * String ends with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(other: Column): Column = withExpr { EndsWith(expr, lit(other).expr) }

  /**
   * String ends with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(literal: String): Column = this.endsWith(lit(literal))

  /**
   * Gives the column an alias. Same as `as`.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".alias("colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def alias(alias: String): Column = name(alias)

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String): Column = name(alias)

  /**
   * (Scala-specific) Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Seq[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Array[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as('colB))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: Symbol): Column = name(alias.name)

  /**
   * Gives the column an alias with metadata.
   * {{{
   *   val metadata: Metadata = ...
   *   df.select($"colA".as("colB", metadata))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String, metadata: Metadata): Column = withExpr {
    Alias(expr, alias)(explicitMetadata = Some(metadata))
  }

  /**
   * Gives the column a name (alias).
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".name("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def name(alias: String): Column = withExpr {
    expr match {
      case ne: NamedExpression => Alias(expr, alias)(explicitMetadata = Some(ne.metadata))
      case other => Alias(other, alias)()
    }
  }

  /**
   * Casts the column to a different data type.
   * {{{
   *   // Casts colA to IntegerType.
   *   import org.apache.spark.sql.types.IntegerType
   *   df.select(df("colA").cast(IntegerType))
   *
   *   // equivalent to
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: DataType): Column = withExpr { Cast(expr, to) }

  /**
   * Casts the column to a different data type, using the canonical string representation
   * of the type. The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
   * `float`, `double`, `decimal`, `date`, `timestamp`.
   * {{{
   *   // Casts colA to integer.
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: String): Column = cast(CatalystSqlParser.parseDataType(to))

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   // Scala
   *   df.sort(df("age").desc)
   *
   *   // Java
   *   df.sort(df.col("age").desc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def desc: Column = withExpr { SortOrder(expr, Descending) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing first.
   *   df.sort(df("age").desc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_first: Column = withExpr { SortOrder(expr, Descending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing last.
   *   df.sort(df("age").desc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_last: Column = withExpr { SortOrder(expr, Descending, NullsLast, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order.
   *   df.sort(df("age").asc)
   *
   *   // Java
   *   df.sort(df.col("age").asc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def asc: Column = withExpr { SortOrder(expr, Ascending) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing first.
   *   df.sort(df("age").asc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_first: Column = withExpr { SortOrder(expr, Ascending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing last.
   *   df.sort(df("age").asc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_last: Column = withExpr { SortOrder(expr, Ascending, NullsLast, Set.empty) }

  /**
   * Prints the expression to the console for debugging purposes.
   *
   * @group df_ops
   * @since 1.3.0
   */
  def explain(extended: Boolean): Unit = {
    // scalastyle:off println
    if (extended) {
      println(expr)
    } else {
      println(expr.sql)
    }
    // scalastyle:on println
  }

  /**
   * Compute bitwise OR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseOR(other: Any): Column = withExpr { BitwiseOr(expr, lit(other).expr) }

  /**
   * Compute bitwise AND of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseAND($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseAND(other: Any): Column = withExpr { BitwiseAnd(expr, lit(other).expr) }

  /**
   * Compute bitwise XOR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseXOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseXOR(other: Any): Column = withExpr { BitwiseXor(expr, lit(other).expr) }

  /**
   * Defines a windowing column.
   *
   * {{{
   *   val w = Window.partitionBy("name").orderBy("id")
   *   df.select(
   *     sum("price").over(w.rangeBetween(Window.unboundedPreceding, 2)),
   *     avg("price").over(w.rowsBetween(Window.currentRow, 4))
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def over(window: expressions.WindowSpec): Column = window.withAggregate(this)

  /**
   * Defines an empty analytic clause. In this case the analytic function is applied
   * and presented for all rows in the result set.
   *
   * {{{
   *   df.select(
   *     sum("price").over(),
   *     avg("price").over()
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def over(): Column = over(Window.spec)

}


/**
 * A convenient class used for constructing schema.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class ColumnName(name: String) extends Column(name) {

  /**
   * Creates a new `StructField` of type boolean.
   * @since 1.3.0
   */
  def boolean: StructField = StructField(name, BooleanType)

  /**
   * Creates a new `StructField` of type byte.
   * @since 1.3.0
   */
  def byte: StructField = StructField(name, ByteType)

  /**
   * Creates a new `StructField` of type short.
   * @since 1.3.0
   */
  def short: StructField = StructField(name, ShortType)

  /**
   * Creates a new `StructField` of type int.
   * @since 1.3.0
   */
  def int: StructField = StructField(name, IntegerType)

  /**
   * Creates a new `StructField` of type long.
   * @since 1.3.0
   */
  def long: StructField = StructField(name, LongType)

  /**
   * Creates a new `StructField` of type float.
   * @since 1.3.0
   */
  def float: StructField = StructField(name, FloatType)

  /**
   * Creates a new `StructField` of type double.
   * @since 1.3.0
   */
  def double: StructField = StructField(name, DoubleType)

  /**
   * Creates a new `StructField` of type string.
   * @since 1.3.0
   */
  def string: StructField = StructField(name, StringType)

  /**
   * Creates a new `StructField` of type date.
   * @since 1.3.0
   */
  def date: StructField = StructField(name, DateType)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal: StructField = StructField(name, DecimalType.USER_DEFAULT)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal(precision: Int, scale: Int): StructField =
    StructField(name, DecimalType(precision, scale))

  /**
   * Creates a new `StructField` of type timestamp.
   * @since 1.3.0
   */
  def timestamp: StructField = StructField(name, TimestampType)

  /**
   * Creates a new `StructField` of type binary.
   * @since 1.3.0
   */
  def binary: StructField = StructField(name, BinaryType)

  /**
   * Creates a new `StructField` of type array.
   * @since 1.3.0
   */
  def array(dataType: DataType): StructField = StructField(name, ArrayType(dataType))

  /**
   * Creates a new `StructField` of type map.
   * @since 1.3.0
   */
  def map(keyType: DataType, valueType: DataType): StructField =
    map(MapType(keyType, valueType))

  def map(mapType: MapType): StructField = StructField(name, mapType)

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(fields: StructField*): StructField = struct(StructType(fields))

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(structType: StructType): StructField = StructField(name, structType)
}

[0m2021.03.04 11:21:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:21:24 INFO  time: compiled root in 0.76s[0m
[0m2021.03.04 11:24:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:24:36 INFO  time: compiled root in 0.19s[0m
[0m2021.03.04 11:24:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:24:41 INFO  time: compiled root in 1.19s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.{typeTag, TypeTag}
import scala.util.Try
import scala.util.control.NonFatal

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.sql.api.java._
import org.apache.spark.sql.catalyst.ScalaReflection
import org.apache.spark.sql.catalyst.analysis.{Star, UnresolvedFunction}
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate._
import org.apache.spark.sql.catalyst.plans.logical.{HintInfo, ResolvedHint}
import org.apache.spark.sql.execution.SparkSqlParser
import org.apache.spark.sql.expressions.{SparkUserDefinedFunction, UserDefinedFunction}
import org.apache.spark.sql.internal.SQLConf
import org.apache.spark.sql.types._
import org.apache.spark.util.Utils


/**
 * Commonly used functions available for DataFrame operations. Using functions defined here provides
 * a little bit more compile-time safety to make sure the function exists.
 *
 * Spark also includes more built-in functions that are less common and are not defined here.
 * You can still access them (and all the functions defined here) using the `functions.expr()` API
 * and calling them through a SQL expression string. You can find the entire list of functions
 * at SQL API documentation.
 *
 * As an example, `isnan` is a function that is defined here. You can use `isnan(col("myCol"))`
 * to invoke the `isnan` function. This way the programming language's compiler ensures `isnan`
 * exists and is of the proper form. You can also use `expr("isnan(myCol)")` function to invoke the
 * same function. In this case, Spark itself will ensure `isnan` exists when it analyzes the query.
 *
 * `regr_count` is an example of a function that is built-in but not defined here, because it is
 * less commonly used. To invoke it, use `expr("regr_count(yCol, xCol)")`.
 *
 * @groupname udf_funcs UDF functions
 * @groupname agg_funcs Aggregate functions
 * @groupname datetime_funcs Date time functions
 * @groupname sort_funcs Sorting functions
 * @groupname normal_funcs Non-aggregate functions
 * @groupname math_funcs Math functions
 * @groupname misc_funcs Misc functions
 * @groupname window_funcs Window functions
 * @groupname string_funcs String functions
 * @groupname collection_funcs Collection functions
 * @groupname Ungrouped Support functions for DataFrames
 * @since 1.3.0
 */
@InterfaceStability.Stable
// scalastyle:off
object functions {
// scalastyle:on

  private def withExpr(expr: Expression): Column = Column(expr)

  private def withAggregateFunction(
    func: AggregateFunction,
    isDistinct: Boolean = false): Column = {
    Column(func.toAggregateExpression(isDistinct))
  }

  /**
   * Returns a [[Column]] based on the given column name.
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def col(colName: String): Column = Column(colName)

  /**
   * Returns a [[Column]] based on the given column name. Alias of [[col]].
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def column(colName: String): Column = Column(colName)

  /**
   * Creates a [[Column]] of literal value.
   *
   * The passed in object is returned directly if it is already a [[Column]].
   * If the object is a Scala Symbol, it is converted into a [[Column]] also.
   * Otherwise, a new [[Column]] is created to represent the literal value.
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def lit(literal: Any): Column = typedLit(literal)

  /**
   * Creates a [[Column]] of literal value.
   *
   * The passed in object is returned directly if it is already a [[Column]].
   * If the object is a Scala Symbol, it is converted into a [[Column]] also.
   * Otherwise, a new [[Column]] is created to represent the literal value.
   * The difference between this function and [[lit]] is that this function
   * can handle parameterized scala types e.g.: List, Seq and Map.
   *
   * @group normal_funcs
   * @since 2.2.0
   */
  def typedLit[T : TypeTag](literal: T): Column = literal match {
    case c: Column => c
    case s: Symbol => new ColumnName(s.name)
    case _ => Column(Literal.create(literal))
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Sort functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   df.sort(asc("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 1.3.0
   */
  def asc(columnName: String): Column = Column(columnName).asc

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   df.sort(asc_nulls_first("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def asc_nulls_first(columnName: String): Column = Column(columnName).asc_nulls_first

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   df.sort(asc_nulls_last("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def asc_nulls_last(columnName: String): Column = Column(columnName).asc_nulls_last

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   df.sort(asc("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 1.3.0
   */
  def desc(columnName: String): Column = Column(columnName).desc

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   df.sort(asc("dept"), desc_nulls_first("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def desc_nulls_first(columnName: String): Column = Column(columnName).desc_nulls_first

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   df.sort(asc("dept"), desc_nulls_last("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def desc_nulls_last(columnName: String): Column = Column(columnName).desc_nulls_last


  //////////////////////////////////////////////////////////////////////////////////////////////
  // Aggregate functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(e: Column): Column = approx_count_distinct(e)

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(columnName: String): Column = approx_count_distinct(columnName)

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(e: Column, rsd: Double): Column = approx_count_distinct(e, rsd)

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(columnName: String, rsd: Double): Column = {
    approx_count_distinct(Column(columnName), rsd)
  }

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(e: Column): Column = withAggregateFunction {
    HyperLogLogPlusPlus(e.expr)
  }

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(columnName: String): Column = approx_count_distinct(column(columnName))

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @param rsd maximum estimation error allowed (default = 0.05)
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(e: Column, rsd: Double): Column = withAggregateFunction {
    HyperLogLogPlusPlus(e.expr, rsd, 0, 0)
  }

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @param rsd maximum estimation error allowed (default = 0.05)
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(columnName: String, rsd: Double): Column = {
    approx_count_distinct(Column(columnName), rsd)
  }

  /**
   * Aggregate function: returns the average of the values in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def avg(e: Column): Column = withAggregateFunction { Average(e.expr) }

  /**
   * Aggregate function: returns the average of the values in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def avg(columnName: String): Column = avg(Column(columnName))

  /**
   * Aggregate function: returns a list of objects with duplicates.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_list(e: Column): Column = withAggregateFunction { CollectList(e.expr) }

  /**
   * Aggregate function: returns a list of objects with duplicates.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_list(columnName: String): Column = collect_list(Column(columnName))

  /**
   * Aggregate function: returns a set of objects with duplicate elements eliminated.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_set(e: Column): Column = withAggregateFunction { CollectSet(e.expr) }

  /**
   * Aggregate function: returns a set of objects with duplicate elements eliminated.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_set(columnName: String): Column = collect_set(Column(columnName))

  /**
   * Aggregate function: returns the Pearson Correlation Coefficient for two columns.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def corr(column1: Column, column2: Column): Column = withAggregateFunction {
    Corr(column1.expr, column2.expr)
  }

  /**
   * Aggregate function: returns the Pearson Correlation Coefficient for two columns.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def corr(columnName1: String, columnName2: String): Column = {
    corr(Column(columnName1), Column(columnName2))
  }

  /**
   * Aggregate function: returns the number of items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def count(e: Column): Column = withAggregateFunction {
    e.expr match {
      // Turn count(*) into count(1)
      case s: Star => Count(Literal(1))
      case _ => Count(e.expr)
    }
  }

  /**
   * Aggregate function: returns the number of items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def count(columnName: String): TypedColumn[Any, Long] =
    count(Column(columnName)).as(ExpressionEncoder[Long]())

  /**
   * Aggregate function: returns the number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  @scala.annotation.varargs
  def countDistinct(expr: Column, exprs: Column*): Column = {
    withAggregateFunction(Count.apply((expr +: exprs).map(_.expr)), isDistinct = true)
  }

  /**
   * Aggregate function: returns the number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  @scala.annotation.varargs
  def countDistinct(columnName: String, columnNames: String*): Column =
    countDistinct(Column(columnName), columnNames.map(Column.apply) : _*)

  /**
   * Aggregate function: returns the population covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_pop(column1: Column, column2: Column): Column = withAggregateFunction {
    CovPopulation(column1.expr, column2.expr)
  }

  /**
   * Aggregate function: returns the population covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_pop(columnName1: String, columnName2: String): Column = {
    covar_pop(Column(columnName1), Column(columnName2))
  }

  /**
   * Aggregate function: returns the sample covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_samp(column1: Column, column2: Column): Column = withAggregateFunction {
    CovSample(column1.expr, column2.expr)
  }

  /**
   * Aggregate function: returns the sample covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_samp(columnName1: String, columnName2: String): Column = {
    covar_samp(Column(columnName1), Column(columnName2))
  }

  /**
   * Aggregate function: returns the first value in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def first(e: Column, ignoreNulls: Boolean): Column = withAggregateFunction {
    new First(e.expr, ignoreNulls)
  }

  /**
   * Aggregate function: returns the first value of a column in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def first(columnName: String, ignoreNulls: Boolean): Column = {
    first(Column(columnName), ignoreNulls)
  }

  /**
   * Aggregate function: returns the first value in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def first(e: Column): Column = first(e, ignoreNulls = false)

  /**
   * Aggregate function: returns the first value of a column in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def first(columnName: String): Column = first(Column(columnName))

  /**
   * Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated
   * or not, returns 1 for aggregated or 0 for not aggregated in the result set.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping(e: Column): Column = Column(Grouping(e.expr))

  /**
   * Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated
   * or not, returns 1 for aggregated or 0 for not aggregated in the result set.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping(columnName: String): Column = grouping(Column(columnName))

  /**
   * Aggregate function: returns the level of grouping, equals to
   *
   * {{{
   *   (grouping(c1) <<; (n-1)) + (grouping(c2) <<; (n-2)) + ... + grouping(cn)
   * }}}
   *
   * @note The list of columns should match with grouping columns exactly, or empty (means all the
   * grouping columns).
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping_id(cols: Column*): Column = Column(GroupingID(cols.map(_.expr)))

  /**
   * Aggregate function: returns the level of grouping, equals to
   *
   * {{{
   *   (grouping(c1) <<; (n-1)) + (grouping(c2) <<; (n-2)) + ... + grouping(cn)
   * }}}
   *
   * @note The list of columns should match with grouping columns exactly.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping_id(colName: String, colNames: String*): Column = {
    grouping_id((Seq(colName) ++ colNames).map(n => Column(n)) : _*)
  }

  /**
   * Aggregate function: returns the kurtosis of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def kurtosis(e: Column): Column = withAggregateFunction { Kurtosis(e.expr) }

  /**
   * Aggregate function: returns the kurtosis of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def kurtosis(columnName: String): Column = kurtosis(Column(columnName))

  /**
   * Aggregate function: returns the last value in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def last(e: Column, ignoreNulls: Boolean): Column = withAggregateFunction {
    new Last(e.expr, ignoreNulls)
  }

  /**
   * Aggregate function: returns the last value of the column in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def last(columnName: String, ignoreNulls: Boolean): Column = {
    last(Column(columnName), ignoreNulls)
  }

  /**
   * Aggregate function: returns the last value in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def last(e: Column): Column = last(e, ignoreNulls = false)

  /**
   * Aggregate function: returns the last value of the column in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def last(columnName: String): Column = last(Column(columnName), ignoreNulls = false)

  /**
   * Aggregate function: returns the maximum value of the expression in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def max(e: Column): Column = withAggregateFunction { Max(e.expr) }

  /**
   * Aggregate function: returns the maximum value of the column in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def max(columnName: String): Column = max(Column(columnName))

  /**
   * Aggregate function: returns the average of the values in a group.
   * Alias for avg.
   *
   * @group agg_funcs
   * @since 1.4.0
   */
  def mean(e: Column): Column = avg(e)

  /**
   * Aggregate function: returns the average of the values in a group.
   * Alias for avg.
   *
   * @group agg_funcs
   * @since 1.4.0
   */
  def mean(columnName: String): Column = avg(columnName)

  /**
   * Aggregate function: returns the minimum value of the expression in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def min(e: Column): Column = withAggregateFunction { Min(e.expr) }

  /**
   * Aggregate function: returns the minimum value of the column in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def min(columnName: String): Column = min(Column(columnName))

  /**
   * Aggregate function: returns the skewness of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def skewness(e: Column): Column = withAggregateFunction { Skewness(e.expr) }

  /**
   * Aggregate function: returns the skewness of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def skewness(columnName: String): Column = skewness(Column(columnName))

  /**
   * Aggregate function: alias for `stddev_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev(e: Column): Column = withAggregateFunction { StddevSamp(e.expr) }

  /**
   * Aggregate function: alias for `stddev_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev(columnName: String): Column = stddev(Column(columnName))

  /**
   * Aggregate function: returns the sample standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_samp(e: Column): Column = withAggregateFunction { StddevSamp(e.expr) }

  /**
   * Aggregate function: returns the sample standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_samp(columnName: String): Column = stddev_samp(Column(columnName))

  /**
   * Aggregate function: returns the population standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_pop(e: Column): Column = withAggregateFunction { StddevPop(e.expr) }

  /**
   * Aggregate function: returns the population standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_pop(columnName: String): Column = stddev_pop(Column(columnName))

  /**
   * Aggregate function: returns the sum of all values in the expression.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sum(e: Column): Column = withAggregateFunction { Sum(e.expr) }

  /**
   * Aggregate function: returns the sum of all values in the given column.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sum(columnName: String): Column = sum(Column(columnName))

  /**
   * Aggregate function: returns the sum of distinct values in the expression.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sumDistinct(e: Column): Column = withAggregateFunction(Sum(e.expr), isDistinct = true)

  /**
   * Aggregate function: returns the sum of distinct values in the expression.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sumDistinct(columnName: String): Column = sumDistinct(Column(columnName))

  /**
   * Aggregate function: alias for `var_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def variance(e: Column): Column = withAggregateFunction { VarianceSamp(e.expr) }

  /**
   * Aggregate function: alias for `var_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def variance(columnName: String): Column = variance(Column(columnName))

  /**
   * Aggregate function: returns the unbiased variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_samp(e: Column): Column = withAggregateFunction { VarianceSamp(e.expr) }

  /**
   * Aggregate function: returns the unbiased variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_samp(columnName: String): Column = var_samp(Column(columnName))

  /**
   * Aggregate function: returns the population variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_pop(e: Column): Column = withAggregateFunction { VariancePop(e.expr) }

  /**
   * Aggregate function: returns the population variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_pop(columnName: String): Column = var_pop(Column(columnName))


  //////////////////////////////////////////////////////////////////////////////////////////////
  // Window functions
  //////////////////////////////////////////////////////////////////////////////////////////////
  /**
   * This function has been deprecated in Spark 2.4. See SPARK-25842 for more information.
   *
   * @group window_funcs
   * @since 2.3.0
   */
  @deprecated("Use Window.unboundedPreceding", "2.4.0")
  def unboundedPreceding(): Column = Column(UnboundedPreceding)

  /**
   * This function has been deprecated in Spark 2.4. See SPARK-25842 for more information.
   *
   * @group window_funcs
   * @since 2.3.0
   */
  @deprecated("Use Window.unboundedFollowing", "2.4.0")
  def unboundedFollowing(): Column = Column(UnboundedFollowing)

  /**
   * This function has been deprecated in Spark 2.4. See SPARK-25842 for more information.
   *
   * @group window_funcs
   * @since 2.3.0
   */
  @deprecated("Use Window.currentRow", "2.4.0")
  def currentRow(): Column = Column(CurrentRow)

  /**
   * Window function: returns the cumulative distribution of values within a window partition,
   * i.e. the fraction of rows that are below the current row.
   *
   * {{{
   *   N = total number of rows in the partition
   *   cumeDist(x) = number of values before (and including) x / N
   * }}}
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def cume_dist(): Column = withExpr { new CumeDist }

  /**
   * Window function: returns the rank of rows within a window partition, without any gaps.
   *
   * The difference between rank and dense_rank is that denseRank leaves no gaps in ranking
   * sequence when there are ties. That is, if you were ranking a competition using dense_rank
   * and had three people tie for second place, you would say that all three were in second
   * place and that the next person came in third. Rank would give me sequential numbers, making
   * the person that came in third place (after the ties) would register as coming in fifth.
   *
   * This is equivalent to the DENSE_RANK function in SQL.
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def dense_rank(): Column = withExpr { new DenseRank }

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `null` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(e: Column, offset: Int): Column = lag(e, offset, null)

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `null` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(columnName: String, offset: Int): Column = lag(columnName, offset, null)

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `defaultValue` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(columnName: String, offset: Int, defaultValue: Any): Column = {
    lag(Column(columnName), offset, defaultValue)
  }

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `defaultValue` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(e: Column, offset: Int, defaultValue: Any): Column = withExpr {
    Lag(e.expr, Literal(offset), Literal(defaultValue))
  }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `null` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(columnName: String, offset: Int): Column = { lead(columnName, offset, null) }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `null` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(e: Column, offset: Int): Column = { lead(e, offset, null) }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `defaultValue` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(columnName: String, offset: Int, defaultValue: Any): Column = {
    lead(Column(columnName), offset, defaultValue)
  }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `defaultValue` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(e: Column, offset: Int, defaultValue: Any): Column = withExpr {
    Lead(e.expr, Literal(offset), Literal(defaultValue))
  }

  /**
   * Window function: returns the ntile group id (from 1 to `n` inclusive) in an ordered window
   * partition. For example, if `n` is 4, the first quarter of the rows will get value 1, the second
   * quarter will get 2, the third quarter will get 3, and the last quarter will get 4.
   *
   * This is equivalent to the NTILE function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def ntile(n: Int): Column = withExpr { new NTile(Literal(n)) }

  /**
   * Window function: returns the relative rank (i.e. percentile) of rows within a window partition.
   *
   * This is computed by:
   * {{{
   *   (rank of row in its partition - 1) / (number of rows in the partition - 1)
   * }}}
   *
   * This is equivalent to the PERCENT_RANK function in SQL.
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def percent_rank(): Column = withExpr { new PercentRank }

  /**
   * Window function: returns the rank of rows within a window partition.
   *
   * The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking
   * sequence when there are ties. That is, if you were ranking a competition using dense_rank
   * and had three people tie for second place, you would say that all three were in second
   * place and that the next person came in third. Rank would give me sequential numbers, making
   * the person that came in third place (after the ties) would register as coming in fifth.
   *
   * This is equivalent to the RANK function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def rank(): Column = withExpr { new Rank }

  /**
   * Window function: returns a sequential number starting at 1 within a window partition.
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def row_number(): Column = withExpr { RowNumber() }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Non-aggregate functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Creates a new array column. The input columns must all have the same data type.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def array(cols: Column*): Column = withExpr { CreateArray(cols.map(_.expr)) }

  /**
   * Creates a new array column. The input columns must all have the same data type.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def array(colName: String, colNames: String*): Column = {
    array((colName +: colNames).map(col) : _*)
  }

  /**
   * Creates a new map column. The input columns must be grouped as key-value pairs, e.g.
   * (key1, value1, key2, value2, ...). The key columns must all have the same data type, and can't
   * be null. The value columns must all have the same data type.
   *
   * @group normal_funcs
   * @since 2.0
   */
  @scala.annotation.varargs
  def map(cols: Column*): Column = withExpr { CreateMap(cols.map(_.expr)) }

  /**
   * Creates a new map column. The array in the first column is used for keys. The array in the
   * second column is used for values. All elements in the array for key should not be null.
   *
   * @group normal_funcs
   * @since 2.4
   */
  def map_from_arrays(keys: Column, values: Column): Column = withExpr {
    MapFromArrays(keys.expr, values.expr)
  }

  /**
   * Marks a DataFrame as small enough for use in broadcast joins.
   *
   * The following example marks the right DataFrame for broadcast hash join using `joinKey`.
   * {{{
   *   // left and right are DataFrames
   *   left.join(broadcast(right), "joinKey")
   * }}}
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  def broadcast[T](df: Dataset[T]): Dataset[T] = {
    Dataset[T](df.sparkSession,
      ResolvedHint(df.logicalPlan, HintInfo(broadcast = true)))(df.exprEnc)
  }

  /**
   * Returns the first column that is not null, or null if all inputs are null.
   *
   * For example, `coalesce(a, b, c)` will return a if a is not null,
   * or b if a is null and b is not null, or c if both a and b are null but c is not null.
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  @scala.annotation.varargs
  def coalesce(e: Column*): Column = withExpr { Coalesce(e.map(_.expr)) }

  /**
   * Creates a string column for the file name of the current Spark task.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def input_file_name(): Column = withExpr { InputFileName() }

  /**
   * Return true iff the column is NaN.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def isnan(e: Column): Column = withExpr { IsNaN(e.expr) }

  /**
   * Return true iff the column is null.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def isnull(e: Column): Column = withExpr { IsNull(e.expr) }

  /**
   * A column expression that generates monotonically increasing 64-bit integers.
   *
   * The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.
   * The current implementation puts the partition ID in the upper 31 bits, and the record number
   * within each partition in the lower 33 bits. The assumption is that the data frame has
   * less than 1 billion partitions, and each partition has less than 8 billion records.
   *
   * As an example, consider a `DataFrame` with two partitions, each with 3 records.
   * This expression would return the following IDs:
   *
   * {{{
   * 0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.
   * }}}
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @deprecated("Use monotonically_increasing_id()", "2.0.0")
  def monotonicallyIncreasingId(): Column = monotonically_increasing_id()

  /**
   * A column expression that generates monotonically increasing 64-bit integers.
   *
   * The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.
   * The current implementation puts the partition ID in the upper 31 bits, and the record number
   * within each partition in the lower 33 bits. The assumption is that the data frame has
   * less than 1 billion partitions, and each partition has less than 8 billion records.
   *
   * As an example, consider a `DataFrame` with two partitions, each with 3 records.
   * This expression would return the following IDs:
   *
   * {{{
   * 0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.
   * }}}
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def monotonically_increasing_id(): Column = withExpr { MonotonicallyIncreasingID() }

  /**
   * Returns col1 if it is not NaN, or col2 if col1 is NaN.
   *
   * Both inputs should be floating point columns (DoubleType or FloatType).
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  def nanvl(col1: Column, col2: Column): Column = withExpr { NaNvl(col1.expr, col2.expr) }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Select the amount column and negates all values.
   *   // Scala:
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   df.select( negate(df.col("amount")) );
   * }}}
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def negate(e: Column): Column = -e

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def not(e: Column): Column = !e

  /**
   * Generate a random column with independent and identically distributed (i.i.d.) samples
   * uniformly distributed in [0.0, 1.0).
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def rand(seed: Long): Column = withExpr { Rand(seed) }

  /**
   * Generate a random column with independent and identically distributed (i.i.d.) samples
   * uniformly distributed in [0.0, 1.0).
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def rand(): Column = rand(Utils.random.nextLong)

  /**
   * Generate a column with independent and identically distributed (i.i.d.) samples from
   * the standard normal distribution.
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def randn(seed: Long): Column = withExpr { Randn(seed) }

  /**
   * Generate a column with independent and identically distributed (i.i.d.) samples from
   * the standard normal distribution.
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def randn(): Column = randn(Utils.random.nextLong)

  /**
   * Partition ID.
   *
   * @note This is non-deterministic because it depends on data partitioning and task scheduling.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def spark_partition_id(): Column = withExpr { SparkPartitionID() }

  /**
   * Computes the square root of the specified float value.
   *
   * @group math_funcs
   * @since 1.3.0
   */
  def sqrt(e: Column): Column = withExpr { Sqrt(e.expr) }

  /**
   * Computes the square root of the specified float value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def sqrt(colName: String): Column = sqrt(Column(colName))

  /**
   * Creates a new struct column.
   * If the input column is a column in a `DataFrame`, or a derived column expression
   * that is named (i.e. aliased), its name would be retained as the StructField's name,
   * otherwise, the newly generated StructField's name would be auto generated as
   * `col` with a suffix `index + 1`, i.e. col1, col2, col3, ...
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def struct(cols: Column*): Column = withExpr { CreateStruct(cols.map(_.expr)) }

  /**
   * Creates a new struct column that composes multiple input columns.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def struct(colName: String, colNames: String*): Column = {
    struct((colName +: colNames).map(col) : _*)
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = withExpr {
    CaseWhen(Seq((condition.expr, lit(value).expr)))
  }

  /**
   * Computes bitwise NOT (~) of a number.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def bitwiseNOT(e: Column): Column = withExpr { BitwiseNot(e.expr) }

  /**
   * Parses the expression string into the column that it represents, similar to
   * [[Dataset#selectExpr]].
   * {{{
   *   // get the number of words of each length
   *   df.groupBy(expr("length(word)")).count()
   * }}}
   *
   * @group normal_funcs
   */
  def expr(expr: String): Column = {
    val parser = SparkSession.getActiveSession.map(_.sessionState.sqlParser).getOrElse {
      new SparkSqlParser(new SQLConf)
    }
    Column(parser.parseExpression(expr))
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Math Functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Computes the absolute value of a numeric value.
   *
   * @group math_funcs
   * @since 1.3.0
   */
  def abs(e: Column): Column = withExpr { Abs(e.expr) }

  /**
   * @return inverse cosine of `e` in radians, as if computed by `java.lang.Math.acos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def acos(e: Column): Column = withExpr { Acos(e.expr) }

  /**
   * @return inverse cosine of `columnName`, as if computed by `java.lang.Math.acos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def acos(columnName: String): Column = acos(Column(columnName))

  /**
   * @return inverse sine of `e` in radians, as if computed by `java.lang.Math.asin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def asin(e: Column): Column = withExpr { Asin(e.expr) }

  /**
   * @return inverse sine of `columnName`, as if computed by `java.lang.Math.asin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def asin(columnName: String): Column = asin(Column(columnName))

  /**
   * @return inverse tangent of `e`, as if computed by `java.lang.Math.atan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan(e: Column): Column = withExpr { Atan(e.expr) }

  /**
   * @return inverse tangent of `columnName`, as if computed by `java.lang.Math.atan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan(columnName: String): Column = atan(Column(columnName))

  /**
   * @param y coordinate on y-axis
   * @param x coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(y: Column, x: Column): Column = withExpr { Atan2(y.expr, x.expr) }

  /**
   * @param y coordinate on y-axis
   * @param xName coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(y: Column, xName: String): Column = atan2(y, Column(xName))

  /**
   * @param yName coordinate on y-axis
   * @param x coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yName: String, x: Column): Column = atan2(Column(yName), x)

  /**
   * @param yName coordinate on y-axis
   * @param xName coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yName: String, xName: String): Column =
    atan2(Column(yName), Column(xName))

  /**
   * @param y coordinate on y-axis
   * @param xValue coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(y: Column, xValue: Double): Column = atan2(y, lit(xValue))

  /**
   * @param yName coordinate on y-axis
   * @param xValue coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yName: String, xValue: Double): Column = atan2(Column(yName), xValue)

  /**
   * @param yValue coordinate on y-axis
   * @param x coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yValue: Double, x: Column): Column = atan2(lit(yValue), x)

  /**
   * @param yValue coordinate on y-axis
   * @param xName coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yValue: Double, xName: String): Column = atan2(yValue, Column(xName))

  /**
   * An expression that returns the string representation of the binary value of the given long
   * column. For example, bin("12") returns "1100".
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def bin(e: Column): Column = withExpr { Bin(e.expr) }

  /**
   * An expression that returns the string representation of the binary value of the given long
   * column. For example, bin("12") returns "1100".
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def bin(columnName: String): Column = bin(Column(columnName))

  /**
   * Computes the cube-root of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cbrt(e: Column): Column = withExpr { Cbrt(e.expr) }

  /**
   * Computes the cube-root of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cbrt(columnName: String): Column = cbrt(Column(columnName))

  /**
   * Computes the ceiling of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def ceil(e: Column): Column = withExpr { Ceil(e.expr) }

  /**
   * Computes the ceiling of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def ceil(columnName: String): Column = ceil(Column(columnName))

  /**
   * Convert a number in a string column from one base to another.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def conv(num: Column, fromBase: Int, toBase: Int): Column = withExpr {
    Conv(num.expr, lit(fromBase).expr, lit(toBase).expr)
  }

  /**
   * @param e angle in radians
   * @return cosine of the angle, as if computed by `java.lang.Math.cos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cos(e: Column): Column = withExpr { Cos(e.expr) }

  /**
   * @param columnName angle in radians
   * @return cosine of the angle, as if computed by `java.lang.Math.cos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cos(columnName: String): Column = cos(Column(columnName))

  /**
   * @param e hyperbolic angle
   * @return hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cosh(e: Column): Column = withExpr { Cosh(e.expr) }

  /**
   * @param columnName hyperbolic angle
   * @return hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cosh(columnName: String): Column = cosh(Column(columnName))

  /**
   * Computes the exponential of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def exp(e: Column): Column = withExpr { Exp(e.expr) }

  /**
   * Computes the exponential of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def exp(columnName: String): Column = exp(Column(columnName))

  /**
   * Computes the exponential of the given value minus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def expm1(e: Column): Column = withExpr { Expm1(e.expr) }

  /**
   * Computes the exponential of the given column minus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def expm1(columnName: String): Column = expm1(Column(columnName))

  /**
   * Computes the factorial of the given value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def factorial(e: Column): Column = withExpr { Factorial(e.expr) }

  /**
   * Computes the floor of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def floor(e: Column): Column = withExpr { Floor(e.expr) }

  /**
   * Computes the floor of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def floor(columnName: String): Column = floor(Column(columnName))

  /**
   * Returns the greatest value of the list of values, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def greatest(exprs: Column*): Column = withExpr { Greatest(exprs.map(_.expr)) }

  /**
   * Returns the greatest value of the list of column names, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def greatest(columnName: String, columnNames: String*): Column = {
    greatest((columnName +: columnNames).map(Column.apply): _*)
  }

  /**
   * Computes hex value of the given column.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def hex(column: Column): Column = withExpr { Hex(column.expr) }

  /**
   * Inverse of hex. Interprets each pair of characters as a hexadecimal number
   * and converts to the byte representation of number.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def unhex(column: Column): Column = withExpr { Unhex(column.expr) }

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Column, r: Column): Column = withExpr { Hypot(l.expr, r.expr) }

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Column, rightName: String): Column = hypot(l, Column(rightName))

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(leftName: String, r: Column): Column = hypot(Column(leftName), r)

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(leftName: String, rightName: String): Column =
    hypot(Column(leftName), Column(rightName))

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Column, r: Double): Column = hypot(l, lit(r))

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(leftName: String, r: Double): Column = hypot(Column(leftName), r)

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Double, r: Column): Column = hypot(lit(l), r)

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Double, rightName: String): Column = hypot(l, Column(rightName))

  /**
   * Returns the least value of the list of values, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def least(exprs: Column*): Column = withExpr { Least(exprs.map(_.expr)) }

  /**
   * Returns the least value of the list of column names, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def least(columnName: String, columnNames: String*): Column = {
    least((columnName +: columnNames).map(Column.apply): _*)
  }

  /**
   * Computes the natural logarithm of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(e: Column): Column = withExpr { Log(e.expr) }

  /**
   * Computes the natural logarithm of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(columnName: String): Column = log(Column(columnName))

  /**
   * Returns the first argument-base logarithm of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(base: Double, a: Column): Column = withExpr { Logarithm(lit(base).expr, a.expr) }

  /**
   * Returns the first argument-base logarithm of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(base: Double, columnName: String): Column = log(base, Column(columnName))

  /**
   * Computes the logarithm of the given value in base 10.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log10(e: Column): Column = withExpr { Log10(e.expr) }

  /**
   * Computes the logarithm of the given value in base 10.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log10(columnName: String): Column = log10(Column(columnName))

  /**
   * Computes the natural logarithm of the given value plus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log1p(e: Column): Column = withExpr { Log1p(e.expr) }

  /**
   * Computes the natural logarithm of the given column plus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log1p(columnName: String): Column = log1p(Column(columnName))

  /**
   * Computes the logarithm of the given column in base 2.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def log2(expr: Column): Column = withExpr { Log2(expr.expr) }

  /**
   * Computes the logarithm of the given value in base 2.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def log2(columnName: String): Column = log2(Column(columnName))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Column, r: Column): Column = withExpr { Pow(l.expr, r.expr) }

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Column, rightName: String): Column = pow(l, Column(rightName))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(leftName: String, r: Column): Column = pow(Column(leftName), r)

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(leftName: String, rightName: String): Column = pow(Column(leftName), Column(rightName))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Column, r: Double): Column = pow(l, lit(r))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(leftName: String, r: Double): Column = pow(Column(leftName), r)

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Double, r: Column): Column = pow(lit(l), r)

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Double, rightName: String): Column = pow(l, Column(rightName))

  /**
   * Returns the positive value of dividend mod divisor.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def pmod(dividend: Column, divisor: Column): Column = withExpr {
    Pmod(dividend.expr, divisor.expr)
  }

  /**
   * Returns the double value that is closest in value to the argument and
   * is equal to a mathematical integer.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def rint(e: Column): Column = withExpr { Rint(e.expr) }

  /**
   * Returns the double value that is closest in value to the argument and
   * is equal to a mathematical integer.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def rint(columnName: String): Column = rint(Column(columnName))

  /**
   * Returns the value of the column `e` rounded to 0 decimal places with HALF_UP round mode.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def round(e: Column): Column = round(e, 0)

  /**
   * Round the value of `e` to `scale` decimal places with HALF_UP round mode
   * if `scale` is greater than or equal to 0 or at integral part when `scale` is less than 0.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def round(e: Column, scale: Int): Column = withExpr { Round(e.expr, Literal(scale)) }

  /**
   * Returns the value of the column `e` rounded to 0 decimal places with HALF_EVEN round mode.
   *
   * @group math_funcs
   * @since 2.0.0
   */
  def bround(e: Column): Column = bround(e, 0)

  /**
   * Round the value of `e` to `scale` decimal places with HALF_EVEN round mode
   * if `scale` is greater than or equal to 0 or at integral part when `scale` is less than 0.
   *
   * @group math_funcs
   * @since 2.0.0
   */
  def bround(e: Column, scale: Int): Column = withExpr { BRound(e.expr, Literal(scale)) }

  /**
   * Shift the given value numBits left. If the given value is a long value, this function
   * will return a long value else it will return an integer value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def shiftLeft(e: Column, numBits: Int): Column = withExpr { ShiftLeft(e.expr, lit(numBits).expr) }

  /**
   * (Signed) shift the given value numBits right. If the given value is a long value, it will
   * return a long value else it will return an integer value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def shiftRight(e: Column, numBits: Int): Column = withExpr {
    ShiftRight(e.expr, lit(numBits).expr)
  }

  /**
   * Unsigned shift the given value numBits right. If the given value is a long value,
   * it will return a long value else it will return an integer value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def shiftRightUnsigned(e: Column, numBits: Int): Column = withExpr {
    ShiftRightUnsigned(e.expr, lit(numBits).expr)
  }

  /**
   * Computes the signum of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def signum(e: Column): Column = withExpr { Signum(e.expr) }

  /**
   * Computes the signum of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def signum(columnName: String): Column = signum(Column(columnName))

  /**
   * @param e angle in radians
   * @return sine of the angle, as if computed by `java.lang.Math.sin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sin(e: Column): Column = withExpr { Sin(e.expr) }

  /**
   * @param columnName angle in radians
   * @return sine of the angle, as if computed by `java.lang.Math.sin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sin(columnName: String): Column = sin(Column(columnName))

  /**
   * @param e hyperbolic angle
   * @return hyperbolic sine of the given value, as if computed by `java.lang.Math.sinh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sinh(e: Column): Column = withExpr { Sinh(e.expr) }

  /**
   * @param columnName hyperbolic angle
   * @return hyperbolic sine of the given value, as if computed by `java.lang.Math.sinh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sinh(columnName: String): Column = sinh(Column(columnName))

  /**
   * @param e angle in radians
   * @return tangent of the given value, as if computed by `java.lang.Math.tan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tan(e: Column): Column = withExpr { Tan(e.expr) }

  /**
   * @param columnName angle in radians
   * @return tangent of the given value, as if computed by `java.lang.Math.tan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tan(columnName: String): Column = tan(Column(columnName))

  /**
   * @param e hyperbolic angle
   * @return hyperbolic tangent of the given value, as if computed by `java.lang.Math.tanh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tanh(e: Column): Column = withExpr { Tanh(e.expr) }

  /**
   * @param columnName hyperbolic angle
   * @return hyperbolic tangent of the given value, as if computed by `java.lang.Math.tanh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tanh(columnName: String): Column = tanh(Column(columnName))

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use degrees", "2.1.0")
  def toDegrees(e: Column): Column = degrees(e)

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use degrees", "2.1.0")
  def toDegrees(columnName: String): Column = degrees(Column(columnName))

  /**
   * Converts an angle measured in radians to an approximately equivalent angle measured in degrees.
   *
   * @param e angle in radians
   * @return angle in degrees, as if computed by `java.lang.Math.toDegrees`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def degrees(e: Column): Column = withExpr { ToDegrees(e.expr) }

  /**
   * Converts an angle measured in radians to an approximately equivalent angle measured in degrees.
   *
   * @param columnName angle in radians
   * @return angle in degrees, as if computed by `java.lang.Math.toDegrees`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def degrees(columnName: String): Column = degrees(Column(columnName))

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use radians", "2.1.0")
  def toRadians(e: Column): Column = radians(e)

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use radians", "2.1.0")
  def toRadians(columnName: String): Column = radians(Column(columnName))

  /**
   * Converts an angle measured in degrees to an approximately equivalent angle measured in radians.
   *
   * @param e angle in degrees
   * @return angle in radians, as if computed by `java.lang.Math.toRadians`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def radians(e: Column): Column = withExpr { ToRadians(e.expr) }

  /**
   * Converts an angle measured in degrees to an approximately equivalent angle measured in radians.
   *
   * @param columnName angle in degrees
   * @return angle in radians, as if computed by `java.lang.Math.toRadians`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def radians(columnName: String): Column = radians(Column(columnName))

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Misc functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Calculates the MD5 digest of a binary column and returns the value
   * as a 32 character hex string.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def md5(e: Column): Column = withExpr { Md5(e.expr) }

  /**
   * Calculates the SHA-1 digest of a binary column and returns the value
   * as a 40 character hex string.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def sha1(e: Column): Column = withExpr { Sha1(e.expr) }

  /**
   * Calculates the SHA-2 family of hash functions of a binary column and
   * returns the value as a hex string.
   *
   * @param e column to compute SHA-2 on.
   * @param numBits one of 224, 256, 384, or 512.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def sha2(e: Column, numBits: Int): Column = {
    require(Seq(0, 224, 256, 384, 512).contains(numBits),
      s"numBits $numBits is not in the permitted values (0, 224, 256, 384, 512)")
    withExpr { Sha2(e.expr, lit(numBits).expr) }
  }

  /**
   * Calculates the cyclic redundancy check value  (CRC32) of a binary column and
   * returns the value as a bigint.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def crc32(e: Column): Column = withExpr { Crc32(e.expr) }

  /**
   * Calculates the hash code of given columns, and returns the result as an int column.
   *
   * @group misc_funcs
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def hash(cols: Column*): Column = withExpr {
    new Murmur3Hash(cols.map(_.expr))
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // String functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Computes the numeric value of the first character of the string column, and returns the
   * result as an int column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def ascii(e: Column): Column = withExpr { Ascii(e.expr) }

  /**
   * Computes the BASE64 encoding of a binary column and returns it as a string column.
   * This is the reverse of unbase64.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def base64(e: Column): Column = withExpr { Base64(e.expr) }

  /**
   * Concatenates multiple input string columns together into a single string column,
   * using the given separator.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def concat_ws(sep: String, exprs: Column*): Column = withExpr {
    ConcatWs(Literal.create(sep, StringType) +: exprs.map(_.expr))
  }

  /**
   * Computes the first argument into a string from a binary using the provided character set
   * (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').
   * If either argument is null, the result will also be null.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def decode(value: Column, charset: String): Column = withExpr {
    Decode(value.expr, lit(charset).expr)
  }

  /**
   * Computes the first argument into a binary from a string using the provided character set
   * (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').
   * If either argument is null, the result will also be null.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def encode(value: Column, charset: String): Column = withExpr {
    Encode(value.expr, lit(charset).expr)
  }

  /**
   * Formats numeric column x to a format like '#,###,###.##', rounded to d decimal places
   * with HALF_EVEN round mode, and returns the result as a string column.
   *
   * If d is 0, the result has no decimal point or fractional part.
   * If d is less than 0, the result will be null.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def format_number(x: Column, d: Int): Column = withExpr {
    FormatNumber(x.expr, lit(d).expr)
  }

  /**
   * Formats the arguments in printf-style and returns the result as a string column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def format_string(format: String, arguments: Column*): Column = withExpr {
    FormatString((lit(format) +: arguments).map(_.expr): _*)
  }

  /**
   * Returns a new string column by converting the first letter of each word to uppercase.
   * Words are delimited by whitespace.
   *
   * For example, "hello world" will become "Hello World".
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def initcap(e: Column): Column = withExpr { InitCap(e.expr) }

  /**
   * Locate the position of the first occurrence of substr column in the given string.
   * Returns null if either of the arguments are null.
   *
   * @note The position is not zero based, but 1 based index. Returns 0 if substr
   * could not be found in str.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def instr(str: Column, substring: String): Column = withExpr {
    StringInstr(str.expr, lit(substring).expr)
  }

  /**
   * Computes the character length of a given string or number of bytes of a binary string.
   * The length of character strings include the trailing spaces. The length of binary strings
   * includes binary zeros.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def length(e: Column): Column = withExpr { Length(e.expr) }

  /**
   * Converts a string column to lower case.
   *
   * @group string_funcs
   * @since 1.3.0
   */
  def lower(e: Column): Column = withExpr { Lower(e.expr) }

  /**
   * Computes the Levenshtein distance of the two given string columns.
   * @group string_funcs
   * @since 1.5.0
   */
  def levenshtein(l: Column, r: Column): Column = withExpr { Levenshtein(l.expr, r.expr) }

  /**
   * Locate the position of the first occurrence of substr.
   *
   * @note The position is not zero based, but 1 based index. Returns 0 if substr
   * could not be found in str.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def locate(substr: String, str: Column): Column = withExpr {
    new StringLocate(lit(substr).expr, str.expr)
  }

  /**
   * Locate the position of the first occurrence of substr in a string column, after position pos.
   *
   * @note The position is not zero based, but 1 based index. returns 0 if substr
   * could not be found in str.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def locate(substr: String, str: Column, pos: Int): Column = withExpr {
    StringLocate(lit(substr).expr, str.expr, lit(pos).expr)
  }

  /**
   * Left-pad the string column with pad to a length of len. If the string column is longer
   * than len, the return value is shortened to len characters.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def lpad(str: Column, len: Int, pad: String): Column = withExpr {
    StringLPad(str.expr, lit(len).expr, lit(pad).expr)
  }

  /**
   * Trim the spaces from left end for the specified string value.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def ltrim(e: Column): Column = withExpr {StringTrimLeft(e.expr) }

  /**
   * Trim the specified character string from left end for the specified string column.
   * @group string_funcs
   * @since 2.3.0
   */
  def ltrim(e: Column, trimString: String): Column = withExpr {
    StringTrimLeft(e.expr, Literal(trimString))
  }

  /**
   * Extract a specific group matched by a Java regex, from the specified string column.
   * If the regex did not match, or the specified group did not match, an empty string is returned.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def regexp_extract(e: Column, exp: String, groupIdx: Int): Column = withExpr {
    RegExpExtract(e.expr, lit(exp).expr, lit(groupIdx).expr)
  }

  /**
   * Replace all substrings of the specified string value that match regexp with rep.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def regexp_replace(e: Column, pattern: String, replacement: String): Column = withExpr {
    RegExpReplace(e.expr, lit(pattern).expr, lit(replacement).expr)
  }

  /**
   * Replace all substrings of the specified string value that match regexp with rep.
   *
   * @group string_funcs
   * @since 2.1.0
   */
  def regexp_replace(e: Column, pattern: Column, replacement: Column): Column = withExpr {
    RegExpReplace(e.expr, pattern.expr, replacement.expr)
  }

  /**
   * Decodes a BASE64 encoded string column and returns it as a binary column.
   * This is the reverse of base64.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def unbase64(e: Column): Column = withExpr { UnBase64(e.expr) }

  /**
   * Right-pad the string column with pad to a length of len. If the string column is longer
   * than len, the return value is shortened to len characters.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def rpad(str: Column, len: Int, pad: String): Column = withExpr {
    StringRPad(str.expr, lit(len).expr, lit(pad).expr)
  }

  /**
   * Repeats a string column n times, and returns it as a new string column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def repeat(str: Column, n: Int): Column = withExpr {
    StringRepeat(str.expr, lit(n).expr)
  }

  /**
   * Trim the spaces from right end for the specified string value.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def rtrim(e: Column): Column = withExpr { StringTrimRight(e.expr) }

  /**
   * Trim the specified character string from right end for the specified string column.
   * @group string_funcs
   * @since 2.3.0
   */
  def rtrim(e: Column, trimString: String): Column = withExpr {
    StringTrimRight(e.expr, Literal(trimString))
  }

  /**
   * Returns the soundex code for the specified expression.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def soundex(e: Column): Column = withExpr { SoundEx(e.expr) }

  /**
   * Splits str around pattern (pattern is a regular expression).
   *
   * @note Pattern is a string representation of the regular expression.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def split(str: Column, pattern: String): Column = withExpr {
    StringSplit(str.expr, lit(pattern).expr)
  }

  /**
   * Substring starts at `pos` and is of length `len` when str is String type or
   * returns the slice of byte array that starts at `pos` in byte and is of length `len`
   * when str is Binary type
   *
   * @note The position is not zero based, but 1 based index.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def substring(str: Column, pos: Int, len: Int): Column = withExpr {
    Substring(str.expr, lit(pos).expr, lit(len).expr)
  }

  /**
   * Returns the substring from string str before count occurrences of the delimiter delim.
   * If count is positive, everything the left of the final delimiter (counting from left) is
   * returned. If count is negative, every to the right of the final delimiter (counting from the
   * right) is returned. substring_index performs a case-sensitive match when searching for delim.
   *
   * @group string_funcs
   */
  def substring_index(str: Column, delim: String, count: Int): Column = withExpr {
    SubstringIndex(str.expr, lit(delim).expr, lit(count).expr)
  }

  /**
   * Translate any character in the src by a character in replaceString.
   * The characters in replaceString correspond to the characters in matchingString.
   * The translate will happen when any character in the string matches the character
   * in the `matchingString`.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def translate(src: Column, matchingString: String, replaceString: String): Column = withExpr {
    StringTranslate(src.expr, lit(matchingString).expr, lit(replaceString).expr)
  }

  /**
   * Trim the spaces from both ends for the specified string column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def trim(e: Column): Column = withExpr { StringTrim(e.expr) }

  /**
   * Trim the specified character from both ends for the specified string column.
   * @group string_funcs
   * @since 2.3.0
   */
  def trim(e: Column, trimString: String): Column = withExpr {
    StringTrim(e.expr, Literal(trimString))
  }

  /**
   * Converts a string column to upper case.
   *
   * @group string_funcs
   * @since 1.3.0
   */
  def upper(e: Column): Column = withExpr { Upper(e.expr) }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // DateTime functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Returns the date that is `numMonths` after `startDate`.
   *
   * @param startDate A date, timestamp or string. If a string, the data must be in a format that
   *                  can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param numMonths The number of months to add to `startDate`, can be negative to subtract months
   * @return A date, or null if `startDate` was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def add_months(startDate: Column, numMonths: Int): Column = withExpr {
    AddMonths(startDate.expr, Literal(numMonths))
  }

  /**
   * Returns the current date as a date column.
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def current_date(): Column = withExpr { CurrentDate() }

  /**
   * Returns the current timestamp as a timestamp column.
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def current_timestamp(): Column = withExpr { CurrentTimestamp() }

  /**
   * Converts a date/timestamp/string to a value of string in the format specified by the date
   * format given by the second argument.
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param dateExpr A date, timestamp or string. If a string, the data must be in a format that
   *                 can be cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param format A pattern `dd.MM.yyyy` would return a string like `18.03.1993`
   * @return A string, or null if `dateExpr` was a string that could not be cast to a timestamp
   * @note Use specialized functions like [[year]] whenever possible as they benefit from a
   * specialized implementation.
   * @throws IllegalArgumentException if the `format` pattern is invalid
   * @group datetime_funcs
   * @since 1.5.0
   */
  def date_format(dateExpr: Column, format: String): Column = withExpr {
    DateFormatClass(dateExpr.expr, Literal(format))
  }

  /**
   * Returns the date that is `days` days after `start`
   *
   * @param start A date, timestamp or string. If a string, the data must be in a format that
   *              can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param days  The number of days to add to `start`, can be negative to subtract days
   * @return A date, or null if `start` was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def date_add(start: Column, days: Int): Column = withExpr { DateAdd(start.expr, Literal(days)) }

  /**
   * Returns the date that is `days` days before `start`
   *
   * @param start A date, timestamp or string. If a string, the data must be in a format that
   *              can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param days  The number of days to subtract from `start`, can be negative to add days
   * @return A date, or null if `start` was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def date_sub(start: Column, days: Int): Column = withExpr { DateSub(start.expr, Literal(days)) }

  /**
   * Returns the number of days from `start` to `end`.
   *
   * Only considers the date part of the input. For example:
   * {{{
   * dateddiff("2018-01-10 00:00:00", "2018-01-09 23:59:59")
   * // returns 1
   * }}}
   *
   * @param end A date, timestamp or string. If a string, the data must be in a format that
   *            can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param start A date, timestamp or string. If a string, the data must be in a format that
   *              can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return An integer, or null if either `end` or `start` were strings that could not be cast to
   *         a date. Negative if `end` is before `start`
   * @group datetime_funcs
   * @since 1.5.0
   */
  def datediff(end: Column, start: Column): Column = withExpr { DateDiff(end.expr, start.expr) }

  /**
   * Extracts the year as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def year(e: Column): Column = withExpr { Year(e.expr) }

  /**
   * Extracts the quarter as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def quarter(e: Column): Column = withExpr { Quarter(e.expr) }

  /**
   * Extracts the month as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def month(e: Column): Column = withExpr { Month(e.expr) }

  /**
   * Extracts the day of the week as an integer from a given date/timestamp/string.
   * Ranges from 1 for a Sunday through to 7 for a Saturday
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 2.3.0
   */
  def dayofweek(e: Column): Column = withExpr { DayOfWeek(e.expr) }

  /**
   * Extracts the day of the month as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def dayofmonth(e: Column): Column = withExpr { DayOfMonth(e.expr) }

  /**
   * Extracts the day of the year as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def dayofyear(e: Column): Column = withExpr { DayOfYear(e.expr) }

  /**
   * Extracts the hours as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def hour(e: Column): Column = withExpr { Hour(e.expr) }

  /**
   * Returns the last day of the month which the given date belongs to.
   * For example, input "2015-07-27" returns "2015-07-31" since July 31 is the last day of the
   * month in July 2015.
   *
   * @param e A date, timestamp or string. If a string, the data must be in a format that can be
   *          cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A date, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def last_day(e: Column): Column = withExpr { LastDay(e.expr) }

  /**
   * Extracts the minutes as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def minute(e: Column): Column = withExpr { Minute(e.expr) }

  /**
   * Returns number of months between dates `start` and `end`.
   *
   * A whole number is returned if both inputs have the same day of month or both are the last day
   * of their respective months. Otherwise, the difference is calculated assuming 31 days per month.
   *
   * For example:
   * {{{
   * months_between("2017-11-14", "2017-07-14")  // returns 4.0
   * months_between("2017-01-01", "2017-01-10")  // returns 0.29032258
   * months_between("2017-06-01", "2017-06-16 12:00:00")  // returns -0.5
   * }}}
   *
   * @param end   A date, timestamp or string. If a string, the data must be in a format that can
   *              be cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param start A date, timestamp or string. If a string, the data must be in a format that can
   *              cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A double, or null if either `end` or `start` were strings that could not be cast to a
   *         timestamp. Negative if `end` is before `start`
   * @group datetime_funcs
   * @since 1.5.0
   */
  def months_between(end: Column, start: Column): Column = withExpr {
    new MonthsBetween(end.expr, start.expr)
  }

  /**
   * Returns number of months between dates `end` and `start`. If `roundOff` is set to true, the
   * result is rounded off to 8 digits; it is not rounded otherwise.
   * @group datetime_funcs
   * @since 2.4.0
   */
  def months_between(end: Column, start: Column, roundOff: Boolean): Column = withExpr {
    MonthsBetween(end.expr, start.expr, lit(roundOff).expr)
  }

  /**
   * Returns the first date which is later than the value of the `date` column that is on the
   * specified day of the week.
   *
   * For example, `next_day('2015-07-27', "Sunday")` returns 2015-08-02 because that is the first
   * Sunday after 2015-07-27.
   *
   * @param date      A date, timestamp or string. If a string, the data must be in a format that
   *                  can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param dayOfWeek Case insensitive, and accepts: "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"
   * @return A date, or null if `date` was a string that could not be cast to a date or if
   *         `dayOfWeek` was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def next_day(date: Column, dayOfWeek: String): Column = withExpr {
    NextDay(date.expr, lit(dayOfWeek).expr)
  }

  /**
   * Extracts the seconds as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a timestamp
   * @group datetime_funcs
   * @since 1.5.0
   */
  def second(e: Column): Column = withExpr { Second(e.expr) }

  /**
   * Extracts the week number as an integer from a given date/timestamp/string.
   *
   * A week is considered to start on a Monday and week 1 is the first week with more than 3 days,
   * as defined by ISO 8601
   *
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def weekofyear(e: Column): Column = withExpr { WeekOfYear(e.expr) }

  /**
   * Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string
   * representing the timestamp of that moment in the current system time zone in the
   * yyyy-MM-dd HH:mm:ss format.
   *
   * @param ut A number of a type that is castable to a long, such as string or integer. Can be
   *           negative for timestamps before the unix epoch
   * @return A string, or null if the input was a string that could not be cast to a long
   * @group datetime_funcs
   * @since 1.5.0
   */
  def from_unixtime(ut: Column): Column = withExpr {
    FromUnixTime(ut.expr, Literal("yyyy-MM-dd HH:mm:ss"))
  }

  /**
   * Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string
   * representing the timestamp of that moment in the current system time zone in the given
   * format.
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param ut A number of a type that is castable to a long, such as string or integer. Can be
   *           negative for timestamps before the unix epoch
   * @param f  A date time pattern that the input will be formatted to
   * @return A string, or null if `ut` was a string that could not be cast to a long or `f` was
   *         an invalid date time pattern
   * @group datetime_funcs
   * @since 1.5.0
   */
  def from_unixtime(ut: Column, f: String): Column = withExpr {
    FromUnixTime(ut.expr, Literal(f))
  }

  /**
   * Returns the current Unix timestamp (in seconds) as a long.
   *
   * @note All calls of `unix_timestamp` within the same query return the same value
   * (i.e. the current timestamp is calculated at the start of query evaluation).
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def unix_timestamp(): Column = withExpr {
    UnixTimestamp(CurrentTimestamp(), Literal("yyyy-MM-dd HH:mm:ss"))
  }

  /**
   * Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds),
   * using the default timezone and the default locale.
   *
   * @param s A date, timestamp or string. If a string, the data must be in the
   *          `yyyy-MM-dd HH:mm:ss` format
   * @return A long, or null if the input was a string not of the correct format
   * @group datetime_funcs
   * @since 1.5.0
   */
  def unix_timestamp(s: Column): Column = withExpr {
    UnixTimestamp(s.expr, Literal("yyyy-MM-dd HH:mm:ss"))
  }

  /**
   * Converts time string with given pattern to Unix timestamp (in seconds).
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param s A date, timestamp or string. If a string, the data must be in a format that can be
   *          cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param p A date time pattern detailing the format of `s` when `s` is a string
   * @return A long, or null if `s` was a string that could not be cast to a date or `p` was
   *         an invalid format
   * @group datetime_funcs
   * @since 1.5.0
   */
  def unix_timestamp(s: Column, p: String): Column = withExpr { UnixTimestamp(s.expr, Literal(p)) }

  /**
   * Converts to a timestamp by casting rules to `TimestampType`.
   *
   * @param s A date, timestamp or string. If a string, the data must be in a format that can be
   *          cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A timestamp, or null if the input was a string that could not be cast to a timestamp
   * @group datetime_funcs
   * @since 2.2.0
   */
  def to_timestamp(s: Column): Column = withExpr {
    new ParseToTimestamp(s.expr)
  }

  /**
   * Converts time string with the given pattern to timestamp.
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param s   A date, timestamp or string. If a string, the data must be in a format that can be
   *            cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss`
   * @param fmt A date time pattern detailing the format of `s` when `s` is a string
   * @return A timestamp, or null if `s` was a string that could not be cast to a timestamp or
   *         `fmt` was an invalid format
   * @group datetime_funcs
   * @since 2.2.0
   */
  def to_timestamp(s: Column, fmt: String): Column = withExpr {
    new ParseToTimestamp(s.expr, Literal(fmt))
  }

  /**
   * Converts the column into `DateType` by casting rules to `DateType`.
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def to_date(e: Column): Column = withExpr { new ParseToDate(e.expr) }

  /**
   * Converts the column into a `DateType` with a specified format
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param e   A date, timestamp or string. If a string, the data must be in a format that can be
   *            cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param fmt A date time pattern detailing the format of `e` when `e`is a string
   * @return A date, or null if `e` was a string that could not be cast to a date or `fmt` was an
   *         invalid format
   * @group datetime_funcs
   * @since 2.2.0
   */
  def to_date(e: Column, fmt: String): Column = withExpr {
    new ParseToDate(e.expr, Literal(fmt))
  }

  /**
   * Returns date truncated to the unit specified by the format.
   *
   * For example, `trunc("2018-11-19 12:01:19", "year")` returns 2018-01-01
   *
   * @param date A date, timestamp or string. If a string, the data must be in a format that can be
   *             cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param format: 'year', 'yyyy', 'yy' for truncate by year,
   *               or 'month', 'mon', 'mm' for truncate by month
   *
   * @return A date, or null if `date` was a string that could not be cast to a date or `format`
   *         was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def trunc(date: Column, format: String): Column = withExpr {
    TruncDate(date.expr, Literal(format))
  }

  /**
   * Returns timestamp truncated to the unit specified by the format.
   *
   * For example, `date_tunc("2018-11-19 12:01:19", "year")` returns 2018-01-01 00:00:00
   *
   * @param format: 'year', 'yyyy', 'yy' for truncate by year,
   *                'month', 'mon', 'mm' for truncate by month,
   *                'day', 'dd' for truncate by day,
   *                Other options are: 'second', 'minute', 'hour', 'week', 'month', 'quarter'
   * @param timestamp A date, timestamp or string. If a string, the data must be in a format that
   *                  can be cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A timestamp, or null if `timestamp` was a string that could not be cast to a timestamp
   *         or `format` was an invalid value
   * @group datetime_funcs
   * @since 2.3.0
   */
  def date_trunc(format: String, timestamp: Column): Column = withExpr {
    TruncTimestamp(Literal(format), timestamp.expr)
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders
   * that time as a timestamp in the given time zone. For example, 'GMT+1' would yield
   * '2017-07-14 03:40:00.0'.
   *
   * @param ts A date, timestamp or string. If a string, the data must be in a format that can be
   *           cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param tz A string detailing the time zone that the input should be adjusted to, such as
   *           `Europe/London`, `PST` or `GMT+5`
   * @return A timestamp, or null if `ts` was a string that could not be cast to a timestamp or
   *         `tz` was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def from_utc_timestamp(ts: Column, tz: String): Column = withExpr {
    FromUTCTimestamp(ts.expr, Literal(tz))
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders
   * that time as a timestamp in the given time zone. For example, 'GMT+1' would yield
   * '2017-07-14 03:40:00.0'.
   * @group datetime_funcs
   * @since 2.4.0
   */
  def from_utc_timestamp(ts: Column, tz: Column): Column = withExpr {
    FromUTCTimestamp(ts.expr, tz.expr)
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time
   * zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield
   * '2017-07-14 01:40:00.0'.
   *
   * @param ts A date, timestamp or string. If a string, the data must be in a format that can be
   *           cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param tz A string detailing the time zone that the input belongs to, such as `Europe/London`,
   *           `PST` or `GMT+5`
   * @return A timestamp, or null if `ts` was a string that could not be cast to a timestamp or
   *         `tz` was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def to_utc_timestamp(ts: Column, tz: String): Column = withExpr {
    ToUTCTimestamp(ts.expr, Literal(tz))
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time
   * zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield
   * '2017-07-14 01:40:00.0'.
   * @group datetime_funcs
   * @since 2.4.0
   */
  def to_utc_timestamp(ts: Column, tz: Column): Column = withExpr {
    ToUTCTimestamp(ts.expr, tz.expr)
  }

  /**
   * Bucketize rows into one or more time windows given a timestamp specifying column. Window
   * starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
   * [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
   * the order of months are not supported. The following example takes the average stock price for
   * a one minute window every 10 seconds starting 5 seconds after the hour:
   *
   * {{{
   *   val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType
   *   df.groupBy(window($"time", "1 minute", "10 seconds", "5 seconds"), $"stockId")
   *     .agg(mean("price"))
   * }}}
   *
   * The windows will look like:
   *
   * {{{
   *   09:00:05-09:01:05
   *   09:00:15-09:01:15
   *   09:00:25-09:01:25 ...
   * }}}
   *
   * For a streaming query, you may use the function `current_timestamp` to generate windows on
   * processing time.
   *
   * @param timeColumn The column or the expression to use as the timestamp for windowing by time.
   *                   The time column must be of TimestampType.
   * @param windowDuration A string specifying the width of the window, e.g. `10 minutes`,
   *                       `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for
   *                       valid duration identifiers. Note that the duration is a fixed length of
   *                       time, and does not vary over time according to a calendar. For example,
   *                       `1 day` always means 86,400,000 milliseconds, not a calendar day.
   * @param slideDuration A string specifying the sliding interval of the window, e.g. `1 minute`.
   *                      A new window will be generated every `slideDuration`. Must be less than
   *                      or equal to the `windowDuration`. Check
   *                      `org.apache.spark.unsafe.types.CalendarInterval` for valid duration
   *                      identifiers. This duration is likewise absolute, and does not vary
   *                      according to a calendar.
   * @param startTime The offset with respect to 1970-01-01 00:00:00 UTC with which to start
   *                  window intervals. For example, in order to have hourly tumbling windows that
   *                  start 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15... provide
   *                  `startTime` as `15 minutes`.
   *
   * @group datetime_funcs
   * @since 2.0.0
   */
  def window(
      timeColumn: Column,
      windowDuration: String,
      slideDuration: String,
      startTime: String): Column = {
    withExpr {
      TimeWindow(timeColumn.expr, windowDuration, slideDuration, startTime)
    }.as("window")
  }


  /**
   * Bucketize rows into one or more time windows given a timestamp specifying column. Window
   * starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
   * [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
   * the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC.
   * The following example takes the average stock price for a one minute window every 10 seconds:
   *
   * {{{
   *   val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType
   *   df.groupBy(window($"time", "1 minute", "10 seconds"), $"stockId")
   *     .agg(mean("price"))
   * }}}
   *
   * The windows will look like:
   *
   * {{{
   *   09:00:00-09:01:00
   *   09:00:10-09:01:10
   *   09:00:20-09:01:20 ...
   * }}}
   *
   * For a streaming query, you may use the function `current_timestamp` to generate windows on
   * processing time.
   *
   * @param timeColumn The column or the expression to use as the timestamp for windowing by time.
   *                   The time column must be of TimestampType.
   * @param windowDuration A string specifying the width of the window, e.g. `10 minutes`,
   *                       `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for
   *                       valid duration identifiers. Note that the duration is a fixed length of
   *                       time, and does not vary over time according to a calendar. For example,
   *                       `1 day` always means 86,400,000 milliseconds, not a calendar day.
   * @param slideDuration A string specifying the sliding interval of the window, e.g. `1 minute`.
   *                      A new window will be generated every `slideDuration`. Must be less than
   *                      or equal to the `windowDuration`. Check
   *                      `org.apache.spark.unsafe.types.CalendarInterval` for valid duration
   *                      identifiers. This duration is likewise absolute, and does not vary
   *                      according to a calendar.
   *
   * @group datetime_funcs
   * @since 2.0.0
   */
  def window(timeColumn: Column, windowDuration: String, slideDuration: String): Column = {
    window(timeColumn, windowDuration, slideDuration, "0 second")
  }

  /**
   * Generates tumbling time windows given a timestamp specifying column. Window
   * starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
   * [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
   * the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC.
   * The following example takes the average stock price for a one minute tumbling window:
   *
   * {{{
   *   val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType
   *   df.groupBy(window($"time", "1 minute"), $"stockId")
   *     .agg(mean("price"))
   * }}}
   *
   * The windows will look like:
   *
   * {{{
   *   09:00:00-09:01:00
   *   09:01:00-09:02:00
   *   09:02:00-09:03:00 ...
   * }}}
   *
   * For a streaming query, you may use the function `current_timestamp` to generate windows on
   * processing time.
   *
   * @param timeColumn The column or the expression to use as the timestamp for windowing by time.
   *                   The time column must be of TimestampType.
   * @param windowDuration A string specifying the width of the window, e.g. `10 minutes`,
   *                       `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for
   *                       valid duration identifiers.
   *
   * @group datetime_funcs
   * @since 2.0.0
   */
  def window(timeColumn: Column, windowDuration: String): Column = {
    window(timeColumn, windowDuration, windowDuration, "0 second")
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Collection functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Returns null if the array is null, true if the array contains `value`, and false otherwise.
   * @group collection_funcs
   * @since 1.5.0
   */
  def array_contains(column: Column, value: Any): Column = withExpr {
    ArrayContains(column.expr, lit(value).expr)
  }

  /**
   * Returns `true` if `a1` and `a2` have at least one non-null element in common. If not and both
   * the arrays are non-empty and any of them contains a `null`, it returns `null`. It returns
   * `false` otherwise.
   * @group collection_funcs
   * @since 2.4.0
   */
  def arrays_overlap(a1: Column, a2: Column): Column = withExpr {
    ArraysOverlap(a1.expr, a2.expr)
  }

  /**
   * Returns an array containing all the elements in `x` from index `start` (or starting from the
   * end if `start` is negative) with the specified `length`.
   *
   * @param x the array column to be sliced
   * @param start the starting index
   * @param length the length of the slice
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def slice(x: Column, start: Int, length: Int): Column = withExpr {
    Slice(x.expr, Literal(start), Literal(length))
  }

  /**
   * Concatenates the elements of `column` using the `delimiter`. Null values are replaced with
   * `nullReplacement`.
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_join(column: Column, delimiter: String, nullReplacement: String): Column = withExpr {
    ArrayJoin(column.expr, Literal(delimiter), Some(Literal(nullReplacement)))
  }

  /**
   * Concatenates the elements of `column` using the `delimiter`.
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_join(column: Column, delimiter: String): Column = withExpr {
    ArrayJoin(column.expr, Literal(delimiter), None)
  }

  /**
   * Concatenates multiple input columns together into a single column.
   * The function works with strings, binary and compatible array columns.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def concat(exprs: Column*): Column = withExpr { Concat(exprs.map(_.expr)) }

  /**
   * Locates the position of the first occurrence of the value in the given array as long.
   * Returns null if either of the arguments are null.
   *
   * @note The position is not zero based, but 1 based index. Returns 0 if value
   * could not be found in array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_position(column: Column, value: Any): Column = withExpr {
    ArrayPosition(column.expr, lit(value).expr)
  }

  /**
   * Returns element of array at given index in value if column is array. Returns value for
   * the given key in value if column is map.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def element_at(column: Column, value: Any): Column = withExpr {
    ElementAt(column.expr, lit(value).expr)
  }

  /**
   * Sorts the input array in ascending order. The elements of the input array must be orderable.
   * Null elements will be placed at the end of the returned array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_sort(e: Column): Column = withExpr { ArraySort(e.expr) }

  /**
   * Remove all elements that equal to element from the given array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_remove(column: Column, element: Any): Column = withExpr {
    ArrayRemove(column.expr, lit(element).expr)
  }

  /**
   * Removes duplicate values from the array.
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_distinct(e: Column): Column = withExpr { ArrayDistinct(e.expr) }

  /**
   * Returns an array of the elements in the intersection of the given two arrays,
   * without duplicates.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_intersect(col1: Column, col2: Column): Column = withExpr {
    ArrayIntersect(col1.expr, col2.expr)
  }

  /**
   * Returns an array of the elements in the union of the given two arrays, without duplicates.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_union(col1: Column, col2: Column): Column = withExpr {
    ArrayUnion(col1.expr, col2.expr)
  }

  /**
   * Returns an array of the elements in the first array but not in the second array,
   * without duplicates. The order of elements in the result is not determined
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_except(col1: Column, col2: Column): Column = withExpr {
    ArrayExcept(col1.expr, col2.expr)
  }

  /**
   * Creates a new row for each element in the given array or map column.
   *
   * @group collection_funcs
   * @since 1.3.0
   */
  def explode(e: Column): Column = withExpr { Explode(e.expr) }

  /**
   * Creates a new row for each element in the given array or map column.
   * Unlike explode, if the array/map is null or empty then null is produced.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def explode_outer(e: Column): Column = withExpr { GeneratorOuter(Explode(e.expr)) }

  /**
   * Creates a new row for each element with position in the given array or map column.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def posexplode(e: Column): Column = withExpr { PosExplode(e.expr) }

  /**
   * Creates a new row for each element with position in the given array or map column.
   * Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def posexplode_outer(e: Column): Column = withExpr { GeneratorOuter(PosExplode(e.expr)) }

  /**
   * Extracts json object from a json string based on json path specified, and returns json string
   * of the extracted json object. It will return null if the input json string is invalid.
   *
   * @group collection_funcs
   * @since 1.6.0
   */
  def get_json_object(e: Column, path: String): Column = withExpr {
    GetJsonObject(e.expr, lit(path).expr)
  }

  /**
   * Creates a new row for a json column according to the given field names.
   *
   * @group collection_funcs
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def json_tuple(json: Column, fields: String*): Column = withExpr {
    require(fields.nonEmpty, "at least 1 field name should be given.")
    JsonTuple(json.expr +: fields.map(Literal.apply))
  }

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `StructType` with the
   * specified schema. Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. Accepts the same options as the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: StructType, options: Map[String, String]): Column =
    from_json(e, schema.asInstanceOf[DataType], options)

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def from_json(e: Column, schema: DataType, options: Map[String, String]): Column = withExpr {
    JsonToStructs(schema, options, e.expr)
  }

  /**
   * (Java-specific) Parses a column containing a JSON string into a `StructType` with the
   * specified schema. Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: StructType, options: java.util.Map[String, String]): Column =
    from_json(e, schema, options.asScala.toMap)

  /**
   * (Java-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def from_json(e: Column, schema: DataType, options: java.util.Map[String, String]): Column =
    from_json(e, schema, options.asScala.toMap)

  /**
   * Parses a column containing a JSON string into a `StructType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: StructType): Column =
    from_json(e, schema, Map.empty[String, String])

  /**
   * Parses a column containing a JSON string into a `MapType` with `StringType` as keys type,
   * `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def from_json(e: Column, schema: DataType): Column =
    from_json(e, schema, Map.empty[String, String])

  /**
   * (Java-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string as a json string. In Spark 2.1,
   *               the user-provided schema has to be in JSON format. Since Spark 2.2, the DDL
   *               format is also supported for the schema.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: String, options: java.util.Map[String, String]): Column = {
    from_json(e, schema, options.asScala.toMap)
  }

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string as a json string, it could be a
   *               JSON format string or a DDL-formatted string.
   *
   * @group collection_funcs
   * @since 2.3.0
   */
  def from_json(e: Column, schema: String, options: Map[String, String]): Column = {
    val dataType = try {
      DataType.fromJson(schema)
    } catch {
      case NonFatal(_) => DataType.fromDDL(schema)
    }
    from_json(e, dataType, options)
  }

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` of `StructType`s with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def from_json(e: Column, schema: Column): Column = {
    from_json(e, schema, Map.empty[String, String].asJava)
  }

  /**
   * (Java-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` of `StructType`s with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def from_json(e: Column, schema: Column, options: java.util.Map[String, String]): Column = {
    withExpr(new JsonToStructs(e.expr, schema.expr, options.asScala.toMap))
  }

  /**
   * Parses a JSON string and infers its schema in DDL format.
   *
   * @param json a JSON string.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def schema_of_json(json: String): Column = schema_of_json(lit(json))

  /**
   * Parses a JSON string and infers its schema in DDL format.
   *
   * @param json a string literal containing a JSON string.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def schema_of_json(json: Column): Column = withExpr(new SchemaOfJson(json.expr))

  /**
   * (Scala-specific) Converts a column containing a `StructType`, `ArrayType` or
   * a `MapType` into a JSON string with the specified schema.
   * Throws an exception, in the case of an unsupported type.
   *
   * @param e a column containing a struct, an array or a map.
   * @param options options to control how the struct column is converted into a json string.
   *                accepts the same options and the json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def to_json(e: Column, options: Map[String, String]): Column = withExpr {
    StructsToJson(options, e.expr)
  }

  /**
   * (Java-specific) Converts a column containing a `StructType`, `ArrayType` or
   * a `MapType` into a JSON string with the specified schema.
   * Throws an exception, in the case of an unsupported type.
   *
   * @param e a column containing a struct, an array or a map.
   * @param options options to control how the struct column is converted into a json string.
   *                accepts the same options and the json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def to_json(e: Column, options: java.util.Map[String, String]): Column =
    to_json(e, options.asScala.toMap)

  /**
   * Converts a column containing a `StructType`, `ArrayType` or
   * a `MapType` into a JSON string with the specified schema.
   * Throws an exception, in the case of an unsupported type.
   *
   * @param e a column containing a struct, an array or a map.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def to_json(e: Column): Column =
    to_json(e, Map.empty[String, String])

  /**
   * Returns length of array or map.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  def size(e: Column): Column = withExpr { Size(e.expr) }

  /**
   * Sorts the input array for the given column in ascending order,
   * according to the natural ordering of the array elements.
   * Null elements will be placed at the beginning of the returned array.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  def sort_array(e: Column): Column = sort_array(e, asc = true)

  /**
   * Sorts the input array for the given column in ascending or descending order,
   * according to the natural ordering of the array elements.
   * Null elements will be placed at the beginning of the returned array in ascending order or
   * at the end of the returned array in descending order.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  def sort_array(e: Column, asc: Boolean): Column = withExpr { SortArray(e.expr, lit(asc).expr) }

  /**
   * Returns the minimum value in the array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_min(e: Column): Column = withExpr { ArrayMin(e.expr) }

  /**
   * Returns the maximum value in the array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_max(e: Column): Column = withExpr { ArrayMax(e.expr) }

  /**
   * Returns a random permutation of the given array.
   *
   * @note The function is non-deterministic.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def shuffle(e: Column): Column = withExpr { Shuffle(e.expr) }

  /**
   * Returns a reversed string or an array with reverse order of elements.
   * @group collection_funcs
   * @since 1.5.0
   */
  def reverse(e: Column): Column = withExpr { Reverse(e.expr) }

  /**
   * Creates a single array from an array of arrays. If a structure of nested arrays is deeper than
   * two levels, only one level of nesting is removed.
   * @group collection_funcs
   * @since 2.4.0
   */
  def flatten(e: Column): Column = withExpr { Flatten(e.expr) }

  /**
   * Generate a sequence of integers from start to stop, incrementing by step.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def sequence(start: Column, stop: Column, step: Column): Column = withExpr {
    new Sequence(start.expr, stop.expr, step.expr)
  }

  /**
   * Generate a sequence of integers from start to stop,
   * incrementing by 1 if start is less than or equal to stop, otherwise -1.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def sequence(start: Column, stop: Column): Column = withExpr {
    new Sequence(start.expr, stop.expr)
  }

  /**
   * Creates an array containing the left argument repeated the number of times given by the
   * right argument.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_repeat(left: Column, right: Column): Column = withExpr {
    ArrayRepeat(left.expr, right.expr)
  }

  /**
   * Creates an array containing the left argument repeated the number of times given by the
   * right argument.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_repeat(e: Column, count: Int): Column = array_repeat(e, lit(count))

  /**
   * Returns an unordered array containing the keys of the map.
   * @group collection_funcs
   * @since 2.3.0
   */
  def map_keys(e: Column): Column = withExpr { MapKeys(e.expr) }

  /**
   * Returns an unordered array containing the values of the map.
   * @group collection_funcs
   * @since 2.3.0
   */
  def map_values(e: Column): Column = withExpr { MapValues(e.expr) }

  /**
   * Returns a map created from the given array of entries.
   * @group collection_funcs
   * @since 2.4.0
   */
  def map_from_entries(e: Column): Column = withExpr { MapFromEntries(e.expr) }

  /**
   * Returns a merged array of structs in which the N-th struct contains all N-th values of input
   * arrays.
   * @group collection_funcs
   * @since 2.4.0
   */
  @scala.annotation.varargs
  def arrays_zip(e: Column*): Column = withExpr { ArraysZip(e.map(_.expr)) }

  /**
   * Returns the union of all the given maps.
   * @group collection_funcs
   * @since 2.4.0
   */
  @scala.annotation.varargs
  def map_concat(cols: Column*): Column = withExpr { MapConcat(cols.map(_.expr)) }

  // scalastyle:off line.size.limit
  // scalastyle:off parameter.number

  /* Use the following code to generate:

  (0 to 10).foreach { x =>
    val types = (1 to x).foldRight("RT")((i, s) => {s"A$i, $s"})
    val typeTags = (1 to x).map(i => s"A$i: TypeTag").foldLeft("RT: TypeTag")(_ + ", " + _)
    val inputSchemas = (1 to x).foldRight("Nil")((i, s) => {s"Try(ScalaReflection.schemaFor(typeTag[A$i])).toOption :: $s"})
    println(s"""
      |/**
      | * Defines a Scala closure of $x arguments as user-defined function (UDF).
      | * The data types are automatically inferred based on the Scala closure's
      | * signature. By default the returned UDF is deterministic. To change it to
      | * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
      | *
      | * @group udf_funcs
      | * @since 1.3.0
      | */
      |def udf[$typeTags](f: Function$x[$types]): UserDefinedFunction = {
      |  val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
      |  val inputSchemas = $inputSchemas
      |  val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
      |  if (nullable) udf else udf.asNonNullable()
      |}""".stripMargin)
  }

  (0 to 10).foreach { i =>
    val extTypeArgs = (0 to i).map(_ => "_").mkString(", ")
    val anyTypeArgs = (0 to i).map(_ => "Any").mkString(", ")
    val anyCast = s".asInstanceOf[UDF$i[$anyTypeArgs]]"
    val anyParams = (1 to i).map(_ => "_: Any").mkString(", ")
    val funcCall = if (i == 0) "() => func" else "func"
    println(s"""
      |/**
      | * Defines a Java UDF$i instance as user-defined function (UDF).
      | * The caller must specify the output data type, and there is no automatic input type coercion.
      | * By default the returned UDF is deterministic. To change it to nondeterministic, call the
      | * API `UserDefinedFunction.asNondeterministic()`.
      | *
      | * @group udf_funcs
      | * @since 2.3.0
      | */
      |def udf(f: UDF$i[$extTypeArgs], returnType: DataType): UserDefinedFunction = {
      |  val func = f$anyCast.call($anyParams)
      |  SparkUserDefinedFunction.create($funcCall, returnType, inputSchemas = Seq.fill($i)(None))
      |}""".stripMargin)
  }

  */

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Scala UDF functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Defines a Scala closure of 0 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag](f: Function0[RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 1 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag](f: Function1[A1, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 2 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag](f: Function2[A1, A2, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 3 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag](f: Function3[A1, A2, A3, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 4 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag](f: Function4[A1, A2, A3, A4, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 5 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag](f: Function5[A1, A2, A3, A4, A5, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 6 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag](f: Function6[A1, A2, A3, A4, A5, A6, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 7 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag](f: Function7[A1, A2, A3, A4, A5, A6, A7, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 8 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag, A8: TypeTag](f: Function8[A1, A2, A3, A4, A5, A6, A7, A8, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A8])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 9 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag, A8: TypeTag, A9: TypeTag](f: Function9[A1, A2, A3, A4, A5, A6, A7, A8, A9, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A8])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A9])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 10 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag, A8: TypeTag, A9: TypeTag, A10: TypeTag](f: Function10[A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A8])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A9])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A10])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Java UDF functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Defines a Java UDF0 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF0[_], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF0[Any]].call()
    SparkUserDefinedFunction.create(() => func, returnType, inputSchemas = Seq.fill(0)(None))
  }

  /**
   * Defines a Java UDF1 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF1[_, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF1[Any, Any]].call(_: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(1)(None))
  }

  /**
   * Defines a Java UDF2 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF2[_, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF2[Any, Any, Any]].call(_: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(2)(None))
  }

  /**
   * Defines a Java UDF3 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF3[_, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF3[Any, Any, Any, Any]].call(_: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(3)(None))
  }

  /**
   * Defines a Java UDF4 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF4[_, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF4[Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(4)(None))
  }

  /**
   * Defines a Java UDF5 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF5[_, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF5[Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(5)(None))
  }

  /**
   * Defines a Java UDF6 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF6[_, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF6[Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(6)(None))
  }

  /**
   * Defines a Java UDF7 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF7[_, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF7[Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(7)(None))
  }

  /**
   * Defines a Java UDF8 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF8[_, _, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF8[Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(8)(None))
  }

  /**
   * Defines a Java UDF9 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF9[_, _, _, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF9[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(9)(None))
  }

  /**
   * Defines a Java UDF10 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF10[_, _, _, _, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF10[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(10)(None))
  }

  // scalastyle:on parameter.number
  // scalastyle:on line.size.limit

  /**
   * Defines a deterministic user-defined function (UDF) using a Scala closure. For this variant,
   * the caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @param f  A closure in Scala
   * @param dataType  The output data type of the UDF
   *
   * @group udf_funcs
   * @since 2.0.0
   */
  def udf(f: AnyRef, dataType: DataType): UserDefinedFunction = {
    // TODO: should call SparkUserDefinedFunction.create() instead but inputSchemas is currently
    // unavailable. We may need to create type-safe overloaded versions of udf() methods.
    new UserDefinedFunction(f, dataType, inputTypes = None)
  }

  /**
   * Call an user-defined function.
   * Example:
   * {{{
   *  import org.apache.spark.sql._
   *
   *  val df = Seq(("id1", 1), ("id2", 4), ("id3", 5)).toDF("id", "value")
   *  val spark = df.sparkSession
   *  spark.udf.register("simpleUDF", (v: Int) => v * v)
   *  df.select($"id", callUDF("simpleUDF", $"value"))
   * }}}
   *
   * @group udf_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def callUDF(udfName: String, cols: Column*): Column = withExpr {
    UnresolvedFunction(udfName, cols.map(_.expr), isDistinct = false)
  }
}

Mar 04, 2021 11:26:36 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3547
[0m2021.03.04 11:26:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:26:38 INFO  time: compiled root in 0.78s[0m
[0m2021.03.04 11:27:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:27:57 INFO  time: compiled root in 0.81s[0m
[0m2021.03.04 11:28:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:28:39 INFO  time: compiled root in 0.89s[0m
[0m2021.03.04 11:28:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:28:52 INFO  time: compiled root in 0.88s[0m
[0m2021.03.04 11:28:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:28:57 INFO  time: compiled root in 0.82s[0m
[0m2021.03.04 11:31:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:31:57 INFO  time: compiled root in 0.17s[0m
[0m2021.03.04 11:32:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:32:06 INFO  time: compiled root in 0.62s[0m
[0m2021.03.04 11:33:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:33:08 INFO  time: compiled root in 1.01s[0m
[0m2021.03.04 11:33:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:33:45 INFO  time: compiled root in 0.27s[0m
[0m2021.03.04 11:33:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:33:52 INFO  time: compiled root in 0.91s[0m
[0m2021.03.04 11:35:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:35:24 INFO  time: compiled root in 0.88s[0m
[0m2021.03.04 11:36:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:36:49 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 11:37:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:37:05 INFO  time: compiled root in 0.26s[0m
[0m2021.03.04 11:37:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:37:15 INFO  time: compiled root in 0.81s[0m
[0m2021.03.04 11:38:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:38:40 INFO  time: compiled root in 0.94s[0m
Mar 04, 2021 11:42:18 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4353
[0m2021.03.04 11:43:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:43:40 INFO  time: compiled root in 0.84s[0m
[0m2021.03.04 11:46:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:46:16 INFO  time: compiled root in 0.96s[0m
[0m2021.03.04 11:47:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:47:51 INFO  time: compiled root in 0.96s[0m
[0m2021.03.04 11:51:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:51:33 INFO  time: compiled root in 0.87s[0m
[0m2021.03.04 11:52:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:52:26 INFO  time: compiled root in 0.88s[0m
[0m2021.03.04 11:53:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:53:31 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 11:53:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:53:48 INFO  time: compiled root in 0.92s[0m
[0m2021.03.04 11:54:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:54:18 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 11:54:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:54:53 INFO  time: compiled root in 0.62s[0m
[0m2021.03.04 11:55:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:55:28 INFO  time: compiled root in 0.58s[0m
[0m2021.03.04 11:55:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:55:33 INFO  time: compiled root in 1.12s[0m
[0m2021.03.04 11:55:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:55:50 INFO  time: compiled root in 1.06s[0m
[0m2021.03.04 11:57:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:57:29 INFO  time: compiled root in 0.82s[0m
[0m2021.03.04 11:58:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:58:12 INFO  time: compiled root in 0.73s[0m
[0m2021.03.04 12:06:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:06:27 INFO  time: compiled root in 0.12s[0m
[0m2021.03.04 12:06:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:06:35 INFO  time: compiled root in 0.87s[0m
[0m2021.03.04 12:07:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:07:29 INFO  time: compiled root in 0.78s[0m
[0m2021.03.04 12:08:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:08:26 INFO  time: compiled root in 0.79s[0m
[0m2021.03.04 12:10:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:10:08 INFO  time: compiled root in 0.79s[0m
[0m2021.03.04 12:10:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:10:13 INFO  time: compiled root in 0.77s[0m
[0m2021.03.04 12:10:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:10:16 INFO  time: compiled root in 0.73s[0m
[0m2021.03.04 12:12:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:12:08 INFO  time: compiled root in 0.82s[0m
[0m2021.03.04 12:14:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:14:43 INFO  time: compiled root in 0.75s[0m
[0m2021.03.04 12:14:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:14:45 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 12:14:59 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:73: error: ) expected but interpolation id found
        $"_tmp".getItem(1).as("Plain Text")
        ^[0m
[0m2021.03.04 12:15:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:15:02 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 12:15:04 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:73: error: ) expected but interpolation id found
        $"_tmp".getItem(1).as("Plain Text")
        ^[0m
[0m2021.03.04 12:15:06 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:73: error: ) expected but interpolation id found
        $"_tmp".getItem(1).as("Plain Text")
        ^[0m
[0m2021.03.04 12:15:13 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:73: error: ) expected but interpolation id found
        $"_tmp".getItem(1).as("Plain Text")
        ^[0m
[0m2021.03.04 12:15:16 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:73: error: ) expected but interpolation id found
        $"_tmp".getItem(1).as("Plain Text")
        ^[0m
[0m2021.03.04 12:15:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:15:18 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 12:15:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:15:23 INFO  time: compiled root in 0.15s[0m
[0m2021.03.04 12:15:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:15:46 INFO  time: compiled root in 0.12s[0m
[0m2021.03.04 12:16:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:16:00 INFO  time: compiled root in 0.1s[0m
[0m2021.03.04 12:16:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:16:11 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 12:18:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:18:17 INFO  time: compiled root in 0.89s[0m
[0m2021.03.04 12:22:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:22:20 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 12:22:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:22:25 INFO  time: compiled root in 0.88s[0m
[0m2021.03.04 12:22:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:22:28 INFO  time: compiled root in 0.85s[0m
[0m2021.03.04 12:39:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:39:46 INFO  time: compiled root in 0.89s[0m
[0m2021.03.04 12:39:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:39:53 INFO  time: compiled root in 1.01s[0m
[0m2021.03.04 12:41:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:41:04 INFO  time: compiled root in 0.96s[0m
[0m2021.03.04 12:41:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:41:37 INFO  time: compiled root in 0.95s[0m
[0m2021.03.04 12:42:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:42:59 INFO  time: compiled root in 0.98s[0m
[0m2021.03.04 12:47:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:47:30 INFO  time: compiled root in 0.22s[0m
[0m2021.03.04 12:47:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:47:32 INFO  time: compiled root in 0.28s[0m
[0m2021.03.04 12:50:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:50:13 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 12:50:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:50:21 INFO  time: compiled root in 0.26s[0m
[0m2021.03.04 12:50:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:50:26 INFO  time: compiled root in 0.96s[0m
[0m2021.03.04 12:56:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:56:09 INFO  time: compiled root in 0.99s[0m
[0m2021.03.04 13:48:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:48:19 INFO  time: compiled root in 1.03s[0m
[0m2021.03.04 13:49:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:49:14 INFO  time: compiled root in 0.23s[0m
[0m2021.03.04 13:49:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:49:29 INFO  time: compiled root in 0.22s[0m
[0m2021.03.04 13:49:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:49:33 INFO  time: compiled root in 1s[0m
[0m2021.03.04 13:51:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:51:02 INFO  time: compiled root in 0.97s[0m
[0m2021.03.04 13:51:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:51:19 INFO  time: compiled root in 0.94s[0m
[0m2021.03.04 13:51:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:51:55 INFO  time: compiled root in 0.87s[0m
[0m2021.03.04 13:53:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:53:25 INFO  time: compiled root in 1.18s[0m
[0m2021.03.04 13:53:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:53:27 INFO  time: compiled root in 1.33s[0m
[0m2021.03.04 13:54:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:54:46 INFO  time: compiled root in 0.95s[0m
[0m2021.03.04 13:55:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:55:05 INFO  time: compiled root in 0.85s[0m
[0m2021.03.04 13:55:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:55:13 INFO  time: compiled root in 0.92s[0m
[0m2021.03.04 13:55:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:55:47 INFO  time: compiled root in 0.78s[0m
[0m2021.03.04 13:56:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:56:58 INFO  time: compiled root in 0.94s[0m
[0m2021.03.04 13:58:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:58:12 INFO  time: compiled root in 0.81s[0m
[0m2021.03.04 13:58:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:58:24 INFO  time: compiled root in 0.96s[0m
[0m2021.03.04 13:59:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 13:59:53 INFO  time: compiled root in 0.78s[0m
[0m2021.03.04 14:01:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:01:58 INFO  time: compiled root in 0.79s[0m
[0m2021.03.04 14:02:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:02:52 INFO  time: compiled root in 0.65s[0m
[0m2021.03.04 14:05:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:05:54 INFO  time: compiled root in 0.58s[0m
[0m2021.03.04 14:06:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:06:28 INFO  time: compiled root in 0.57s[0m
[0m2021.03.04 14:12:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:12:03 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 14:12:04 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:45: error: ; expected but integer constant found
    Read a whole years worth of data - Jan-Dec 2020
                                               ^[0m
[0m2021.03.04 14:12:05 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:45: error: ; expected but integer constant found
    Read a whole years worth of data - Jan-Dec 2020
                                               ^[0m
[0m2021.03.04 14:12:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:12:15 INFO  time: compiled root in 0.23s[0m
[0m2021.03.04 14:12:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:12:27 INFO  time: compiled root in 0.2s[0m
[0m2021.03.04 14:12:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:12:39 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 14:12:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:12:48 INFO  time: compiled root in 0.12s[0m
[0m2021.03.04 14:12:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:12:53 INFO  time: compiled root in 0.21s[0m
[0m2021.03.04 14:13:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:13:09 INFO  time: compiled root in 99ms[0m
[0m2021.03.04 14:13:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:13:14 INFO  time: compiled root in 0.2s[0m
[0m2021.03.04 14:13:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:13:25 INFO  time: compiled root in 0.18s[0m
Mar 04, 2021 2:13:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 126 is not a valid line number, allowed [0..125]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 126 is not a valid line number, allowed [0..125]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 126 is not a valid line number, allowed [0..125]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.lineLength$1(Position.scala:57)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:60)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.04 14:13:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:13:29 INFO  time: compiled root in 0.11s[0m
Mar 04, 2021 2:13:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 126 is not a valid line number, allowed [0..125]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 126 is not a valid line number, allowed [0..125]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 126 is not a valid line number, allowed [0..125]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.lineLength$1(Position.scala:57)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:60)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.03.04 14:13:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:13:32 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 14:13:33 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:125: error: end of file expected but } found
  }
  ^[0m
[0m2021.03.04 14:14:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:14:02 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 14:14:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:14:11 INFO  time: compiled root in 0.1s[0m
[0m2021.03.04 14:14:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:14:26 INFO  time: compiled root in 0.57s[0m
[0m2021.03.04 14:14:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:14:48 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 14:14:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:14:55 INFO  time: compiled root in 0.69s[0m
[0m2021.03.04 14:15:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:15:04 INFO  time: compiled root in 0.85s[0m
[0m2021.03.04 14:15:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:15:39 INFO  time: compiled root in 0.2s[0m
[0m2021.03.04 14:15:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:15:46 INFO  time: compiled root in 0.17s[0m
[0m2021.03.04 14:15:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:15:48 INFO  time: compiled root in 0.17s[0m
[0m2021.03.04 14:16:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:16:10 INFO  time: compiled root in 0.17s[0m
[0m2021.03.04 14:16:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:16:35 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 14:16:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:16:47 INFO  time: compiled root in 0.2s[0m
[0m2021.03.04 14:17:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:17:10 INFO  time: compiled root in 0.21s[0m
[0m2021.03.04 14:17:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:17:16 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 14:17:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:17:24 INFO  time: compiled root in 0.21s[0m
[0m2021.03.04 14:17:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:17:49 INFO  time: compiled root in 0.85s[0m
[0m2021.03.04 14:17:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:17:57 INFO  time: compiled root in 0.8s[0m
[0m2021.03.04 14:18:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:18:22 INFO  time: compiled root in 0.81s[0m
[0m2021.03.04 14:18:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:18:51 INFO  time: compiled root in 0.81s[0m
[0m2021.03.04 14:19:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:19:02 INFO  time: compiled root in 0.86s[0m
[0m2021.03.04 14:19:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:19:43 INFO  time: compiled root in 0.73s[0m
[0m2021.03.04 14:20:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:20:01 INFO  time: compiled root in 0.74s[0m
[0m2021.03.04 14:21:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:21:05 INFO  time: compiled root in 0.8s[0m
Mar 04, 2021 2:21:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8886
[0m2021.03.04 14:21:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:21:18 INFO  time: compiled root in 0.74s[0m
Mar 04, 2021 2:21:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8926
[0m2021.03.04 14:21:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:21:27 INFO  time: compiled root in 0.76s[0m
[0m2021.03.04 14:21:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:21:58 INFO  time: compiled root in 0.78s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.Closeable
import java.util.concurrent.atomic.AtomicReference

import scala.collection.JavaConverters._
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.spark.{SPARK_VERSION, SparkConf, SparkContext, TaskContext}
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd}
import org.apache.spark.sql.catalog.Catalog
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions.AttributeReference
import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, Range}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.internal._
import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
import org.apache.spark.sql.sources.BaseRelation
import org.apache.spark.sql.streaming._
import org.apache.spark.sql.types.{DataType, StructType}
import org.apache.spark.sql.util.ExecutionListenerManager
import org.apache.spark.util.{CallSite, Utils}


/**
 * The entry point to programming Spark with the Dataset and DataFrame API.
 *
 * In environments that this has been created upfront (e.g. REPL, notebooks), use the builder
 * to get an existing session:
 *
 * {{{
 *   SparkSession.builder().getOrCreate()
 * }}}
 *
 * The builder can also be used to create a new session:
 *
 * {{{
 *   SparkSession.builder
 *     .master("local")
 *     .appName("Word Count")
 *     .config("spark.some.config.option", "some-value")
 *     .getOrCreate()
 * }}}
 *
 * @param sparkContext The Spark context associated with this Spark session.
 * @param existingSharedState If supplied, use the existing shared state
 *                            instead of creating a new one.
 * @param parentSessionState If supplied, inherit all session state (i.e. temporary
 *                            views, SQL config, UDFs etc) from parent.
 */
@InterfaceStability.Stable
class SparkSession private(
    @transient val sparkContext: SparkContext,
    @transient private val existingSharedState: Option[SharedState],
    @transient private val parentSessionState: Option[SessionState],
    @transient private[sql] val extensions: SparkSessionExtensions)
  extends Serializable with Closeable with Logging { self =>

  // The call site where this SparkSession was constructed.
  private val creationSite: CallSite = Utils.getCallSite()

  private[sql] def this(sc: SparkContext) {
    this(sc, None, None, new SparkSessionExtensions)
  }

  sparkContext.assertNotStopped()

  // If there is no active SparkSession, uses the default SQL conf. Otherwise, use the session's.
  SQLConf.setSQLConfGetter(() => {
    SparkSession.getActiveSession.filterNot(_.sparkContext.isStopped).map(_.sessionState.conf)
      .getOrElse(SQLConf.getFallbackConf)
  })

  /**
   * The version of Spark on which this application is running.
   *
   * @since 2.0.0
   */
  def version: String = SPARK_VERSION

  /* ----------------------- *
   |  Session-related state  |
   * ----------------------- */

  /**
   * State shared across sessions, including the `SparkContext`, cached data, listener,
   * and a catalog that interacts with external systems.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sharedState: SharedState = {
    existingSharedState.getOrElse(new SharedState(sparkContext))
  }

  /**
   * Initial options for session. This options are applied once when sessionState is created.
   */
  @transient
  private[sql] val initialSessionOptions = new scala.collection.mutable.HashMap[String, String]

  /**
   * State isolated across sessions, including SQL configurations, temporary tables, registered
   * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]].
   * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sessionState: SessionState = {
    parentSessionState
      .map(_.clone(this))
      .getOrElse {
        val state = SparkSession.instantiateSessionState(
          SparkSession.sessionStateClassName(sparkContext.conf),
          self)
        initialSessionOptions.foreach { case (k, v) => state.conf.setConfString(k, v) }
        state
      }
  }

  /**
   * A wrapped version of this session in the form of a [[SQLContext]], for backward compatibility.
   *
   * @since 2.0.0
   */
  @transient
  val sqlContext: SQLContext = new SQLContext(this)

  /**
   * Runtime configuration interface for Spark.
   *
   * This is the interface through which the user can get and set all Spark and Hadoop
   * configurations that are relevant to Spark SQL. When getting the value of a config,
   * this defaults to the value set in the underlying `SparkContext`, if any.
   *
   * @since 2.0.0
   */
  @transient lazy val conf: RuntimeConfig = new RuntimeConfig(sessionState.conf)

  /**
   * :: Experimental ::
   * An interface to register custom [[org.apache.spark.sql.util.QueryExecutionListener]]s
   * that listen for execution metrics.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def listenerManager: ExecutionListenerManager = sessionState.listenerManager

  /**
   * :: Experimental ::
   * A collection of methods that are considered experimental, but can be used to hook into
   * the query planner for advanced functionality.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Unstable
  def experimental: ExperimentalMethods = sessionState.experimentalMethods

  /**
   * A collection of methods for registering user-defined functions (UDF).
   *
   * The following example registers a Scala closure as UDF:
   * {{{
   *   sparkSession.udf.register("myUDF", (arg1: Int, arg2: String) => arg2 + arg1)
   * }}}
   *
   * The following example registers a UDF in Java:
   * {{{
   *   sparkSession.udf().register("myUDF",
   *       (Integer arg1, String arg2) -> arg2 + arg1,
   *       DataTypes.StringType);
   * }}}
   *
   * @note The user-defined functions must be deterministic. Due to optimization,
   * duplicate invocations may be eliminated or the function may even be invoked more times than
   * it is present in the query.
   *
   * @since 2.0.0
   */
  def udf: UDFRegistration = sessionState.udfRegistration

  /**
   * :: Experimental ::
   * Returns a `StreamingQueryManager` that allows managing all the
   * `StreamingQuery`s active on `this`.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Unstable
  def streams: StreamingQueryManager = sessionState.streamingQueryManager

  /**
   * Start a new session with isolated SQL configurations, temporary tables, registered
   * functions are isolated, but sharing the underlying `SparkContext` and cached data.
   *
   * @note Other than the `SparkContext`, all shared state is initialized lazily.
   * This method will force the initialization of the shared state to ensure that parent
   * and child sessions are set up with the same shared state. If the underlying catalog
   * implementation is Hive, this will initialize the metastore, which may take some time.
   *
   * @since 2.0.0
   */
  def newSession(): SparkSession = {
    new SparkSession(sparkContext, Some(sharedState), parentSessionState = None, extensions)
  }

  /**
   * Create an identical copy of this `SparkSession`, sharing the underlying `SparkContext`
   * and shared state. All the state of this session (i.e. SQL configurations, temporary tables,
   * registered functions) is copied over, and the cloned session is set up with the same shared
   * state as this session. The cloned session is independent of this session, that is, any
   * non-global change in either session is not reflected in the other.
   *
   * @note Other than the `SparkContext`, all shared state is initialized lazily.
   * This method will force the initialization of the shared state to ensure that parent
   * and child sessions are set up with the same shared state. If the underlying catalog
   * implementation is Hive, this will initialize the metastore, which may take some time.
   */
  private[sql] def cloneSession(): SparkSession = {
    val result = new SparkSession(sparkContext, Some(sharedState), Some(sessionState), extensions)
    result.sessionState // force copy of SessionState
    result
  }


  /* --------------------------------- *
   |  Methods for creating DataFrames  |
   * --------------------------------- */

  /**
   * Returns a `DataFrame` with no rows or columns.
   *
   * @since 2.0.0
   */
  @transient
  lazy val emptyDataFrame: DataFrame = {
    createDataFrame(sparkContext.emptyRDD[Row].setName("empty"), StructType(Nil))
  }

  /**
   * :: Experimental ::
   * Creates a new [[Dataset]] of type T containing zero elements.
   *
   * @return 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def emptyDataset[T: Encoder]: Dataset[T] = {
    val encoder = implicitly[Encoder[T]]
    new Dataset(self, LocalRelation(encoder.schema.toAttributes), encoder)
  }

  /**
   * :: Experimental ::
   * Creates a `DataFrame` from an RDD of Product (e.g. case classes, tuples).
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = {
    SparkSession.setActiveSession(this)
    val encoder = Encoders.product[A]
    Dataset.ofRows(self, ExternalRDD(rdd, self)(encoder))
  }

  /**
   * :: Experimental ::
   * Creates a `DataFrame` from a local Seq of Product.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataFrame[A <: Product : TypeTag](data: Seq[A]): DataFrame = {
    SparkSession.setActiveSession(this)
    val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]
    val attributeSeq = schema.toAttributes
    Dataset.ofRows(self, LocalRelation.fromProduct(attributeSeq, data))
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from an `RDD` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided RDD matches
   * the provided schema. Otherwise, there will be runtime exception.
   * Example:
   * {{{
   *  import org.apache.spark.sql._
   *  import org.apache.spark.sql.types._
   *  val sparkSession = new org.apache.spark.sql.SparkSession(sc)
   *
   *  val schema =
   *    StructType(
   *      StructField("name", StringType, false) ::
   *      StructField("age", IntegerType, true) :: Nil)
   *
   *  val people =
   *    sc.textFile("examples/src/main/resources/people.txt").map(
   *      _.split(",")).map(p => Row(p(0), p(1).trim.toInt))
   *  val dataFrame = sparkSession.createDataFrame(people, schema)
   *  dataFrame.printSchema
   *  // root
   *  // |-- name: string (nullable = false)
   *  // |-- age: integer (nullable = true)
   *
   *  dataFrame.createOrReplaceTempView("people")
   *  sparkSession.sql("select name from people").collect.foreach(println)
   * }}}
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame = {
    createDataFrame(rowRDD, schema, needsConversion = true)
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from a `JavaRDD` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided RDD matches
   * the provided schema. Otherwise, there will be runtime exception.
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rowRDD: JavaRDD[Row], schema: StructType): DataFrame = {
    createDataFrame(rowRDD.rdd, schema)
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from a `java.util.List` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided List matches
   * the provided schema. Otherwise, there will be runtime exception.
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rows: java.util.List[Row], schema: StructType): DataFrame = {
    Dataset.ofRows(self, LocalRelation.fromExternalRows(schema.toAttributes, rows.asScala))
  }

  /**
   * Applies a schema to an RDD of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   * SELECT * queries will return the columns in an undefined order.
   *
   * @since 2.0.0
   */
  def createDataFrame(rdd: RDD[_], beanClass: Class[_]): DataFrame = {
    val attributeSeq: Seq[AttributeReference] = getSchema(beanClass)
    val className = beanClass.getName
    val rowRdd = rdd.mapPartitions { iter =>
    // BeanInfo is not serializable so we must rediscover it remotely for each partition.
      SQLContext.beansToRows(iter, Utils.classForName(className), attributeSeq)
    }
    Dataset.ofRows(self, LogicalRDD(attributeSeq, rowRdd.setName(rdd.name))(self))
  }

  /**
   * Applies a schema to an RDD of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   * SELECT * queries will return the columns in an undefined order.
   *
   * @since 2.0.0
   */
  def createDataFrame(rdd: JavaRDD[_], beanClass: Class[_]): DataFrame = {
    createDataFrame(rdd.rdd, beanClass)
  }

  /**
   * Applies a schema to a List of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   *          SELECT * queries will return the columns in an undefined order.
   * @since 1.6.0
   */
  def createDataFrame(data: java.util.List[_], beanClass: Class[_]): DataFrame = {
    val attrSeq = getSchema(beanClass)
    val rows = SQLContext.beansToRows(data.asScala.iterator, beanClass, attrSeq)
    Dataset.ofRows(self, LocalRelation(attrSeq, rows.toSeq))
  }

  /**
   * Convert a `BaseRelation` created for external data sources into a `DataFrame`.
   *
   * @since 2.0.0
   */
  def baseRelationToDataFrame(baseRelation: BaseRelation): DataFrame = {
    Dataset.ofRows(self, LogicalRelation(baseRelation))
  }

  /* ------------------------------- *
   |  Methods for creating DataSets  |
   * ------------------------------- */

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from a local Seq of data of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * == Example ==
   *
   * {{{
   *
   *   import spark.implicits._
   *   case class Person(name: String, age: Long)
   *   val data = Seq(Person("Michael", 29), Person("Andy", 30), Person("Justin", 19))
   *   val ds = spark.createDataset(data)
   *
   *   ds.show()
   *   // +-------+---+
   *   // |   name|age|
   *   // +-------+---+
   *   // |Michael| 29|
   *   // |   Andy| 30|
   *   // | Justin| 19|
   *   // +-------+---+
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: Seq[T]): Dataset[T] = {
    // `ExpressionEncoder` is not thread-safe, here we create a new encoder.
    val enc = encoderFor[T].copy()
    val attributes = enc.schema.toAttributes
    val encoded = data.map(d => enc.toRow(d).copy())
    val plan = new LocalRelation(attributes, encoded)
    Dataset[T](self, plan)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from an RDD of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: RDD[T]): Dataset[T] = {
    Dataset[T](self, ExternalRDD(data, self))
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from a `java.util.List` of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * == Java Example ==
   *
   * {{{
   *     List<String> data = Arrays.asList("hello", "world");
   *     Dataset<String> ds = spark.createDataset(data, Encoders.STRING());
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: java.util.List[T]): Dataset[T] = {
    createDataset(data.asScala)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from 0 to `end` (exclusive) with step value 1.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(end: Long): Dataset[java.lang.Long] = range(0, end)

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with step value 1.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long): Dataset[java.lang.Long] = {
    range(start, end, step = 1, numPartitions = sparkContext.defaultParallelism)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with a step value.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long, step: Long): Dataset[java.lang.Long] = {
    range(start, end, step, numPartitions = sparkContext.defaultParallelism)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with a step value, with partition number
   * specified.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long] = {
    new Dataset(self, Range(start, end, step, numPartitions), Encoders.LONG)
  }

  /**
   * Creates a `DataFrame` from an `RDD[InternalRow]`.
   */
  private[sql] def internalCreateDataFrame(
      catalystRows: RDD[InternalRow],
      schema: StructType,
      isStreaming: Boolean = false): DataFrame = {
    // TODO: use MutableProjection when rowRDD is another DataFrame and the applied
    // schema differs from the existing schema on any field data type.
    val logicalPlan = LogicalRDD(
      schema.toAttributes,
      catalystRows,
      isStreaming = isStreaming)(self)
    Dataset.ofRows(self, logicalPlan)
  }

  /**
   * Creates a `DataFrame` from an `RDD[Row]`.
   * User can specify whether the input rows should be converted to Catalyst rows.
   */
  private[sql] def createDataFrame(
      rowRDD: RDD[Row],
      schema: StructType,
      needsConversion: Boolean) = {
    // TODO: use MutableProjection when rowRDD is another DataFrame and the applied
    // schema differs from the existing schema on any field data type.
    val catalystRows = if (needsConversion) {
      val encoder = RowEncoder(schema)
      rowRDD.map(encoder.toRow)
    } else {
      rowRDD.map { r: Row => InternalRow.fromSeq(r.toSeq) }
    }
    internalCreateDataFrame(catalystRows.setName(rowRDD.name), schema)
  }


  /* ------------------------- *
   |  Catalog-related methods  |
   * ------------------------- */

  /**
   * Interface through which the user may create, drop, alter or query underlying
   * databases, tables, functions etc.
   *
   * @since 2.0.0
   */
  @transient lazy val catalog: Catalog = new CatalogImpl(self)

  /**
   * Returns the specified table/view as a `DataFrame`.
   *
   * @param tableName is either a qualified or unqualified name that designates a table or view.
   *                  If a database is specified, it identifies the table/view from the database.
   *                  Otherwise, it first attempts to find a temporary view with the given name
   *                  and then match the table/view from the current database.
   *                  Note that, the global temporary view database is also valid here.
   * @since 2.0.0
   */
  def table(tableName: String): DataFrame = {
    table(sessionState.sqlParser.parseTableIdentifier(tableName))
  }

  private[sql] def table(tableIdent: TableIdentifier): DataFrame = {
    Dataset.ofRows(self, UnresolvedRelation(tableIdent))
  }

  /* ----------------- *
   |  Everything else  |
   * ----------------- */

  /**
   * Executes a SQL query using Spark, returning the result as a `DataFrame`.
   * The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'.
   *
   * @since 2.0.0
   */
  def sql(sqlText: String): DataFrame = {
    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))
  }

  /**
   * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a
   * `DataFrame`.
   * {{{
   *   sparkSession.read.parquet("/path/to/file.parquet")
   *   sparkSession.read.schema(schema).json("/path/to/file.json")
   * }}}
   *
   * @since 2.0.0
   */
  def read: DataFrameReader = new DataFrameReader(self)

  /**
   * Returns a `DataStreamReader` that can be used to read streaming data in as a `DataFrame`.
   * {{{
   *   sparkSession.readStream.parquet("/path/to/directory/of/parquet/files")
   *   sparkSession.readStream.schema(schema).json("/path/to/directory/of/json/files")
   * }}}
   *
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def readStream: DataStreamReader = new DataStreamReader(self)

  /**
   * Executes some code block and prints to stdout the time taken to execute the block. This is
   * available in Scala only and is used primarily for interactive testing and debugging.
   *
   * @since 2.1.0
   */
  def time[T](f: => T): T = {
    val start = System.nanoTime()
    val ret = f
    val end = System.nanoTime()
    // scalastyle:off println
    println(s"Time taken: ${(end - start) / 1000 / 1000} ms")
    // scalastyle:on println
    ret
  }

  // scalastyle:off
  // Disable style checker so "implicits" object can start with lowercase i
  /**
   * :: Experimental ::
   * (Scala-specific) Implicit methods available in Scala for converting
   * common Scala objects into `DataFrame`s.
   *
   * {{{
   *   val sparkSession = SparkSession.builder.getOrCreate()
   *   import sparkSession.implicits._
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  object implicits extends SQLImplicits with Serializable {
    protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext
  }
  // scalastyle:on

  /**
   * Stop the underlying `SparkContext`.
   *
   * @since 2.0.0
   */
  def stop(): Unit = {
    sparkContext.stop()
  }

  /**
   * Synonym for `stop()`.
   *
   * @since 2.1.0
   */
  override def close(): Unit = stop()

  /**
   * Parses the data type in our internal string representation. The data type string should
   * have the same format as the one generated by `toString` in scala.
   * It is only used by PySpark.
   */
  protected[sql] def parseDataType(dataTypeString: String): DataType = {
    DataType.fromJson(dataTypeString)
  }

  /**
   * Apply a schema defined by the schemaString to an RDD. It is only used by PySpark.
   */
  private[sql] def applySchemaToPythonRDD(
      rdd: RDD[Array[Any]],
      schemaString: String): DataFrame = {
    val schema = DataType.fromJson(schemaString).asInstanceOf[StructType]
    applySchemaToPythonRDD(rdd, schema)
  }

  /**
   * Apply `schema` to an RDD.
   *
   * @note Used by PySpark only
   */
  private[sql] def applySchemaToPythonRDD(
      rdd: RDD[Array[Any]],
      schema: StructType): DataFrame = {
    val rowRdd = rdd.mapPartitions { iter =>
      val fromJava = python.EvaluatePython.makeFromJava(schema)
      iter.map(r => fromJava(r).asInstanceOf[InternalRow])
    }
    internalCreateDataFrame(rowRdd, schema)
  }

  /**
   * Returns a Catalyst Schema for the given java bean class.
   */
  private def getSchema(beanClass: Class[_]): Seq[AttributeReference] = {
    val (dataType, _) = JavaTypeInference.inferDataType(beanClass)
    dataType.asInstanceOf[StructType].fields.map { f =>
      AttributeReference(f.name, f.dataType, f.nullable)()
    }
  }

}


@InterfaceStability.Stable
object SparkSession extends Logging {

  /**
   * Builder for [[SparkSession]].
   */
  @InterfaceStability.Stable
  class Builder extends Logging {

    private[this] val options = new scala.collection.mutable.HashMap[String, String]

    private[this] val extensions = new SparkSessionExtensions

    private[this] var userSuppliedContext: Option[SparkContext] = None

    private[spark] def sparkContext(sparkContext: SparkContext): Builder = synchronized {
      userSuppliedContext = Option(sparkContext)
      this
    }

    /**
     * Sets a name for the application, which will be shown in the Spark web UI.
     * If no application name is set, a randomly generated name will be used.
     *
     * @since 2.0.0
     */
    def appName(name: String): Builder = config("spark.app.name", name)

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: String): Builder = synchronized {
      options += key -> value
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Long): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Double): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Boolean): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a list of config options based on the given `SparkConf`.
     *
     * @since 2.0.0
     */
    def config(conf: SparkConf): Builder = synchronized {
      conf.getAll.foreach { case (k, v) => options += k -> v }
      this
    }

    /**
     * Sets the Spark master URL to connect to, such as "local" to run locally, "local[4]" to
     * run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone cluster.
     *
     * @since 2.0.0
     */
    def master(master: String): Builder = config("spark.master", master)

    /**
     * Enables Hive support, including connectivity to a persistent Hive metastore, support for
     * Hive serdes, and Hive user-defined functions.
     *
     * @since 2.0.0
     */
    def enableHiveSupport(): Builder = synchronized {
      if (hiveClassesArePresent) {
        config(CATALOG_IMPLEMENTATION.key, "hive")
      } else {
        throw new IllegalArgumentException(
          "Unable to instantiate SparkSession with Hive support because " +
            "Hive classes are not found.")
      }
    }

    /**
     * Inject extensions into the [[SparkSession]]. This allows a user to add Analyzer rules,
     * Optimizer rules, Planning Strategies or a customized parser.
     *
     * @since 2.2.0
     */
    def withExtensions(f: SparkSessionExtensions => Unit): Builder = synchronized {
      f(extensions)
      this
    }

    /**
     * Gets an existing [[SparkSession]] or, if there is no existing one, creates a new
     * one based on the options set in this builder.
     *
     * This method first checks whether there is a valid thread-local SparkSession,
     * and if yes, return that one. It then checks whether there is a valid global
     * default SparkSession, and if yes, return that one. If no valid global default
     * SparkSession exists, the method creates a new SparkSession and assigns the
     * newly created SparkSession as the global default.
     *
     * In case an existing SparkSession is returned, the non-static config options specified in
     * this builder will be applied to the existing SparkSession.
     *
     * @since 2.0.0
     */
    def getOrCreate(): SparkSession = synchronized {
      assertOnDriver()
      // Get the session from current thread's active session.
      var session = activeThreadSession.get()
      if ((session ne null) && !session.sparkContext.isStopped) {
        applyModifiableSettings(session)
        return session
      }

      // Global synchronization so we will only set the default session once.
      SparkSession.synchronized {
        // If the current thread does not have an active session, get it from the global session.
        session = defaultSession.get()
        if ((session ne null) && !session.sparkContext.isStopped) {
          applyModifiableSettings(session)
          return session
        }

        // No active nor global default session. Create a new one.
        val sparkContext = userSuppliedContext.getOrElse {
          val sparkConf = new SparkConf()
          options.foreach { case (k, v) => sparkConf.set(k, v) }

          // set a random app name if not given.
          if (!sparkConf.contains("spark.app.name")) {
            sparkConf.setAppName(java.util.UUID.randomUUID().toString)
          }

          SparkContext.getOrCreate(sparkConf)
          // Do not update `SparkConf` for existing `SparkContext`, as it's shared by all sessions.
        }

        // Initialize extensions if the user has defined a configurator class.
        val extensionConfOption = sparkContext.conf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS)
        if (extensionConfOption.isDefined) {
          val extensionConfClassName = extensionConfOption.get
          try {
            val extensionConfClass = Utils.classForName(extensionConfClassName)
            val extensionConf = extensionConfClass.newInstance()
              .asInstanceOf[SparkSessionExtensions => Unit]
            extensionConf(extensions)
          } catch {
            // Ignore the error if we cannot find the class or when the class has the wrong type.
            case e @ (_: ClassCastException |
                      _: ClassNotFoundException |
                      _: NoClassDefFoundError) =>
              logWarning(s"Cannot use $extensionConfClassName to configure session extensions.", e)
          }
        }

        session = new SparkSession(sparkContext, None, None, extensions)
        options.foreach { case (k, v) => session.initialSessionOptions.put(k, v) }
        setDefaultSession(session)
        setActiveSession(session)

        // Register a successfully instantiated context to the singleton. This should be at the
        // end of the class definition so that the singleton is updated only if there is no
        // exception in the construction of the instance.
        sparkContext.addSparkListener(new SparkListener {
          override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {
            defaultSession.set(null)
          }
        })
      }

      return session
    }

    private def applyModifiableSettings(session: SparkSession): Unit = {
      val (staticConfs, otherConfs) =
        options.partition(kv => SQLConf.staticConfKeys.contains(kv._1))

      otherConfs.foreach { case (k, v) => session.sessionState.conf.setConfString(k, v) }

      if (staticConfs.nonEmpty) {
        logWarning("Using an existing SparkSession; the static sql configurations will not take" +
          " effect.")
      }
      if (otherConfs.nonEmpty) {
        logWarning("Using an existing SparkSession; some spark core configurations may not take" +
          " effect.")
      }
    }
  }

  /**
   * Creates a [[SparkSession.Builder]] for constructing a [[SparkSession]].
   *
   * @since 2.0.0
   */
  def builder(): Builder = new Builder

  /**
   * Changes the SparkSession that will be returned in this thread and its children when
   * SparkSession.getOrCreate() is called. This can be used to ensure that a given thread receives
   * a SparkSession with an isolated session, instead of the global (first created) context.
   *
   * @since 2.0.0
   */
  def setActiveSession(session: SparkSession): Unit = {
    activeThreadSession.set(session)
  }

  /**
   * Clears the active SparkSession for current thread. Subsequent calls to getOrCreate will
   * return the first created context instead of a thread-local override.
   *
   * @since 2.0.0
   */
  def clearActiveSession(): Unit = {
    activeThreadSession.remove()
  }

  /**
   * Sets the default SparkSession that is returned by the builder.
   *
   * @since 2.0.0
   */
  def setDefaultSession(session: SparkSession): Unit = {
    defaultSession.set(session)
  }

  /**
   * Clears the default SparkSession that is returned by the builder.
   *
   * @since 2.0.0
   */
  def clearDefaultSession(): Unit = {
    defaultSession.set(null)
  }

  /**
   * Returns the active SparkSession for the current thread, returned by the builder.
   *
   * @note Return None, when calling this function on executors
   *
   * @since 2.2.0
   */
  def getActiveSession: Option[SparkSession] = {
    if (TaskContext.get != null) {
      // Return None when running on executors.
      None
    } else {
      Option(activeThreadSession.get)
    }
  }

  /**
   * Returns the default SparkSession that is returned by the builder.
   *
   * @note Return None, when calling this function on executors
   *
   * @since 2.2.0
   */
  def getDefaultSession: Option[SparkSession] = {
    if (TaskContext.get != null) {
      // Return None when running on executors.
      None
    } else {
      Option(defaultSession.get)
    }
  }

  /**
   * Returns the currently active SparkSession, otherwise the default one. If there is no default
   * SparkSession, throws an exception.
   *
   * @since 2.4.0
   */
  def active: SparkSession = {
    getActiveSession.getOrElse(getDefaultSession.getOrElse(
      throw new IllegalStateException("No active or default Spark session found")))
  }

  ////////////////////////////////////////////////////////////////////////////////////////
  // Private methods from now on
  ////////////////////////////////////////////////////////////////////////////////////////

  /** The active SparkSession for the current thread. */
  private val activeThreadSession = new InheritableThreadLocal[SparkSession]

  /** Reference to the root SparkSession. */
  private val defaultSession = new AtomicReference[SparkSession]

  private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME =
    "org.apache.spark.sql.hive.HiveSessionStateBuilder"

  private def sessionStateClassName(conf: SparkConf): String = {
    conf.get(CATALOG_IMPLEMENTATION) match {
      case "hive" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME
      case "in-memory" => classOf[SessionStateBuilder].getCanonicalName
    }
  }

  private def assertOnDriver(): Unit = {
    if (Utils.isTesting && TaskContext.get != null) {
      // we're accessing it during task execution, fail.
      throw new IllegalStateException(
        "SparkSession should only be created and accessed on the driver.")
    }
  }

  /**
   * Helper method to create an instance of `SessionState` based on `className` from conf.
   * The result is either `SessionState` or a Hive based `SessionState`.
   */
  private def instantiateSessionState(
      className: String,
      sparkSession: SparkSession): SessionState = {
    try {
      // invoke `new [Hive]SessionStateBuilder(SparkSession, Option[SessionState])`
      val clazz = Utils.classForName(className)
      val ctor = clazz.getConstructors.head
      ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build()
    } catch {
      case NonFatal(e) =>
        throw new IllegalArgumentException(s"Error while instantiating '$className':", e)
    }
  }

  /**
   * @return true if Hive classes can be loaded, otherwise false.
   */
  private[spark] def hiveClassesArePresent: Boolean = {
    try {
      Utils.classForName(HIVE_SESSION_STATE_BUILDER_CLASS_NAME)
      Utils.classForName("org.apache.hadoop.hive.conf.HiveConf")
      true
    } catch {
      case _: ClassNotFoundException | _: NoClassDefFoundError => false
    }
  }

  private[spark] def cleanupAnyExistingSession(): Unit = {
    val session = getActiveSession.orElse(getDefaultSession)
    if (session.isDefined) {
      logWarning(
        s"""An existing Spark session exists as the active or default session.
           |This probably means another suite leaked it. Attempting to stop it before continuing.
           |This existing Spark session was created at:
           |
           |${session.get.creationSite.longForm}
           |
         """.stripMargin)
      session.get.stop()
      SparkSession.clearActiveSession()
      SparkSession.clearDefaultSession()
    }
  }
}

[0m2021.03.04 14:27:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:27:21 INFO  time: compiled root in 0.94s[0m
[0m2021.03.04 14:28:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:28:15 INFO  time: compiled root in 0.82s[0m
[0m2021.03.04 14:30:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:30:03 INFO  time: compiled root in 0.81s[0m
[0m2021.03.04 14:30:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:30:14 INFO  time: compiled root in 0.82s[0m
[0m2021.03.04 14:30:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:30:46 INFO  time: compiled root in 0.76s[0m
[0m2021.03.04 14:31:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:31:34 INFO  time: compiled root in 0.9s[0m
[0m2021.03.04 14:32:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:32:26 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 14:32:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:32:34 INFO  time: compiled root in 0.2s[0m
[0m2021.03.04 14:32:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:32:44 INFO  time: compiled root in 0.21s[0m
[0m2021.03.04 14:34:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:34:28 INFO  time: compiled root in 1.19s[0m
[0m2021.03.04 14:34:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:34:53 INFO  time: compiled root in 1.47s[0m
[0m2021.03.04 14:36:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:36:30 INFO  time: compiled root in 0.8s[0m
[0m2021.03.04 14:37:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:37:13 INFO  time: compiled root in 0.77s[0m
[0m2021.03.04 14:37:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:37:15 INFO  time: compiled root in 0.76s[0m
[0m2021.03.04 14:38:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:38:07 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 14:39:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:39:00 INFO  time: compiled root in 0.82s[0m
[0m2021.03.04 14:41:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:41:17 INFO  time: compiled root in 0.81s[0m
[0m2021.03.04 14:41:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:41:50 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 14:42:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:42:30 INFO  time: compiled root in 0.19s[0m
[0m2021.03.04 14:42:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:42:54 INFO  time: compiled root in 0.79s[0m
[0m2021.03.04 14:43:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:43:48 INFO  time: compiled root in 0.93s[0m
[0m2021.03.04 14:44:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:44:23 INFO  time: compiled root in 0.86s[0m
[0m2021.03.04 14:45:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:45:57 INFO  time: compiled root in 0.8s[0m
[0m2021.03.04 14:46:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:46:18 INFO  time: compiled root in 0.94s[0m
[0m2021.03.04 14:48:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:48:01 INFO  time: compiled root in 0.83s[0m
/*
 * Scala (https://www.scala-lang.org)
 *
 * Copyright EPFL and Lightbend, Inc.
 *
 * Licensed under Apache License 2.0
 * (http://www.apache.org/licenses/LICENSE-2.0).
 *
 * See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.
 */

package scala
package io

import scala.collection.AbstractIterator
import java.io.{ FileInputStream, InputStream, PrintStream, File => JFile, Closeable }
import java.net.{ URI, URL }

/** This object provides convenience methods to create an iterable
 *  representation of a source file.
 *
 *  @author  Burak Emir, Paul Phillips
 */
object Source {
  val DefaultBufSize = 2048

  /** Creates a `Source` from System.in.
   */
  def stdin = fromInputStream(System.in)

  /** Creates a Source from an Iterable.
   *
   *  @param    iterable  the Iterable
   *  @return   the Source
   */
  def fromIterable(iterable: Iterable[Char]): Source = new Source {
    val iter = iterable.iterator
  } withReset(() => fromIterable(iterable))

  /** Creates a Source instance from a single character.
   */
  def fromChar(c: Char): Source = fromIterable(Array(c))

  /** creates Source from array of characters, with empty description.
   */
  def fromChars(chars: Array[Char]): Source = fromIterable(chars)

  /** creates Source from a String, with no description.
   */
  def fromString(s: String): Source = fromIterable(s)

  /** creates Source from file with given name, setting its description to
   *  filename.
   */
  def fromFile(name: String)(implicit codec: Codec): BufferedSource =
    fromFile(new JFile(name))(codec)

  /** creates Source from file with given name, using given encoding, setting
   *  its description to filename.
   */
  def fromFile(name: String, enc: String): BufferedSource =
    fromFile(name)(Codec(enc))

  /** creates `source` from file with given file `URI`.
   */
  def fromFile(uri: URI)(implicit codec: Codec): BufferedSource =
    fromFile(new JFile(uri))(codec)

  /** creates Source from file with given file: URI
   */
  def fromFile(uri: URI, enc: String): BufferedSource =
    fromFile(uri)(Codec(enc))

  /** creates Source from file, using default character encoding, setting its
   *  description to filename.
   */
  def fromFile(file: JFile)(implicit codec: Codec): BufferedSource =
    fromFile(file, Source.DefaultBufSize)(codec)

  /** same as fromFile(file, enc, Source.DefaultBufSize)
   */
  def fromFile(file: JFile, enc: String): BufferedSource =
    fromFile(file)(Codec(enc))

  def fromFile(file: JFile, enc: String, bufferSize: Int): BufferedSource =
    fromFile(file, bufferSize)(Codec(enc))

  /** Creates Source from `file`, using given character encoding, setting
   *  its description to filename. Input is buffered in a buffer of size
   *  `bufferSize`.
   */
  def fromFile(file: JFile, bufferSize: Int)(implicit codec: Codec): BufferedSource = {
    val inputStream = new FileInputStream(file)

    createBufferedSource(
      inputStream,
      bufferSize,
      () => fromFile(file, bufferSize)(codec),
      () => inputStream.close()
    )(codec) withDescription ("file:" + file.getAbsolutePath)
  }

  /** Create a `Source` from array of bytes, decoding
   *  the bytes according to codec.
   *
   *  @return      the created `Source` instance.
   */
  def fromBytes(bytes: Array[Byte])(implicit codec: Codec): Source =
    fromString(new String(bytes, codec.name))

  def fromBytes(bytes: Array[Byte], enc: String): Source =
    fromBytes(bytes)(Codec(enc))

  /** Create a `Source` from array of bytes, assuming
   *  one byte per character (ISO-8859-1 encoding.)
   */
  def fromRawBytes(bytes: Array[Byte]): Source =
    fromString(new String(bytes, Codec.ISO8859.name))

  /** creates `Source` from file with given file: URI
   */
  def fromURI(uri: URI)(implicit codec: Codec): BufferedSource =
    fromFile(new JFile(uri))(codec)

  /** same as fromURL(new URL(s))(Codec(enc))
   */
  def fromURL(s: String, enc: String): BufferedSource =
    fromURL(s)(Codec(enc))

  /** same as fromURL(new URL(s))
   */
  def fromURL(s: String)(implicit codec: Codec): BufferedSource =
    fromURL(new URL(s))(codec)

  /** same as fromInputStream(url.openStream())(Codec(enc))
   */
  def fromURL(url: URL, enc: String): BufferedSource =
    fromURL(url)(Codec(enc))

  /** same as fromInputStream(url.openStream())(codec)
   */
  def fromURL(url: URL)(implicit codec: Codec): BufferedSource =
    fromInputStream(url.openStream())(codec)

  /** Reads data from inputStream with a buffered reader, using the encoding
   *  in implicit parameter codec.
   *
   *  @param  inputStream  the input stream from which to read
   *  @param  bufferSize   buffer size (defaults to Source.DefaultBufSize)
   *  @param  reset        a () => Source which resets the stream (if unset, reset() will throw an Exception)
   *  @param  close        a () => Unit method which closes the stream (if unset, close() will do nothing)
   *  @param  codec        (implicit) a scala.io.Codec specifying behavior (defaults to Codec.default)
   *  @return              the buffered source
   */
  def createBufferedSource(
    inputStream: InputStream,
    bufferSize: Int = DefaultBufSize,
    reset: () => Source = null,
    close: () => Unit = null
  )(implicit codec: Codec): BufferedSource = {
    // workaround for default arguments being unable to refer to other parameters
    val resetFn = if (reset == null) () => createBufferedSource(inputStream, bufferSize, reset, close)(codec) else reset

    new BufferedSource(inputStream, bufferSize)(codec) withReset resetFn withClose close
  }

  def fromInputStream(is: InputStream, enc: String): BufferedSource =
    fromInputStream(is)(Codec(enc))

  def fromInputStream(is: InputStream)(implicit codec: Codec): BufferedSource =
    createBufferedSource(is, reset = () => fromInputStream(is)(codec), close = () => is.close())(codec)

  /** Reads data from a classpath resource, using either a context classloader (default) or a passed one.
   *
   *  @param  resource     name of the resource to load from the classpath
   *  @param  classLoader  classloader to be used, or context classloader if not specified
   *  @return              the buffered source
   */
  def fromResource(resource: String, classLoader: ClassLoader = Thread.currentThread().getContextClassLoader())(implicit codec: Codec): BufferedSource =
    fromInputStream(classLoader.getResourceAsStream(resource))

}

/** An iterable representation of source data.
 *  It may be reset with the optional [[reset]] method.
 *
 *  Subclasses must supply [[scala.io.Source.iter the underlying iterator]].
 *
 *  Error handling may be customized by overriding the [[scala.io.Source.report report]] method.
 *
 *  The [[scala.io.Source.ch current input]] and [[scala.io.Source.pos position]],
 *  as well as the [[scala.io.Source.next next character]] methods delegate to
 *  [[scala.io.Source#Positioner the positioner]].
 *
 *  The default positioner encodes line and column numbers in the position passed to [[report]].
 *  This behavior can be changed by supplying a
 *  [[scala.io.Source.withPositioning(pos:* custom positioner]].
 *
 */
abstract class Source extends Iterator[Char] with Closeable {
  /** the actual iterator */
  protected val iter: Iterator[Char]

  // ------ public values

  /** description of this source, default empty */
  var descr: String = ""
  var nerrors = 0
  var nwarnings = 0

  private def lineNum(line: Int): String = (getLines() drop (line - 1) take 1).mkString

  class LineIterator extends AbstractIterator[String] with Iterator[String] {
    private[this] val sb = new StringBuilder

    lazy val iter: BufferedIterator[Char] = Source.this.iter.buffered
    def isNewline(ch: Char) = ch == '\r' || ch == '\n'
    def getc() = iter.hasNext && {
      val ch = iter.next()
      if (ch == '\n') false
      else if (ch == '\r') {
        if (iter.hasNext && iter.head == '\n')
          iter.next()

        false
      }
      else {
        sb append ch
        true
      }
    }
    def hasNext = iter.hasNext
    def next = {
      sb.clear()
      while (getc()) { }
      sb.toString
    }
  }

  /** Returns an iterator who returns lines (NOT including newline character(s)).
   *  It will treat any of \r\n, \r, or \n as a line separator (longest match) - if
   *  you need more refined behavior you can subclass Source#LineIterator directly.
   */
  def getLines(): Iterator[String] = new LineIterator()

  /** Returns `'''true'''` if this source has more characters.
   */
  def hasNext = iter.hasNext

  /** Returns next character.
   */
  def next(): Char = positioner.next()

  class Positioner(encoder: Position) {
    def this() = this(RelaxedPosition)
    /** the last character returned by next. */
    var ch: Char = _

    /** position of last character returned by next */
    var pos = 0

    /** current line and column */
    var cline = 1
    var ccol = 1

    /** default col increment for tabs '\t', set to 4 initially */
    var tabinc = 4

    def next(): Char = {
      ch = iter.next()
      pos = encoder.encode(cline, ccol)
      ch match {
        case '\n' =>
          ccol = 1
          cline += 1
        case '\t' =>
          ccol += tabinc
        case _ =>
          ccol += 1
      }
      ch
    }
  }
  /** A Position implementation which ignores errors in
   *  the positions.
   */
  object RelaxedPosition extends Position {
    def checkInput(line: Int, column: Int): Unit = ()
  }
  object RelaxedPositioner extends Positioner(RelaxedPosition) { }
  object NoPositioner extends Positioner(Position) {
    override def next(): Char = iter.next()
  }
  def ch = positioner.ch
  def pos = positioner.pos

  /** Reports an error message to the output stream `out`.
   *
   *  @param pos the source position (line/column)
   *  @param msg the error message to report
   *  @param out PrintStream to use (optional: defaults to `Console.err`)
   */
  def reportError(
    pos: Int,
    msg: String,
    out: PrintStream = Console.err)
  {
    nerrors += 1
    report(pos, msg, out)
  }

  private def spaces(n: Int) = List.fill(n)(' ').mkString
  /**
   *  @param pos the source position (line/column)
   *  @param msg the error message to report
   *  @param out PrintStream to use
   */
  def report(pos: Int, msg: String, out: PrintStream) {
    val line  = Position line pos
    val col   = Position column pos

    out println "%s:%d:%d: %s%s%s^".format(descr, line, col, msg, lineNum(line), spaces(col - 1))
  }

  /**
   *  @param pos the source position (line/column)
   *  @param msg the warning message to report
   *  @param out PrintStream to use (optional: defaults to `Console.out`)
   */
  def reportWarning(
    pos: Int,
    msg: String,
    out: PrintStream = Console.out)
  {
    nwarnings += 1
    report(pos, "warning! " + msg, out)
  }

  private[this] var resetFunction: () => Source = null
  private[this] var closeFunction: () => Unit = null
  private[this] var positioner: Positioner = RelaxedPositioner

  def withReset(f: () => Source): this.type = {
    resetFunction = f
    this
  }
  def withClose(f: () => Unit): this.type = {
    closeFunction = f
    this
  }
  def withDescription(text: String): this.type = {
    descr = text
    this
  }
  /** Change or disable the positioner. */
  def withPositioning(on: Boolean): this.type = {
    positioner = if (on) RelaxedPositioner else NoPositioner
    this
  }
  def withPositioning(pos: Positioner): this.type = {
    positioner = pos
    this
  }

  /** The close() method closes the underlying resource. */
  def close() {
    if (closeFunction != null) closeFunction()
  }

  /** The reset() method creates a fresh copy of this Source. */
  def reset(): Source =
    if (resetFunction != null) resetFunction()
    else throw new UnsupportedOperationException("Source's reset() method was not set.")
}

[0m2021.03.04 14:49:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:49:39 INFO  time: compiled root in 0.86s[0m
[0m2021.03.04 14:50:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:50:24 INFO  time: compiled root in 0.82s[0m
[0m2021.03.04 14:56:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:56:48 INFO  time: compiled root in 0.83s[0m
[0m2021.03.04 14:57:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:57:04 INFO  time: compiled root in 0.88s[0m
[0m2021.03.04 14:57:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:57:10 INFO  time: compiled root in 0.8s[0m
[0m2021.03.04 14:57:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:57:16 INFO  time: compiled root in 0.79s[0m
[0m2021.03.04 14:58:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:58:20 INFO  time: compiled root in 0.79s[0m
[0m2021.03.04 15:18:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:18:46 INFO  time: compiled root in 0.88s[0m
[0m2021.03.04 15:19:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:19:12 INFO  time: compiled root in 0.88s[0m
[0m2021.03.04 15:19:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:19:21 INFO  time: compiled root in 0.84s[0m
[0m2021.03.04 15:19:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:19:33 INFO  time: compiled root in 0.82s[0m
[0m2021.03.04 15:20:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:20:21 INFO  time: compiled root in 0.86s[0m
[0m2021.03.04 15:23:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:23:21 INFO  time: compiled root in 0.85s[0m
[0m2021.03.04 15:24:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:24:02 INFO  time: compiled root in 0.16s[0m
[0m2021.03.04 15:24:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:24:26 INFO  time: compiled root in 0.81s[0m
[0m2021.03.04 15:27:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:27:17 INFO  time: compiled root in 0.87s[0m
[0m2021.03.04 15:27:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:27:25 INFO  time: compiled root in 0.89s[0m
[0m2021.03.04 15:27:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:27:56 INFO  time: compiled root in 0.9s[0m
[0m2021.03.04 15:28:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:28:50 INFO  time: compiled root in 0.21s[0m
[0m2021.03.04 15:31:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:31:28 INFO  time: compiled root in 0.2s[0m
[0m2021.03.04 15:31:41 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:61: error: illegal start of simple expression
            val bucket2 = spark.read.textFile(s"s3a://commoncrawl/$path")
            ^[0m
[0m2021.03.04 15:31:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:31:45 INFO  time: compiled root in 0.18s[0m
[0m2021.03.04 15:32:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:32:17 INFO  time: compiled root in 0.17s[0m
[0m2021.03.04 15:32:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:32:32 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 15:33:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:33:14 INFO  time: compiled root in 0.18s[0m
[0m2021.03.04 15:33:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:33:24 INFO  time: compiled root in 0.86s[0m
[0m2021.03.04 15:34:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:34:19 INFO  time: compiled root in 0.21s[0m
[0m2021.03.04 15:35:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:35:13 INFO  time: compiled root in 0.17s[0m
[0m2021.03.04 15:35:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:35:20 INFO  time: compiled root in 0.21s[0m
[0m2021.03.04 15:35:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:35:37 INFO  time: compiled root in 0.23s[0m
[0m2021.03.04 15:35:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:35:46 INFO  time: compiled root in 0.78s[0m
[0m2021.03.04 15:36:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:36:57 INFO  time: compiled root in 0.98s[0m
[0m2021.03.04 15:37:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:37:33 INFO  time: compiled root in 0.81s[0m
[0m2021.03.04 15:37:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:37:57 INFO  time: compiled root in 0.86s[0m
[0m2021.03.04 15:38:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:38:31 INFO  time: compiled root in 0.92s[0m
[0m2021.03.04 15:40:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:40:36 INFO  time: compiled root in 0.23s[0m
[0m2021.03.04 15:40:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:40:43 INFO  time: compiled root in 0.19s[0m
[0m2021.03.04 15:41:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:41:40 INFO  time: compiled root in 0.18s[0m
[0m2021.03.04 15:41:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:41:44 INFO  time: compiled root in 0.88s[0m
[0m2021.03.04 15:42:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:42:49 INFO  time: compiled root in 0.91s[0m
[0m2021.03.04 15:44:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:44:16 INFO  time: compiled root in 1.23s[0m
[0m2021.03.04 15:44:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:44:51 INFO  time: compiled root in 0.27s[0m
[0m2021.03.04 15:45:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:45:14 INFO  time: compiled root in 1.76s[0m
[0m2021.03.04 15:45:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:45:21 INFO  time: compiled root in 0.79s[0m
[0m2021.03.04 15:46:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:46:42 INFO  time: compiled root in 0.78s[0m
[0m2021.03.04 15:46:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:46:52 INFO  time: compiled root in 0.81s[0m
Mar 04, 2021 3:47:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12373
[0m2021.03.04 15:47:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:47:48 INFO  time: compiled root in 0.81s[0m
[0m2021.03.04 15:52:22 INFO  shutting down Metals[0m
[0m2021.03.04 15:52:22 INFO  Shut down connection with build server.[0m
[0m2021.03.04 15:52:22 INFO  Shut down connection with build server.[0m
[0m2021.03.04 15:52:22 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.05 09:08:06 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.54.0.[0m
[0m2021.03.05 09:08:07 INFO  time: initialize in 0.46s[0m
[0m2021.03.05 09:08:07 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.05 09:08:07 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher485213406661261175/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.05 09:08:07 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.05 09:08:10 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.DataFrame
import java.io.File
import scala.io.Source

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?

//wet file paths
    //"s3a://commoncrawl/crawl-data/...../wet.paths.gz"

    //Read a whole years worth of data - Jan-Dec 2020
    val monthArray = Array(
      "05",
      "10",
      "16",
      "24",
      "29",
      "34",
      "40",
      "45",
      "50"
    )
    for (num <- monthArray) {
      println(num)
      spark.read
        //access a months worth of wet segment paths
        .textFile(
          s"s3a://commoncrawl/crawl-data/CC-MAIN-2020-$num/wet.paths.gz"
        )
        .foreach(path => {
          println(path)
          //read the individual segment path for the data
          spark.read
            .textFile(s"s3a://commoncrawl/$path")
            .foreach(line => {
              //split the WARC info from the plain text
              val col = line.split("\n").map(_.trim())
              val df = spark.read
                .option("lineSep", "WARC/1.0")
                .text(line)
                .as[String]
                .map((str) => { str.substring(str.indexOf("\n") + 1) })
                .toDF("cut WET")
              commonCrawl(df, spark)
            })

        })

    }

    def commonCrawl(df: DataFrame, spark: SparkSession) = {
      import spark.implicits._
      val cuttingCrawl = df
        .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
        .select(
          $"_tmp".getItem(0).as("WARC Header"),
          $"_tmp".getItem(1).as("Plain Text")
        )

      //find job urls
      val techJob = cuttingCrawl
        .filter(
          $"WARC Header" rlike ".*WARC-Target-URI:.*career.*" or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*")
        )
        //filter for tech ads
        .filter(
          $"Plain Text" rlike ".*Frontend.*" or ($"Plain Text" rlike ".*Backendend.*") or ($"Plain Text" rlike ".*Fullstack.*")
            or ($"Plain Text" rlike ".*Cybersecurity.*") or ($"Plain Text" rlike ".*Software.*") or ($"Plain Text" rlike ".*Computer.*")
        )
      techJob.show(false)
    }
  }
}

No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher485213406661261175/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher485213406661261175/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 09:08:12 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 09:08:12 INFO  time: code lens generation in 3.52s[0m
[0m2021.03.05 09:08:12 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0mOpening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2091694223909938861/bsp.socket'...
2021.03.05 09:08:12 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7865960295071720713/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2091694223909938861/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2091694223909938861/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 09:08:12 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7865960295071720713/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7865960295071720713/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 09:08:12 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 09:08:12 INFO  time: Connected to build server in 4.93s[0m
[0m2021.03.05 09:08:12 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.05 09:08:12 INFO  time: Imported build in 0.23s[0m
[0m2021.03.05 09:08:14 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.05 09:08:14 INFO  time: indexed workspace in 2.5s[0m
[0m2021.03.05 09:14:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:14:53 INFO  time: compiled root in 5.24s[0m
[0m2021.03.05 09:14:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:14:56 INFO  time: compiled root in 1.75s[0m
[0m2021.03.05 09:15:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:15:10 INFO  time: compiled root in 1.84s[0m
[0m2021.03.05 09:15:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:15:17 INFO  time: compiled root in 1.45s[0m
[0m2021.03.05 09:15:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:15:24 INFO  time: compiled root in 1.12s[0m
[0m2021.03.05 09:18:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:18:23 INFO  time: compiled root in 1.19s[0m
[0m2021.03.05 09:18:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:18:37 INFO  time: compiled root in 0.41s[0m
[0m2021.03.05 09:18:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:18:47 INFO  time: compiled root in 1.04s[0m
[0m2021.03.05 09:19:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:19:13 INFO  time: compiled root in 1.08s[0m
Mar 05, 2021 9:38:28 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 542
[0m2021.03.05 09:38:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:38:48 INFO  time: compiled root in 0.18s[0m
[0m2021.03.05 09:38:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:38:53 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 09:39:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:39:04 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 09:39:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:39:07 INFO  time: compiled root in 0.11s[0m
[0m2021.03.05 09:39:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:39:35 INFO  time: compiled root in 0.11s[0m
[0m2021.03.05 09:39:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:39:50 INFO  time: compiled root in 0.21s[0m
[0m2021.03.05 09:40:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:40:04 INFO  time: compiled root in 0.18s[0m
[0m2021.03.05 09:40:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:40:19 INFO  time: compiled root in 0.78s[0m
[0m2021.03.05 09:41:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:41:01 INFO  time: compiled root in 0.18s[0m
[0m2021.03.05 09:41:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:102:21: stale bloop error: not found: value cuttingCrawl
      val techJob = cuttingCrawl
                    ^^^^^^^^^^^^[0m
[0m2021.03.05 09:41:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:102:21: stale bloop error: not found: value cuttingCrawl
      val techJob = cuttingCrawl
                    ^^^^^^^^^^^^[0m
[0m2021.03.05 09:41:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:41:28 INFO  time: compiled root in 0.98s[0m
[0m2021.03.05 09:45:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:45:20 INFO  time: compiled root in 0.13s[0m
[0m2021.03.05 09:46:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:46:51 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 09:47:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:47:15 INFO  time: compiled root in 0.33s[0m
[0m2021.03.05 09:48:00 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:79: error: ; expected but . found
            .foreach(line => {
            ^[0m
[0m2021.03.05 09:48:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:48:01 INFO  time: compiled root in 0.11s[0m
[0m2021.03.05 09:48:02 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:79: error: ; expected but . found
            .foreach(line => {
            ^[0m
[0m2021.03.05 09:48:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:48:14 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 09:49:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:49:10 INFO  time: compiled root in 0.14s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.03.05 09:50:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:50:17 INFO  time: compiled root in 1.22s[0m
[0m2021.03.05 09:50:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:50:24 INFO  time: compiled root in 1.01s[0m
[0m2021.03.05 10:12:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:12:46 INFO  time: compiled root in 1.01s[0m
[0m2021.03.05 10:24:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:24:36 INFO  time: compiled root in 0.23s[0m
[0m2021.03.05 10:24:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:54: stale bloop error: not found: value num
        s"s3a://commoncrawl/crawl-data/CC-MAIN-2020-$num/wet.paths.gz"
                                                     ^^^[0m
[0m2021.03.05 10:24:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:59:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 10:24:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:63:19: stale bloop error: not found: value fullPath1
        .textFile(fullPath1)
                  ^^^^^^^^^[0m
[0m2021.03.05 10:24:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:58:54: stale bloop error: not found: value num
        s"s3a://commoncrawl/crawl-data/CC-MAIN-2020-$num/wet.paths.gz"
                                                     ^^^[0m
[0m2021.03.05 10:24:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:59:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 10:24:45 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:63:19: stale bloop error: not found: value fullPath1
        .textFile(fullPath1)
                  ^^^^^^^^^[0m
[0m2021.03.05 10:24:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:24:47 INFO  time: compiled root in 0.86s[0m
[0m2021.03.05 10:28:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:28:04 INFO  time: compiled root in 1.08s[0m
[0m2021.03.05 12:01:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:01:24 INFO  time: compiled root in 0.91s[0m
[0m2021.03.05 12:21:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:21:29 INFO  time: compiled root in 1.33s[0m
[0m2021.03.05 12:22:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:22:25 INFO  time: compiled root in 0.35s[0m
[0m2021.03.05 12:22:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:22:44 INFO  time: compiled root in 1.86s[0m
[0m2021.03.05 12:28:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:28:11 INFO  time: compiled root in 1.49s[0m
[0m2021.03.05 12:30:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:30:54 INFO  time: compiled root in 1.5s[0m
[0m2021.03.05 12:33:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:33:30 INFO  time: compiled root in 1.35s[0m
[0m2021.03.05 12:34:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:34:03 INFO  time: compiled root in 1.31s[0m
[0m2021.03.05 12:34:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:34:05 INFO  time: compiled root in 1.33s[0m
[0m2021.03.05 12:34:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:34:59 INFO  time: compiled root in 1.18s[0m
[0m2021.03.05 12:35:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:35:32 INFO  time: compiled root in 1.19s[0m
[0m2021.03.05 12:37:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:37:22 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 12:37:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:37:28 INFO  time: compiled root in 0.11s[0m
[0m2021.03.05 12:37:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:37:34 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 12:37:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:37:51 INFO  time: compiled root in 1.7s[0m
[0m2021.03.05 12:38:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:38:57 INFO  time: compiled root in 1.26s[0m
[0m2021.03.05 12:39:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:39:02 INFO  time: compiled root in 1.15s[0m
[0m2021.03.05 12:39:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:39:37 INFO  time: compiled root in 1.18s[0m
[0m2021.03.05 12:40:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:40:18 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 12:40:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:40:23 INFO  time: compiled root in 1.19s[0m
[0m2021.03.05 12:40:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:40:30 INFO  time: compiled root in 1.1s[0m
[0m2021.03.05 12:41:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:41:02 INFO  time: compiled root in 1.25s[0m
[0m2021.03.05 12:42:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:42:17 INFO  time: compiled root in 1.2s[0m
Mar 05, 2021 12:42:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2301
[0m2021.03.05 12:42:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:42:24 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 12:42:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:42:25 INFO  time: compiled root in 0.38s[0m
[0m2021.03.05 12:42:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:42:41 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 12:44:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:44:09 INFO  time: compiled root in 1.3s[0m
[0m2021.03.05 12:45:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:45:31 INFO  time: compiled root in 1.24s[0m
[0m2021.03.05 12:48:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:48:14 INFO  time: compiled root in 0.3s[0m
[0m2021.03.05 12:48:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:125:52: stale bloop error: not found: value num
      s"s3a://commoncrawl/crawl-data/CC-MAIN-2020-$num/wet/paths.gz"
                                                   ^^^[0m
[0m2021.03.05 12:48:22 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:125:52: stale bloop error: not found: value num
      s"s3a://commoncrawl/crawl-data/CC-MAIN-2020-$num/wet/paths.gz"
                                                   ^^^[0m
[0m2021.03.05 12:48:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:48:30 INFO  time: compiled root in 1.1s[0m
[0m2021.03.05 12:50:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:50:17 INFO  time: compiled root in 1.18s[0m
[0m2021.03.05 12:50:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:50:26 INFO  time: compiled root in 1.43s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.util.{Locale, Properties}

import scala.collection.JavaConverters._

import com.fasterxml.jackson.databind.ObjectMapper
import com.univocity.parsers.csv.CsvParser

import org.apache.spark.Partition
import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.json.{CreateJacksonParser, JacksonParser, JSONOptions}
import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap
import org.apache.spark.sql.execution.command.DDLUtils
import org.apache.spark.sql.execution.datasources.{DataSource, FailureSafeParser}
import org.apache.spark.sql.execution.datasources.csv._
import org.apache.spark.sql.execution.datasources.jdbc._
import org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource
import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation
import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils
import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport}
import org.apache.spark.sql.types.{StringType, StructType}
import org.apache.spark.unsafe.types.UTF8String

/**
 * Interface used to load a [[Dataset]] from external storage systems (e.g. file systems,
 * key-value stores, etc). Use `SparkSession.read` to access this.
 *
 * @since 1.4.0
 */
@InterfaceStability.Stable
class DataFrameReader private[sql](sparkSession: SparkSession) extends Logging {

  /**
   * Specifies the input data source format.
   *
   * @since 1.4.0
   */
  def format(source: String): DataFrameReader = {
    this.source = source
    this
  }

  /**
   * Specifies the input schema. Some data sources (e.g. JSON) can infer the input schema
   * automatically from data. By specifying the schema here, the underlying data source can
   * skip the schema inference step, and thus speed up data loading.
   *
   * @since 1.4.0
   */
  def schema(schema: StructType): DataFrameReader = {
    this.userSpecifiedSchema = Option(schema)
    this
  }

  /**
   * Specifies the schema by using the input DDL-formatted string. Some data sources (e.g. JSON) can
   * infer the input schema automatically from data. By specifying the schema here, the underlying
   * data source can skip the schema inference step, and thus speed up data loading.
   *
   * {{{
   *   spark.read.schema("a INT, b STRING, c DOUBLE").csv("test.csv")
   * }}}
   *
   * @since 2.3.0
   */
  def schema(schemaString: String): DataFrameReader = {
    this.userSpecifiedSchema = Option(StructType.fromDDL(schemaString))
    this
  }

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def option(key: String, value: String): DataFrameReader = {
    this.extraOptions += (key -> value)
    this
  }

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Boolean): DataFrameReader = option(key, value.toString)

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Long): DataFrameReader = option(key, value.toString)

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Double): DataFrameReader = option(key, value.toString)

  /**
   * (Scala-specific) Adds input options for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def options(options: scala.collection.Map[String, String]): DataFrameReader = {
    this.extraOptions ++= options
    this
  }

  /**
   * Adds input options for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def options(options: java.util.Map[String, String]): DataFrameReader = {
    this.options(options.asScala)
    this
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that don't require a path (e.g. external
   * key-value stores).
   *
   * @since 1.4.0
   */
  def load(): DataFrame = {
    load(Seq.empty: _*) // force invocation of `load(...varargs...)`
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that require a path (e.g. data backed by
   * a local or distributed file system).
   *
   * @since 1.4.0
   */
  def load(path: String): DataFrame = {
    // force invocation of `load(...varargs...)`
    option(DataSourceOptions.PATH_KEY, path).load(Seq.empty: _*)
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that support multiple paths.
   * Only works if the source is a HadoopFsRelationProvider.
   *
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def load(paths: String*): DataFrame = {
    if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) {
      throw new AnalysisException("Hive data source can only be used with tables, you can not " +
        "read files of Hive data source directly.")
    }

    val cls = DataSource.lookupDataSource(source, sparkSession.sessionState.conf)
    if (classOf[DataSourceV2].isAssignableFrom(cls)) {
      val ds = cls.newInstance().asInstanceOf[DataSourceV2]
      if (ds.isInstanceOf[ReadSupport]) {
        val sessionOptions = DataSourceV2Utils.extractSessionConfigs(
          ds = ds, conf = sparkSession.sessionState.conf)
        val pathsOption = {
          val objectMapper = new ObjectMapper()
          DataSourceOptions.PATHS_KEY -> objectMapper.writeValueAsString(paths.toArray)
        }
        Dataset.ofRows(sparkSession, DataSourceV2Relation.create(
          ds, sessionOptions ++ extraOptions.toMap + pathsOption,
          userSpecifiedSchema = userSpecifiedSchema))
      } else {
        loadV1Source(paths: _*)
      }
    } else {
      loadV1Source(paths: _*)
    }
  }

  private def loadV1Source(paths: String*) = {
    // Code path for data source v1.
    sparkSession.baseRelationToDataFrame(
      DataSource.apply(
        sparkSession,
        paths = paths,
        userSpecifiedSchema = userSpecifiedSchema,
        className = source,
        options = extraOptions.toMap).resolveRelation())
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table and connection properties.
   *
   * @since 1.4.0
   */
  def jdbc(url: String, table: String, properties: Properties): DataFrame = {
    assertNoSpecifiedSchema("jdbc")
    // properties should override settings in extraOptions.
    this.extraOptions ++= properties.asScala
    // explicit url and dbtable should override all
    this.extraOptions ++= Seq(JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table)
    format("jdbc").load()
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table. Partitions of the table will be retrieved in parallel based on the parameters
   * passed to this function.
   *
   * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
   * your external database systems.
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`.
   * @param table Name of the table in the external database.
   * @param columnName the name of a column of numeric, date, or timestamp type
   *                   that will be used for partitioning.
   * @param lowerBound the minimum value of `columnName` used to decide partition stride.
   * @param upperBound the maximum value of `columnName` used to decide partition stride.
   * @param numPartitions the number of partitions. This, along with `lowerBound` (inclusive),
   *                      `upperBound` (exclusive), form partition strides for generated WHERE
   *                      clause expressions used to split the column `columnName` evenly. When
   *                      the input is less than 1, the number is set to 1.
   * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
   *                             tag/value. Normally at least a "user" and "password" property
   *                             should be included. "fetchsize" can be used to control the
   *                             number of rows per fetch and "queryTimeout" can be used to wait
   *                             for a Statement object to execute to the given number of seconds.
   * @since 1.4.0
   */
  def jdbc(
      url: String,
      table: String,
      columnName: String,
      lowerBound: Long,
      upperBound: Long,
      numPartitions: Int,
      connectionProperties: Properties): DataFrame = {
    // columnName, lowerBound, upperBound and numPartitions override settings in extraOptions.
    this.extraOptions ++= Map(
      JDBCOptions.JDBC_PARTITION_COLUMN -> columnName,
      JDBCOptions.JDBC_LOWER_BOUND -> lowerBound.toString,
      JDBCOptions.JDBC_UPPER_BOUND -> upperBound.toString,
      JDBCOptions.JDBC_NUM_PARTITIONS -> numPartitions.toString)
    jdbc(url, table, connectionProperties)
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table using connection properties. The `predicates` parameter gives a list
   * expressions suitable for inclusion in WHERE clauses; each one defines one partition
   * of the `DataFrame`.
   *
   * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
   * your external database systems.
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`
   * @param table Name of the table in the external database.
   * @param predicates Condition in the where clause for each partition.
   * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
   *                             tag/value. Normally at least a "user" and "password" property
   *                             should be included. "fetchsize" can be used to control the
   *                             number of rows per fetch.
   * @since 1.4.0
   */
  def jdbc(
      url: String,
      table: String,
      predicates: Array[String],
      connectionProperties: Properties): DataFrame = {
    assertNoSpecifiedSchema("jdbc")
    // connectionProperties should override settings in extraOptions.
    val params = extraOptions ++ connectionProperties.asScala
    val options = new JDBCOptions(url, table, params)
    val parts: Array[Partition] = predicates.zipWithIndex.map { case (part, i) =>
      JDBCPartition(part, i) : Partition
    }
    val relation = JDBCRelation(parts, options)(sparkSession)
    sparkSession.baseRelationToDataFrame(relation)
  }

  /**
   * Loads a JSON file and returns the results as a `DataFrame`.
   *
   * See the documentation on the overloaded `json()` method with varargs for more details.
   *
   * @since 1.4.0
   */
  def json(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    json(Seq(path): _*)
  }

  /**
   * Loads JSON files and returns the results as a `DataFrame`.
   *
   * <a href="http://jsonlines.org/">JSON Lines</a> (newline-delimited JSON) is supported by
   * default. For JSON (one record per file), set the `multiLine` option to true.
   *
   * This function goes through the input once to determine the input schema. If you know the
   * schema in advance, use the version that specifies the schema to avoid the extra scan.
   *
   * You can set the following JSON-specific options to deal with non-standard JSON files:
   * <ul>
   * <li>`primitivesAsString` (default `false`): infers all primitive values as a string type</li>
   * <li>`prefersDecimal` (default `false`): infers all floating-point values as a decimal
   * type. If the values do not fit in decimal, then it infers them as doubles.</li>
   * <li>`allowComments` (default `false`): ignores Java/C++ style comment in JSON records</li>
   * <li>`allowUnquotedFieldNames` (default `false`): allows unquoted JSON field names</li>
   * <li>`allowSingleQuotes` (default `true`): allows single quotes in addition to double quotes
   * </li>
   * <li>`allowNumericLeadingZeros` (default `false`): allows leading zeros in numbers
   * (e.g. 00012)</li>
   * <li>`allowBackslashEscapingAnyCharacter` (default `false`): allows accepting quoting of all
   * character using backslash quoting mechanism</li>
   * <li>`allowUnquotedControlChars` (default `false`): allows JSON Strings to contain unquoted
   * control characters (ASCII characters with value less than 32, including tab and line feed
   * characters) or not.</li>
   * <li>`mode` (default `PERMISSIVE`): allows a mode for dealing with corrupt records
   * during parsing.
   *   <ul>
   *     <li>`PERMISSIVE` : when it meets a corrupted record, puts the malformed string into a
   *     field configured by `columnNameOfCorruptRecord`, and sets other fields to `null`. To
   *     keep corrupt records, an user can set a string type field named
   *     `columnNameOfCorruptRecord` in an user-defined schema. If a schema does not have the
   *     field, it drops corrupt records during parsing. When inferring a schema, it implicitly
   *     adds a `columnNameOfCorruptRecord` field in an output schema.</li>
   *     <li>`DROPMALFORMED` : ignores the whole corrupted records.</li>
   *     <li>`FAILFAST` : throws an exception when it meets corrupted records.</li>
   *   </ul>
   * </li>
   * <li>`columnNameOfCorruptRecord` (default is the value specified in
   * `spark.sql.columnNameOfCorruptRecord`): allows renaming the new field having malformed string
   * created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.</li>
   * <li>`dateFormat` (default `yyyy-MM-dd`): sets the string that indicates a date format.
   * Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to
   * date type.</li>
   * <li>`timestampFormat` (default `yyyy-MM-dd'T'HH:mm:ss.SSSXXX`): sets the string that
   * indicates a timestamp format. Custom date formats follow the formats at
   * `java.text.SimpleDateFormat`. This applies to timestamp type.</li>
   * <li>`multiLine` (default `false`): parse one record, which may span multiple lines,
   * per file</li>
   * <li>`encoding` (by default it is not set): allows to forcibly set one of standard basic
   * or extended encoding for the JSON files. For example UTF-16BE, UTF-32LE. If the encoding
   * is not specified and `multiLine` is set to `true`, it will be detected automatically.</li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * <li>`samplingRatio` (default is 1.0): defines fraction of input JSON objects used
   * for schema inferring.</li>
   * <li>`dropFieldIfAllNull` (default `false`): whether to ignore column of all null values or
   * empty array/struct during schema inference.</li>
   * </ul>
   *
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def json(paths: String*): DataFrame = format("json").load(paths : _*)

  /**
   * Loads a `JavaRDD[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON
   * Lines text format or newline-delimited JSON</a>) and returns the result as
   * a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonRDD input RDD with one JSON object per record
   * @since 1.4.0
   */
  @deprecated("Use json(Dataset[String]) instead.", "2.2.0")
  def json(jsonRDD: JavaRDD[String]): DataFrame = json(jsonRDD.rdd)

  /**
   * Loads an `RDD[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON Lines
   * text format or newline-delimited JSON</a>) and returns the result as a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonRDD input RDD with one JSON object per record
   * @since 1.4.0
   */
  @deprecated("Use json(Dataset[String]) instead.", "2.2.0")
  def json(jsonRDD: RDD[String]): DataFrame = {
    json(sparkSession.createDataset(jsonRDD)(Encoders.STRING))
  }

  /**
   * Loads a `Dataset[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON Lines
   * text format or newline-delimited JSON</a>) and returns the result as a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonDataset input Dataset with one JSON object per record
   * @since 2.2.0
   */
  def json(jsonDataset: Dataset[String]): DataFrame = {
    val parsedOptions = new JSONOptions(
      extraOptions.toMap,
      sparkSession.sessionState.conf.sessionLocalTimeZone,
      sparkSession.sessionState.conf.columnNameOfCorruptRecord)

    val schema = userSpecifiedSchema.getOrElse {
      TextInputJsonDataSource.inferFromDataset(jsonDataset, parsedOptions)
    }

    verifyColumnNameOfCorruptRecord(schema, parsedOptions.columnNameOfCorruptRecord)
    val actualSchema =
      StructType(schema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord))

    val createParser = CreateJacksonParser.string _
    val parsed = jsonDataset.rdd.mapPartitions { iter =>
      val rawParser = new JacksonParser(actualSchema, parsedOptions)
      val parser = new FailureSafeParser[String](
        input => rawParser.parse(input, createParser, UTF8String.fromString),
        parsedOptions.parseMode,
        schema,
        parsedOptions.columnNameOfCorruptRecord)
      iter.flatMap(parser.parse)
    }
    sparkSession.internalCreateDataFrame(parsed, schema, isStreaming = jsonDataset.isStreaming)
  }

  /**
   * Loads a CSV file and returns the result as a `DataFrame`. See the documentation on the
   * other overloaded `csv()` method for more details.
   *
   * @since 2.0.0
   */
  def csv(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    csv(Seq(path): _*)
  }

  /**
   * Loads an `Dataset[String]` storing CSV rows and returns the result as a `DataFrame`.
   *
   * If the schema is not specified using `schema` function and `inferSchema` option is enabled,
   * this function goes through the input once to determine the input schema.
   *
   * If the schema is not specified using `schema` function and `inferSchema` option is disabled,
   * it determines the columns as string types and it reads only the first line to determine the
   * names and the number of fields.
   *
   * If the enforceSchema is set to `false`, only the CSV header in the first line is checked
   * to conform specified or inferred schema.
   *
   * @param csvDataset input Dataset with one CSV row per record
   * @since 2.2.0
   */
  def csv(csvDataset: Dataset[String]): DataFrame = {
    val parsedOptions: CSVOptions = new CSVOptions(
      extraOptions.toMap,
      sparkSession.sessionState.conf.csvColumnPruning,
      sparkSession.sessionState.conf.sessionLocalTimeZone)
    val filteredLines: Dataset[String] =
      CSVUtils.filterCommentAndEmpty(csvDataset, parsedOptions)
    val maybeFirstLine: Option[String] = filteredLines.take(1).headOption

    val schema = userSpecifiedSchema.getOrElse {
      TextInputCSVDataSource.inferFromDataset(
        sparkSession,
        csvDataset,
        maybeFirstLine,
        parsedOptions)
    }

    verifyColumnNameOfCorruptRecord(schema, parsedOptions.columnNameOfCorruptRecord)
    val actualSchema =
      StructType(schema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord))

    val linesWithoutHeader = if (parsedOptions.headerFlag && maybeFirstLine.isDefined) {
      val firstLine = maybeFirstLine.get
      val parser = new CsvParser(parsedOptions.asParserSettings)
      val columnNames = parser.parseLine(firstLine)
      CSVDataSource.checkHeaderColumnNames(
        actualSchema,
        columnNames,
        csvDataset.getClass.getCanonicalName,
        parsedOptions.enforceSchema,
        sparkSession.sessionState.conf.caseSensitiveAnalysis)
      filteredLines.rdd.mapPartitions(CSVUtils.filterHeaderLine(_, firstLine, parsedOptions))
    } else {
      filteredLines.rdd
    }

    val parsed = linesWithoutHeader.mapPartitions { iter =>
      val rawParser = new UnivocityParser(actualSchema, parsedOptions)
      val parser = new FailureSafeParser[String](
        input => Seq(rawParser.parse(input)),
        parsedOptions.parseMode,
        schema,
        parsedOptions.columnNameOfCorruptRecord)
      iter.flatMap(parser.parse)
    }
    sparkSession.internalCreateDataFrame(parsed, schema, isStreaming = csvDataset.isStreaming)
  }

  /**
   * Loads CSV files and returns the result as a `DataFrame`.
   *
   * This function will go through the input once to determine the input schema if `inferSchema`
   * is enabled. To avoid going through the entire data once, disable `inferSchema` option or
   * specify the schema explicitly using `schema`.
   *
   * You can set the following CSV-specific options to deal with CSV files:
   * <ul>
   * <li>`sep` (default `,`): sets a single character as a separator for each
   * field and value.</li>
   * <li>`encoding` (default `UTF-8`): decodes the CSV files by the given encoding
   * type.</li>
   * <li>`quote` (default `"`): sets a single character used for escaping quoted values where
   * the separator can be part of the value. If you would like to turn off quotations, you need to
   * set not `null` but an empty string. This behaviour is different from
   * `com.databricks.spark.csv`.</li>
   * <li>`escape` (default `\`): sets a single character used for escaping quotes inside
   * an already quoted value.</li>
   * <li>`charToEscapeQuoteEscaping` (default `escape` or `\0`): sets a single character used for
   * escaping the escape for the quote character. The default value is escape character when escape
   * and quote characters are different, `\0` otherwise.</li>
   * <li>`comment` (default empty string): sets a single character used for skipping lines
   * beginning with this character. By default, it is disabled.</li>
   * <li>`header` (default `false`): uses the first line as names of columns.</li>
   * <li>`enforceSchema` (default `true`): If it is set to `true`, the specified or inferred schema
   * will be forcibly applied to datasource files, and headers in CSV files will be ignored.
   * If the option is set to `false`, the schema will be validated against all headers in CSV files
   * in the case when the `header` option is set to `true`. Field names in the schema
   * and column names in CSV headers are checked by their positions taking into account
   * `spark.sql.caseSensitive`. Though the default value is true, it is recommended to disable
   * the `enforceSchema` option to avoid incorrect results.</li>
   * <li>`inferSchema` (default `false`): infers the input schema automatically from data. It
   * requires one extra pass over the data.</li>
   * <li>`samplingRatio` (default is 1.0): defines fraction of rows used for schema inferring.</li>
   * <li>`ignoreLeadingWhiteSpace` (default `false`): a flag indicating whether or not leading
   * whitespaces from values being read should be skipped.</li>
   * <li>`ignoreTrailingWhiteSpace` (default `false`): a flag indicating whether or not trailing
   * whitespaces from values being read should be skipped.</li>
   * <li>`nullValue` (default empty string): sets the string representation of a null value. Since
   * 2.0.1, this applies to all supported types including the string type.</li>
   * <li>`emptyValue` (default empty string): sets the string representation of an empty value.</li>
   * <li>`nanValue` (default `NaN`): sets the string representation of a non-number" value.</li>
   * <li>`positiveInf` (default `Inf`): sets the string representation of a positive infinity
   * value.</li>
   * <li>`negativeInf` (default `-Inf`): sets the string representation of a negative infinity
   * value.</li>
   * <li>`dateFormat` (default `yyyy-MM-dd`): sets the string that indicates a date format.
   * Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to
   * date type.</li>
   * <li>`timestampFormat` (default `yyyy-MM-dd'T'HH:mm:ss.SSSXXX`): sets the string that
   * indicates a timestamp format. Custom date formats follow the formats at
   * `java.text.SimpleDateFormat`. This applies to timestamp type.</li>
   * <li>`maxColumns` (default `20480`): defines a hard limit of how many columns
   * a record can have.</li>
   * <li>`maxCharsPerColumn` (default `-1`): defines the maximum number of characters allowed
   * for any given value being read. By default, it is -1 meaning unlimited length</li>
   * <li>`mode` (default `PERMISSIVE`): allows a mode for dealing with corrupt records
   *    during parsing. It supports the following case-insensitive modes. Note that Spark tries
   *    to parse only required columns in CSV under column pruning. Therefore, corrupt records
   *    can be different based on required set of fields. This behavior can be controlled by
   *    `spark.sql.csv.parser.columnPruning.enabled` (enabled by default).
   *   <ul>
   *     <li>`PERMISSIVE` : when it meets a corrupted record, puts the malformed string into a
   *     field configured by `columnNameOfCorruptRecord`, and sets other fields to `null`. To keep
   *     corrupt records, an user can set a string type field named `columnNameOfCorruptRecord`
   *     in an user-defined schema. If a schema does not have the field, it drops corrupt records
   *     during parsing. A record with less/more tokens than schema is not a corrupted record to
   *     CSV. When it meets a record having fewer tokens than the length of the schema, sets
   *     `null` to extra fields. When the record has more tokens than the length of the schema,
   *     it drops extra tokens.</li>
   *     <li>`DROPMALFORMED` : ignores the whole corrupted records.</li>
   *     <li>`FAILFAST` : throws an exception when it meets corrupted records.</li>
   *   </ul>
   * </li>
   * <li>`columnNameOfCorruptRecord` (default is the value specified in
   * `spark.sql.columnNameOfCorruptRecord`): allows renaming the new field having malformed string
   * created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.</li>
   * <li>`multiLine` (default `false`): parse one record, which may span multiple lines.</li>
   * </ul>
   *
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def csv(paths: String*): DataFrame = format("csv").load(paths : _*)

  /**
   * Loads a Parquet file, returning the result as a `DataFrame`. See the documentation
   * on the other overloaded `parquet()` method for more details.
   *
   * @since 2.0.0
   */
  def parquet(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    parquet(Seq(path): _*)
  }

  /**
   * Loads a Parquet file, returning the result as a `DataFrame`.
   *
   * You can set the following Parquet-specific option(s) for reading Parquet files:
   * <ul>
   * <li>`mergeSchema` (default is the value specified in `spark.sql.parquet.mergeSchema`): sets
   * whether we should merge schemas collected from all Parquet part-files. This will override
   * `spark.sql.parquet.mergeSchema`.</li>
   * </ul>
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def parquet(paths: String*): DataFrame = {
    format("parquet").load(paths: _*)
  }

  /**
   * Loads an ORC file and returns the result as a `DataFrame`.
   *
   * @param path input path
   * @since 1.5.0
   */
  def orc(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    orc(Seq(path): _*)
  }

  /**
   * Loads ORC files and returns the result as a `DataFrame`.
   *
   * @param paths input paths
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orc(paths: String*): DataFrame = format("orc").load(paths: _*)

  /**
   * Returns the specified table as a `DataFrame`.
   *
   * @since 1.4.0
   */
  def table(tableName: String): DataFrame = {
    assertNoSpecifiedSchema("table")
    sparkSession.table(tableName)
  }

  /**
   * Loads text files and returns a `DataFrame` whose schema starts with a string column named
   * "value", and followed by partitioned columns if there are any. See the documentation on
   * the other overloaded `text()` method for more details.
   *
   * @since 2.0.0
   */
  def text(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    text(Seq(path): _*)
  }

  /**
   * Loads text files and returns a `DataFrame` whose schema starts with a string column named
   * "value", and followed by partitioned columns if there are any.
   *
   * By default, each line in the text files is a new row in the resulting DataFrame. For example:
   * {{{
   *   // Scala:
   *   spark.read.text("/path/to/spark/README.md")
   *
   *   // Java:
   *   spark.read().text("/path/to/spark/README.md")
   * }}}
   *
   * You can set the following text-specific option(s) for reading text files:
   * <ul>
   * <li>`wholetext` (default `false`): If true, read a file as a single row and not split by "\n".
   * </li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * </ul>
   *
   * @param paths input paths
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def text(paths: String*): DataFrame = format("text").load(paths : _*)

  /**
   * Loads text files and returns a [[Dataset]] of String. See the documentation on the
   * other overloaded `textFile()` method for more details.
   * @since 2.0.0
   */
  def textFile(path: String): Dataset[String] = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    textFile(Seq(path): _*)
  }

  /**
   * Loads text files and returns a [[Dataset]] of String. The underlying schema of the Dataset
   * contains a single string column named "value".
   *
   * If the directory structure of the text files contains partitioning information, those are
   * ignored in the resulting Dataset. To include partitioning information as columns, use `text`.
   *
   * By default, each line in the text files is a new row in the resulting DataFrame. For example:
   * {{{
   *   // Scala:
   *   spark.read.textFile("/path/to/spark/README.md")
   *
   *   // Java:
   *   spark.read().textFile("/path/to/spark/README.md")
   * }}}
   *
   * You can set the following textFile-specific option(s) for reading text files:
   * <ul>
   * <li>`wholetext` (default `false`): If true, read a file as a single row and not split by "\n".
   * </li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * </ul>
   *
   * @param paths input path
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def textFile(paths: String*): Dataset[String] = {
    assertNoSpecifiedSchema("textFile")
    text(paths : _*).select("value").as[String](sparkSession.implicits.newStringEncoder)
  }

  /**
   * A convenient function for schema validation in APIs.
   */
  private def assertNoSpecifiedSchema(operation: String): Unit = {
    if (userSpecifiedSchema.nonEmpty) {
      throw new AnalysisException(s"User specified schema not supported with `$operation`")
    }
  }

  /**
   * A convenient function for schema validation in datasources supporting
   * `columnNameOfCorruptRecord` as an option.
   */
  private def verifyColumnNameOfCorruptRecord(
      schema: StructType,
      columnNameOfCorruptRecord: String): Unit = {
    schema.getFieldIndex(columnNameOfCorruptRecord).foreach { corruptFieldIndex =>
      val f = schema(corruptFieldIndex)
      if (f.dataType != StringType || !f.nullable) {
        throw new AnalysisException(
          "The field for corrupt records must be string type and nullable")
      }
    }
  }

  ///////////////////////////////////////////////////////////////////////////////////////
  // Builder pattern config options
  ///////////////////////////////////////////////////////////////////////////////////////

  private var source: String = sparkSession.sessionState.conf.defaultDataSourceName

  private var userSpecifiedSchema: Option[StructType] = None

  private var extraOptions = CaseInsensitiveMap[String](Map.empty)

}

[0m2021.03.05 12:51:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:51:20 INFO  time: compiled root in 1.16s[0m
[0m2021.03.05 12:51:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:51:45 INFO  time: compiled root in 1.12s[0m
[0m2021.03.05 12:51:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:51:51 INFO  time: compiled root in 1.16s[0m
[0m2021.03.05 12:52:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:52:27 INFO  time: compiled root in 1.21s[0m
[0m2021.03.05 12:53:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:53:43 INFO  time: compiled root in 1.23s[0m
[0m2021.03.05 12:54:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:54:38 INFO  time: compiled root in 1.34s[0m
[0m2021.03.05 12:59:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:59:19 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 12:59:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:49 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:50 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:52 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:53 INFO  /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:127:15: stale bloop error: not found: value fullPath1
      println(fullPath1)
              ^^^^^^^^^[0m
[0m2021.03.05 12:59:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:59:58 INFO  time: compiled root in 0.33s[0m
[0m2021.03.05 13:00:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:00:03 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 13:00:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:00:05 INFO  time: compiled root in 0.3s[0m
[0m2021.03.05 13:00:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:00:15 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 13:00:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:00:53 INFO  time: compiled root in 0.32s[0m
[0m2021.03.05 13:01:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:01:01 INFO  time: compiled root in 1.19s[0m
[0m2021.03.05 13:04:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:04:07 INFO  time: compiled root in 0.23s[0m
[0m2021.03.05 13:04:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:04:33 INFO  time: compiled root in 1.48s[0m
[0m2021.03.05 13:04:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:04:57 INFO  time: compiled root in 0.1s[0m
[0m2021.03.05 13:05:00 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:142: error: ; expected but catch found
    } catch {
      ^[0m
[0m2021.03.05 13:05:01 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:142: error: ; expected but catch found
    } catch {
      ^[0m
[0m2021.03.05 13:05:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:05:20 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 13:05:28 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:148: error: ; expected but ) found
        })
         ^[0m
[0m2021.03.05 13:05:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:05:38 INFO  time: compiled root in 1.04s[0m
[0m2021.03.05 13:05:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:05:46 INFO  time: compiled root in 0.99s[0m
[0m2021.03.05 13:06:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:06:03 INFO  time: compiled root in 1.01s[0m
[0m2021.03.05 13:07:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:07:07 INFO  time: compiled root in 0.92s[0m
[0m2021.03.05 13:07:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:07:27 INFO  time: compiled root in 1.04s[0m
[0m2021.03.05 13:07:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:07:55 INFO  time: compiled root in 0.98s[0m
[0m2021.03.05 13:09:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:09:12 INFO  time: compiled root in 0.93s[0m
[0m2021.03.05 13:10:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:10:17 INFO  time: compiled root in 0.98s[0m
[0m2021.03.05 13:15:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:15:05 INFO  time: compiled root in 1s[0m
[0m2021.03.05 13:15:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:15:53 INFO  time: compiled root in 0.99s[0m
[0m2021.03.05 13:16:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:16:25 INFO  time: compiled root in 1s[0m
[0m2021.03.05 13:16:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:16:36 INFO  time: compiled root in 0.97s[0m
[0m2021.03.05 13:17:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:17:04 INFO  time: compiled root in 0.96s[0m
[0m2021.03.05 13:18:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:18:28 INFO  time: compiled root in 0.99s[0m
[0m2021.03.05 13:18:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 13:18:43 INFO  time: compiled root in 0.96s[0m
[0m2021.03.05 14:04:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:04:52 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 14:07:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:07:28 INFO  time: compiled root in 1.39s[0m
[0m2021.03.05 14:07:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:08:00 INFO  time: compiled root in 1.2s[0m
[0m2021.03.05 14:09:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:09:43 INFO  time: compiled root in 1.19s[0m
[0m2021.03.05 14:10:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:10:22 INFO  time: compiled root in 1.26s[0m
[0m2021.03.05 14:14:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:14:52 INFO  time: compiled root in 0.32s[0m
[0m2021.03.05 14:15:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:15:07 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 14:15:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:15:14 INFO  time: compiled root in 0.38s[0m
[0m2021.03.05 14:15:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:15:52 INFO  time: compiled root in 0.13s[0m
[0m2021.03.05 14:20:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:20:17 INFO  time: compiled root in 1.21s[0m
[0m2021.03.05 14:22:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:22:50 INFO  time: compiled root in 0.13s[0m
[0m2021.03.05 14:23:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:23:15 INFO  time: compiled root in 0.11s[0m
[0m2021.03.05 14:23:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:23:21 INFO  time: compiled root in 1.15s[0m
Mar 05, 2021 2:24:18 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 05, 2021 2:24:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5168
[0m2021.03.05 14:24:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:24:33 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 14:24:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:24:48 INFO  time: compiled root in 0.32s[0m
[0m2021.03.05 14:24:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:24:53 INFO  time: compiled root in 0.31s[0m
[0m2021.03.05 14:25:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:25:07 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 14:25:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:26:00 INFO  time: compiled root in 1.13s[0m
[0m2021.03.05 14:27:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:27:45 INFO  time: compiled root in 1.29s[0m
[0m2021.03.05 14:28:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:28:46 INFO  time: compiled root in 1.85s[0m
[0m2021.03.05 14:29:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:29:36 INFO  time: compiled root in 1.15s[0m
[0m2021.03.05 14:29:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:29:43 INFO  time: compiled root in 1.11s[0m
[0m2021.03.05 14:29:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:29:57 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 14:30:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:30:06 INFO  time: compiled root in 1.1s[0m
Mar 05, 2021 2:30:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5761
[0m2021.03.05 14:31:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:31:35 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 14:31:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:31:45 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 14:32:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:32:01 INFO  time: compiled root in 0.11s[0m
[0m2021.03.05 14:32:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:32:07 INFO  time: compiled root in 0.15s[0m
Mar 05, 2021 2:32:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5916
[0m2021.03.05 14:34:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:34:02 INFO  time: compiled root in 1.25s[0m
Mar 05, 2021 2:34:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5952
Mar 05, 2021 2:35:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5987
Mar 05, 2021 2:37:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6006
[0m2021.03.05 14:38:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:38:36 INFO  time: compiled root in 1.08s[0m
[0m2021.03.05 14:38:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:38:48 INFO  time: compiled root in 1.1s[0m
[0m2021.03.05 14:42:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:42:45 INFO  time: compiled root in 0.95s[0m
[0m2021.03.05 14:45:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:45:37 INFO  time: compiled root in 1.02s[0m
[0m2021.03.05 14:46:09 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:149: error: ; expected but catch found
    } catch {
      ^[0m
[0m2021.03.05 14:46:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:46:17 INFO  time: compiled root in 0.24s[0m
[0m2021.03.05 14:46:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:46:30 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 14:46:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:46:35 INFO  time: compiled root in 0.24s[0m
[0m2021.03.05 14:46:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:46:45 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 14:46:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:47:00 INFO  time: compiled root in 1.02s[0m
[0m2021.03.05 14:49:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:49:31 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 14:49:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:49:36 INFO  time: compiled root in 1.02s[0m
[0m2021.03.05 14:49:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:49:52 INFO  time: compiled root in 1.06s[0m
[0m2021.03.05 14:52:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:53:00 INFO  time: compiled root in 1.15s[0m
[0m2021.03.05 14:54:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:54:21 INFO  time: compiled root in 1.1s[0m
[0m2021.03.05 14:55:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:55:06 INFO  time: compiled root in 1.11s[0m
[0m2021.03.05 14:56:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:56:52 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 14:57:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:57:27 INFO  time: compiled root in 1.1s[0m
[0m2021.03.05 14:57:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:57:42 INFO  time: compiled root in 1.14s[0m
[0m2021.03.05 14:58:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 14:58:13 INFO  time: compiled root in 1.23s[0m
[0m2021.03.05 15:01:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:01:09 INFO  time: compiled root in 1.08s[0m
[0m2021.03.05 15:01:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:01:18 INFO  time: compiled root in 1.07s[0m
[0m2021.03.05 15:02:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:02:54 INFO  time: compiled root in 1.18s[0m
[0m2021.03.05 15:02:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:02:57 INFO  time: compiled root in 1.2s[0m
[0m2021.03.05 15:03:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:03:08 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 15:03:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:03:51 INFO  time: compiled root in 1.24s[0m
[0m2021.03.05 15:05:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:05:54 INFO  time: compiled root in 1.25s[0m
[0m2021.03.05 15:08:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:08:28 INFO  time: compiled root in 1.3s[0m
[0m2021.03.05 15:08:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:08:41 INFO  time: compiled root in 1.47s[0m
[0m2021.03.05 15:09:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:09:51 INFO  time: compiled root in 1.84s[0m
[0m2021.03.05 15:11:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:11:00 INFO  time: compiled root in 0.25s[0m
[0m2021.03.05 15:12:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:12:00 INFO  time: compiled root in 0.3s[0m
[0m2021.03.05 15:12:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:12:21 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 15:12:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:12:26 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 15:12:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:12:48 INFO  time: compiled root in 0.31s[0m
[0m2021.03.05 15:12:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:12:59 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 15:13:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:13:30 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 15:15:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:15:17 INFO  time: compiled root in 1.15s[0m
[0m2021.03.05 15:16:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:16:23 INFO  time: compiled root in 1.12s[0m
[0m2021.03.05 15:16:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:16:34 INFO  time: compiled root in 1.55s[0m
[0m2021.03.05 15:23:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:23:33 INFO  time: compiled root in 0.31s[0m
[0m2021.03.05 15:23:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:23:51 INFO  time: compiled root in 1.05s[0m
[0m2021.03.05 15:24:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:24:09 INFO  time: compiled root in 1.16s[0m
[0m2021.03.05 15:25:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:25:55 INFO  time: compiled root in 1.09s[0m
[0m2021.03.05 15:28:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:28:26 INFO  time: compiled root in 1.12s[0m
[0m2021.03.05 15:29:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:29:24 INFO  time: compiled root in 1.14s[0m
[0m2021.03.05 15:29:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:29:52 INFO  time: compiled root in 0.31s[0m
[0m2021.03.05 15:29:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:30:01 INFO  time: compiled root in 1.19s[0m
[0m2021.03.05 15:31:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:32:00 INFO  time: compiled root in 1.22s[0m
[0m2021.03.05 15:32:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:32:17 INFO  time: compiled root in 1.11s[0m
[0m2021.03.05 15:32:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:32:21 INFO  time: compiled root in 1.14s[0m
[0m2021.03.05 15:33:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:33:58 INFO  time: compiled root in 1.28s[0m
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in Seq[<error>]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 5329, 5329, 5342)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in Seq[<error>]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 5329, 5329, 5335)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in Seq[<error>]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 5329, 5329, 5336)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in Seq[<error>]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 5329, 5329, 5337)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in Seq[String]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 5329, 5329, 5340)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in Seq[String]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 5329, 5329, 5340)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in Seq[String]RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 5329, 5329, 5340)
[0m2021.03.05 15:36:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:36:13 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 15:39:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:39:00 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 15:39:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:39:38 INFO  time: compiled root in 1.15s[0m
[0m2021.03.05 15:41:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:41:22 INFO  time: compiled root in 1.19s[0m
[0m2021.03.05 15:41:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:41:25 INFO  time: compiled root in 1.15s[0m
[0m2021.03.05 15:41:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:41:49 INFO  time: compiled root in 1.32s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.encoders.{encoderFor, ExpressionEncoder}
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
import org.apache.spark.sql.catalyst.util.toPrettySQL
import org.apache.spark.sql.execution.aggregate.TypedAggregateExpression
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.types._

private[sql] object Column {

  def apply(colName: String): Column = new Column(colName)

  def apply(expr: Expression): Column = new Column(expr)

  def unapply(col: Column): Option[Expression] = Some(col.expr)

  private[sql] def generateAlias(e: Expression): String = {
    e match {
      case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
        a.aggregateFunction.toString
      case expr => toPrettySQL(expr)
    }
  }
}

/**
 * A [[Column]] where an [[Encoder]] has been given for the expected input and return type.
 * To create a [[TypedColumn]], use the `as` function on a [[Column]].
 *
 * @tparam T The input type expected for this expression.  Can be `Any` if the expression is type
 *           checked by the analyzer instead of the compiler (i.e. `expr("sum(...)")`).
 * @tparam U The output type of this column.
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class TypedColumn[-T, U](
    expr: Expression,
    private[sql] val encoder: ExpressionEncoder[U])
  extends Column(expr) {

  /**
   * Inserts the specific input type and schema into any expressions that are expected to operate
   * on a decoded object.
   */
  private[sql] def withInputType(
      inputEncoder: ExpressionEncoder[_],
      inputAttributes: Seq[Attribute]): TypedColumn[T, U] = {
    val unresolvedDeserializer = UnresolvedDeserializer(inputEncoder.deserializer, inputAttributes)
    val newExpr = expr transform {
      case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty =>
        ta.withInputInfo(
          deser = unresolvedDeserializer,
          cls = inputEncoder.clsTag.runtimeClass,
          schema = inputEncoder.schema)
    }
    new TypedColumn[T, U](newExpr, encoder)
  }

  /**
   * Gives the [[TypedColumn]] a name (alias).
   * If the current `TypedColumn` has metadata associated with it, this metadata will be propagated
   * to the new column.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  override def name(alias: String): TypedColumn[T, U] =
    new TypedColumn[T, U](super.name(alias).expr, encoder)

}

/**
 * A column that will be computed based on the data in a `DataFrame`.
 *
 * A new column can be constructed based on the input columns present in a DataFrame:
 *
 * {{{
 *   df("columnName")            // On a specific `df` DataFrame.
 *   col("columnName")           // A generic column not yet associated with a DataFrame.
 *   col("columnName.field")     // Extracting a struct field
 *   col("`a.column.with.dots`") // Escape `.` in column names.
 *   $"columnName"               // Scala short hand for a named column.
 * }}}
 *
 * [[Column]] objects can be composed to form complex expressions:
 *
 * {{{
 *   $"a" + 1
 *   $"a" === $"b"
 * }}}
 *
 * @note The internal Catalyst expression can be accessed via [[expr]], but this method is for
 * debugging purposes only and can change in any future Spark releases.
 *
 * @groupname java_expr_ops Java-specific expression operators
 * @groupname expr_ops Expression operators
 * @groupname df_ops DataFrame functions
 * @groupname Ungrouped Support functions for DataFrames
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class Column(val expr: Expression) extends Logging {

  def this(name: String) = this(name match {
    case "*" => UnresolvedStar(None)
    case _ if name.endsWith(".*") =>
      val parts = UnresolvedAttribute.parseAttributeName(name.substring(0, name.length - 2))
      UnresolvedStar(Some(parts))
    case _ => UnresolvedAttribute.quotedString(name)
  })

  override def toString: String = toPrettySQL(expr)

  override def equals(that: Any): Boolean = that match {
    case that: Column => that.expr.equals(this.expr)
    case _ => false
  }

  override def hashCode: Int = this.expr.hashCode()

  /** Creates a column based on the given expression. */
  private def withExpr(newExpr: Expression): Column = new Column(newExpr)

  /**
   * Returns the expression for this column either with an existing or auto assigned name.
   */
  private[sql] def named: NamedExpression = expr match {
    // Wrap UnresolvedAttribute with UnresolvedAlias, as when we resolve UnresolvedAttribute, we
    // will remove intermediate Alias for ExtractValue chain, and we need to alias it again to
    // make it a NamedExpression.
    case u: UnresolvedAttribute => UnresolvedAlias(u)

    case u: UnresolvedExtractValue => UnresolvedAlias(u)

    case expr: NamedExpression => expr

    // Leave an unaliased generator with an empty list of names since the analyzer will generate
    // the correct defaults after the nested expression's type has been resolved.
    case g: Generator => MultiAlias(g, Nil)

    case func: UnresolvedFunction => UnresolvedAlias(func, Some(Column.generateAlias))

    // If we have a top level Cast, there is a chance to give it a better alias, if there is a
    // NamedExpression under this Cast.
    case c: Cast =>
      c.transformUp {
        case c @ Cast(_: NamedExpression, _, _) => UnresolvedAlias(c)
      } match {
        case ne: NamedExpression => ne
        case _ => Alias(expr, toPrettySQL(expr))()
      }

    case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
      UnresolvedAlias(a, Some(Column.generateAlias))

    // Wait until the struct is resolved. This will generate a nicer looking alias.
    case struct: CreateNamedStructLike => UnresolvedAlias(struct)

    case expr: Expression => Alias(expr, toPrettySQL(expr))()
  }

  /**
   * Provides a type hint about the expected return value of this column.  This information can
   * be used by operations such as `select` on a [[Dataset]] to automatically convert the
   * results into the correct JVM types.
   * @since 1.6.0
   */
  def as[U : Encoder]: TypedColumn[Any, U] = new TypedColumn[Any, U](expr, encoderFor[U])

  /**
   * Extracts a value or values from a complex type.
   * The following types of extraction are supported:
   * <ul>
   * <li>Given an Array, an integer ordinal can be used to retrieve a single value.</li>
   * <li>Given a Map, a key of the correct type can be used to retrieve an individual value.</li>
   * <li>Given a Struct, a string fieldName can be used to extract that field.</li>
   * <li>Given an Array of Structs, a string fieldName can be used to extract filed
   *    of every struct in that array, and return an Array of fields.</li>
   * </ul>
   * @group expr_ops
   * @since 1.4.0
   */
  def apply(extraction: Any): Column = withExpr {
    UnresolvedExtractValue(expr, lit(extraction).expr)
  }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Scala: select the amount column and negates all values.
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.select( negate(col("amount") );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_- : Column = withExpr { UnaryMinus(expr) }

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_! : Column = withExpr { Not(expr) }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def === (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} = $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualTo(expr, right)
  }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def equalTo(other: Any): Column = this === other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") =!= df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
    */
  def =!= (other: Any): Column = withExpr{ Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
    */
  @deprecated("!== does not have the same precedence as ===, use =!= instead", "2.0.0")
  def !== (other: Any): Column = this =!= other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def notEqual(other: Any): Column = withExpr { Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > 21 )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def > (other: Any): Column = withExpr { GreaterThan(expr, lit(other).expr) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > lit(21) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def gt(other: Any): Column = this > other

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def < (other: Any): Column = withExpr { LessThan(expr, lit(other).expr) }

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def lt(other: Any): Column = this < other

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <= (other: Any): Column = withExpr { LessThanOrEqual(expr, lit(other).expr) }

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def leq(other: Any): Column = this <= other

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def >= (other: Any): Column = withExpr { GreaterThanOrEqual(expr, lit(other).expr) }

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def geq(other: Any): Column = this >= other

  /**
   * Equality test that is safe for null values.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <=> (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} <=> $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualNullSafe(expr, right)
  }

  /**
   * Equality test that is safe for null values.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def eqNullSafe(other: Any): Column = this <=> other

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches :+ ((condition.expr, lit(value).expr))) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "when() cannot be applied once otherwise() is applied")
    case _ =>
      throw new IllegalArgumentException(
        "when() can only be applied on a Column previously generated by when() function")
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def otherwise(value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches, Option(lit(value).expr)) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied once on a Column previously generated by when()")
    case _ =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied on a Column previously generated by when()")
  }

  /**
   * True if the current column is between the lower bound and upper bound, inclusive.
   *
   * @group java_expr_ops
   * @since 1.4.0
   */
  def between(lowerBound: Any, upperBound: Any): Column = {
    (this >= lowerBound) && (this <= upperBound)
  }

  /**
   * True if the current expression is NaN.
   *
   * @group expr_ops
   * @since 1.5.0
   */
  def isNaN: Column = withExpr { IsNaN(expr) }

  /**
   * True if the current expression is null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNull: Column = withExpr { IsNull(expr) }

  /**
   * True if the current expression is NOT null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNotNull: Column = withExpr { IsNotNull(expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def || (other: Any): Column = withExpr { Or(expr, lit(other).expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def or(other: Column): Column = this || other

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def && (other: Any): Column = withExpr { And(expr, lit(other).expr) }

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def and(other: Column): Column = this && other

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def + (other: Any): Column = withExpr { Add(expr, lit(other).expr) }

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def plus(other: Any): Column = this + other

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def - (other: Any): Column = withExpr { Subtract(expr, lit(other).expr) }

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def minus(other: Any): Column = this - other

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def * (other: Any): Column = withExpr { Multiply(expr, lit(other).expr) }

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def multiply(other: Any): Column = this * other

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def / (other: Any): Column = withExpr { Divide(expr, lit(other).expr) }

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def divide(other: Any): Column = this / other

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def % (other: Any): Column = withExpr { Remainder(expr, lit(other).expr) }

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def mod(other: Any): Column = this % other

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the evaluated values of the arguments.
   *
   * Note: Since the type of the elements in the list are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def isin(list: Any*): Column = withExpr { In(expr, list.map(lit(_).expr)) }

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: scala.collection.Iterable[_]): Column = isin(values.toSeq: _*)

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group java_expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: java.lang.Iterable[_]): Column = isInCollection(values.asScala)

  /**
   * SQL like expression. Returns a boolean column based on a SQL LIKE match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def like(literal: String): Column = withExpr { Like(expr, lit(literal).expr) }

  /**
   * SQL RLIKE expression (LIKE with Regex). Returns a boolean column based on a regex
   * match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def rlike(literal: String): Column = withExpr { RLike(expr, lit(literal).expr) }

  /**
   * An expression that gets an item at position `ordinal` out of an array,
   * or gets a value by key `key` in a `MapType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getItem(key: Any): Column = withExpr { UnresolvedExtractValue(expr, Literal(key)) }

  /**
   * An expression that gets a field by name in a `StructType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getField(fieldName: String): Column = withExpr {
    UnresolvedExtractValue(expr, Literal(fieldName))
  }

  /**
   * An expression that returns a substring.
   * @param startPos expression for the starting position.
   * @param len expression for the length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Column, len: Column): Column = withExpr {
    Substring(expr, startPos.expr, len.expr)
  }

  /**
   * An expression that returns a substring.
   * @param startPos starting position.
   * @param len length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Int, len: Int): Column = withExpr {
    Substring(expr, lit(startPos).expr, lit(len).expr)
  }

  /**
   * Contains the other element. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def contains(other: Any): Column = withExpr { Contains(expr, lit(other).expr) }

  /**
   * String starts with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(other: Column): Column = withExpr { StartsWith(expr, lit(other).expr) }

  /**
   * String starts with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(literal: String): Column = this.startsWith(lit(literal))

  /**
   * String ends with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(other: Column): Column = withExpr { EndsWith(expr, lit(other).expr) }

  /**
   * String ends with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(literal: String): Column = this.endsWith(lit(literal))

  /**
   * Gives the column an alias. Same as `as`.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".alias("colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def alias(alias: String): Column = name(alias)

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String): Column = name(alias)

  /**
   * (Scala-specific) Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Seq[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Array[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as('colB))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: Symbol): Column = name(alias.name)

  /**
   * Gives the column an alias with metadata.
   * {{{
   *   val metadata: Metadata = ...
   *   df.select($"colA".as("colB", metadata))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String, metadata: Metadata): Column = withExpr {
    Alias(expr, alias)(explicitMetadata = Some(metadata))
  }

  /**
   * Gives the column a name (alias).
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".name("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def name(alias: String): Column = withExpr {
    expr match {
      case ne: NamedExpression => Alias(expr, alias)(explicitMetadata = Some(ne.metadata))
      case other => Alias(other, alias)()
    }
  }

  /**
   * Casts the column to a different data type.
   * {{{
   *   // Casts colA to IntegerType.
   *   import org.apache.spark.sql.types.IntegerType
   *   df.select(df("colA").cast(IntegerType))
   *
   *   // equivalent to
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: DataType): Column = withExpr { Cast(expr, to) }

  /**
   * Casts the column to a different data type, using the canonical string representation
   * of the type. The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
   * `float`, `double`, `decimal`, `date`, `timestamp`.
   * {{{
   *   // Casts colA to integer.
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: String): Column = cast(CatalystSqlParser.parseDataType(to))

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   // Scala
   *   df.sort(df("age").desc)
   *
   *   // Java
   *   df.sort(df.col("age").desc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def desc: Column = withExpr { SortOrder(expr, Descending) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing first.
   *   df.sort(df("age").desc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_first: Column = withExpr { SortOrder(expr, Descending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing last.
   *   df.sort(df("age").desc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_last: Column = withExpr { SortOrder(expr, Descending, NullsLast, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order.
   *   df.sort(df("age").asc)
   *
   *   // Java
   *   df.sort(df.col("age").asc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def asc: Column = withExpr { SortOrder(expr, Ascending) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing first.
   *   df.sort(df("age").asc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_first: Column = withExpr { SortOrder(expr, Ascending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing last.
   *   df.sort(df("age").asc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_last: Column = withExpr { SortOrder(expr, Ascending, NullsLast, Set.empty) }

  /**
   * Prints the expression to the console for debugging purposes.
   *
   * @group df_ops
   * @since 1.3.0
   */
  def explain(extended: Boolean): Unit = {
    // scalastyle:off println
    if (extended) {
      println(expr)
    } else {
      println(expr.sql)
    }
    // scalastyle:on println
  }

  /**
   * Compute bitwise OR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseOR(other: Any): Column = withExpr { BitwiseOr(expr, lit(other).expr) }

  /**
   * Compute bitwise AND of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseAND($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseAND(other: Any): Column = withExpr { BitwiseAnd(expr, lit(other).expr) }

  /**
   * Compute bitwise XOR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseXOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseXOR(other: Any): Column = withExpr { BitwiseXor(expr, lit(other).expr) }

  /**
   * Defines a windowing column.
   *
   * {{{
   *   val w = Window.partitionBy("name").orderBy("id")
   *   df.select(
   *     sum("price").over(w.rangeBetween(Window.unboundedPreceding, 2)),
   *     avg("price").over(w.rowsBetween(Window.currentRow, 4))
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def over(window: expressions.WindowSpec): Column = window.withAggregate(this)

  /**
   * Defines an empty analytic clause. In this case the analytic function is applied
   * and presented for all rows in the result set.
   *
   * {{{
   *   df.select(
   *     sum("price").over(),
   *     avg("price").over()
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def over(): Column = over(Window.spec)

}


/**
 * A convenient class used for constructing schema.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class ColumnName(name: String) extends Column(name) {

  /**
   * Creates a new `StructField` of type boolean.
   * @since 1.3.0
   */
  def boolean: StructField = StructField(name, BooleanType)

  /**
   * Creates a new `StructField` of type byte.
   * @since 1.3.0
   */
  def byte: StructField = StructField(name, ByteType)

  /**
   * Creates a new `StructField` of type short.
   * @since 1.3.0
   */
  def short: StructField = StructField(name, ShortType)

  /**
   * Creates a new `StructField` of type int.
   * @since 1.3.0
   */
  def int: StructField = StructField(name, IntegerType)

  /**
   * Creates a new `StructField` of type long.
   * @since 1.3.0
   */
  def long: StructField = StructField(name, LongType)

  /**
   * Creates a new `StructField` of type float.
   * @since 1.3.0
   */
  def float: StructField = StructField(name, FloatType)

  /**
   * Creates a new `StructField` of type double.
   * @since 1.3.0
   */
  def double: StructField = StructField(name, DoubleType)

  /**
   * Creates a new `StructField` of type string.
   * @since 1.3.0
   */
  def string: StructField = StructField(name, StringType)

  /**
   * Creates a new `StructField` of type date.
   * @since 1.3.0
   */
  def date: StructField = StructField(name, DateType)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal: StructField = StructField(name, DecimalType.USER_DEFAULT)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal(precision: Int, scale: Int): StructField =
    StructField(name, DecimalType(precision, scale))

  /**
   * Creates a new `StructField` of type timestamp.
   * @since 1.3.0
   */
  def timestamp: StructField = StructField(name, TimestampType)

  /**
   * Creates a new `StructField` of type binary.
   * @since 1.3.0
   */
  def binary: StructField = StructField(name, BinaryType)

  /**
   * Creates a new `StructField` of type array.
   * @since 1.3.0
   */
  def array(dataType: DataType): StructField = StructField(name, ArrayType(dataType))

  /**
   * Creates a new `StructField` of type map.
   * @since 1.3.0
   */
  def map(keyType: DataType, valueType: DataType): StructField =
    map(MapType(keyType, valueType))

  def map(mapType: MapType): StructField = StructField(name, mapType)

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(fields: StructField*): StructField = struct(StructType(fields))

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(structType: StructType): StructField = StructField(name, structType)
}

[0m2021.03.05 15:43:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:43:43 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 15:43:44 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:170: error: ) expected but def found
    def commonCrawl(df: DataFrame, spark: SparkSession) = {
    ^[0m
[0m2021.03.05 15:43:45 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:170: error: ) expected but def found
    def commonCrawl(df: DataFrame, spark: SparkSession) = {
    ^[0m
[0m2021.03.05 15:43:49 ERROR scalafmt: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala:171: error: ) expected but def found
    def commonCrawl(df: DataFrame, spark: SparkSession) = {
    ^[0m
[0m2021.03.05 15:43:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:43:58 INFO  time: compiled root in 1.23s[0m
[0m2021.03.05 15:46:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:46:29 INFO  time: compiled root in 0.37s[0m
[0m2021.03.05 15:46:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:46:40 INFO  time: compiled root in 0.3s[0m
[0m2021.03.05 15:46:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:46:42 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 15:47:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:47:03 INFO  time: compiled root in 0.33s[0m
[0m2021.03.05 15:47:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:47:38 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 15:49:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:49:31 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 15:49:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:49:44 INFO  time: compiled root in 1.2s[0m
Mar 05, 2021 3:49:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 11239
[0m2021.03.05 15:49:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:49:58 INFO  time: compiled root in 1.55s[0m
Mar 05, 2021 3:52:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 11280
Mar 05, 2021 3:52:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 11281
[0m2021.03.05 15:52:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:52:31 INFO  Deduplicating compilation of root from bsp client 'Metals 0.10.0' (since 6h 44m 19.132s)[0m
[0m2021.03.05 15:52:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:52:33 INFO  time: compiled root in 1.26s[0m
[0m2021.03.05 15:53:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:53:51 INFO  time: compiled root in 1.36s[0m
[0m2021.03.05 15:54:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:54:15 INFO  time: compiled root in 0.18s[0m
[0m2021.03.05 15:54:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:54:18 INFO  time: compiled root in 0.39s[0m
[0m2021.03.05 15:54:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:54:48 INFO  time: compiled root in 0.35s[0m
[0m2021.03.05 15:55:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:55:54 INFO  time: compiled root in 0.13s[0m
[0m2021.03.05 15:56:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:56:08 INFO  time: compiled root in 1.23s[0m
[0m2021.03.05 15:57:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:57:38 INFO  time: compiled root in 1.78s[0m
[0m2021.03.05 15:58:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:58:06 INFO  time: compiled root in 1.18s[0m
[0m2021.03.05 16:00:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:00:33 INFO  time: compiled root in 1.28s[0m
[0m2021.03.05 16:03:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:03:04 INFO  time: compiled root in 2.23s[0m
[0m2021.03.05 16:03:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:03:48 INFO  time: compiled root in 1.19s[0m
Mar 05, 2021 4:07:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 11664
Mar 05, 2021 4:09:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.InterruptedException
java.util.concurrent.CompletionException: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.pc.VirtualFileParams.checkCanceled(VirtualFileParams.java:25)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:76)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:75)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.toStream(Iterator.scala:1320)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1334)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:125)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:243)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at com.google.gson.Gson.toJson(Gson.java:648)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
	... 9 more

[0m2021.03.05 16:09:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:09:18 INFO  time: compiled root in 0.37s[0m
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in (String, <error>)RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 4164, 4164, 4181)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in (String, <error>)RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 4164, 4164, 4182)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in (String, String, <error>)RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 4164, 4164, 4189)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in (String, String, String, <error>)RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 4164, 4164, 4197)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in (String, String, Int, <error>)RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 4164, 4164, 4194)
something's wrong: no file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala in (String, String, Int, Int, <error>)RangePosition(file:///home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala, 4164, 4164, 4199)
[0m2021.03.05 16:11:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:11:11 INFO  time: compiled root in 1.24s[0m
[0m2021.03.05 16:11:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:11:31 INFO  time: compiled root in 1.13s[0m
[0m2021.03.05 16:11:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:11:53 INFO  time: compiled root in 0.33s[0m
[0m2021.03.05 16:12:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:12:11 INFO  time: compiled root in 1.26s[0m
[0m2021.03.05 16:13:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:13:07 INFO  time: compiled root in 1.26s[0m
[0m2021.03.05 16:13:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:13:12 INFO  time: compiled root in 1.23s[0m
[0m2021.03.05 16:15:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:15:19 INFO  time: compiled root in 1.25s[0m
[0m2021.03.05 16:16:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:16:19 INFO  time: compiled root in 1.26s[0m
[0m2021.03.05 16:16:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:16:27 INFO  time: compiled root in 1.24s[0m
[0m2021.03.05 16:35:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:35:06 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 16:36:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:36:34 INFO  time: compiled root in 1.21s[0m
Mar 05, 2021 5:05:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12580
Mar 05, 2021 5:06:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12597
Mar 05, 2021 5:06:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12604
[0m2021.03.05 17:52:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:52:27 INFO  time: compiled root in 3.24s[0m
[0m2021.03.05 17:53:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:53:47 INFO  time: compiled root in 0.43s[0m
[0m2021.03.05 17:53:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:53:56 INFO  time: compiled root in 2.26s[0m
[0m2021.03.05 17:53:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:53:57 INFO  time: compiled root in 0.39s[0m
[0m2021.03.05 17:54:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:54:36 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 17:55:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:55:26 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 17:56:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:56:27 INFO  time: compiled root in 1.25s[0m
[0m2021.03.05 17:58:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:58:12 INFO  time: compiled root in 1.14s[0m
[0m2021.03.05 17:59:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:59:02 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 17:59:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:59:12 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 17:59:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:59:22 INFO  time: compiled root in 1.15s[0m
[0m2021.03.05 18:00:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:00:37 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 18:01:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:01:12 INFO  time: compiled root in 0.24s[0m
[0m2021.03.05 18:02:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:02:02 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 18:02:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:02:12 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 18:02:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:02:21 INFO  time: compiled root in 0.1s[0m
[0m2021.03.05 18:02:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:02:43 INFO  time: compiled root in 1.28s[0m
[0m2021.03.05 18:49:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:49:49 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 18:53:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:53:24 INFO  time: compiled root in 1.18s[0m
[0m2021.03.05 18:53:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:53:28 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 18:53:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:53:38 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 18:53:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:53:42 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 18:54:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:54:16 INFO  time: compiled root in 1.2s[0m
[0m2021.03.05 18:54:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:54:58 INFO  time: compiled root in 1.13s[0m
[0m2021.03.05 18:55:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:55:09 INFO  time: compiled root in 1.12s[0m
[0m2021.03.05 18:55:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:55:33 INFO  time: compiled root in 1.17s[0m
[0m2021.03.05 18:56:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:56:10 INFO  time: compiled root in 1.14s[0m
[0m2021.03.05 19:40:41 INFO  shutting down Metals[0m
[0m2021.03.05 19:40:41 INFO  Shut down connection with build server.[0m
[0m2021.03.05 19:40:41 INFO  Shut down connection with build server.[0m
[0m2021.03.05 19:40:41 INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.06 10:34:47 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.54.1.[0m
[0m2021.03.06 10:34:48 INFO  time: initialize in 0.45s[0m
[0m2021.03.06 10:34:47 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.06 10:34:48 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8930114054612414284/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.06 10:34:48 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.06 10:34:50 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.DataFrame
import scala.io.Source
import java.nio.file.{Paths, Files}
import java.io.PrintWriter

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data.
    val rawStateList = Seq(
      ("AL", "Alabama"),
      ("AK", "Alaska"),
      ("AZ", "Arizona"),
      ("AR", "Arkansas"),
      ("CA", "California"),
      ("CO", "Colorado"),
      ("CT", "Connecticut"),
      ("DE", "Delaware"),
      ("DC", "District of Columbia"),
      ("FL", "Florida"),
      ("GA", "Georgia"),
      ("HI", "Hawaii"),
      ("ID", "Idaho"),
      ("IL", "Illinois"),
      ("IN", "Indiana"),
      ("IA", "Iowa"),
      ("KS", "Kansas"),
      ("KY", "Kentucky"),
      ("LA", "Louisiana"),
      ("ME", "Maine"),
      ("MD", "Maryland"),
      ("MA", "Massachusetts"),
      ("MI", "Michigan"),
      ("MN", "Minnesota"),
      ("MS", "Mississippi"),
      ("MO", "Missouri"),
      ("MT", "Montana"),
      ("NE", "Nebraska"),
      ("NV", "Nevada"),
      ("NH", "New Hampshire"),
      ("NJ", "New Jersey"),
      ("NM", "New Mexico"),
      ("NY", "New York"),
      ("NC", "North Carolina"),
      ("ND", "North Dakota"),
      ("OH", "Ohio"),
      ("OK", "Oklahoma"),
      ("OR", "Oregon"),
      ("PA", "Pennsylvania"),
      ("RI", "Rhode Island"),
      ("SC", "South Carolina"),
      ("SD", "South Dakota"),
      ("TN", "Tennessee"),
      ("TX", "Texas"),
      ("UT", "Utah"),
      ("VT", "Vermont"),
      ("VA", "Virginia"),
      ("WA", "Washington"),
      ("WV", "West Virginia"),
      ("WI", "Wisconsin"),
      ("WY", "Wyoming")
    )

    val stateList = rawStateList.toDF("State Code", "State Name")

    // Combined the two dataFrames to get state codes assocaited with area name.

    val combinedCensusData =
      censusData.join(stateList, $"Geographic Area Name" === $"State Name")


//Get a years worth of 2020 segment paths to spark.read for data
    //Read a whole years worth of data - Jan-Dec 2020
    // val ob1 = Source
    //   .fromFile("csv/part-00000-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob2 = Source
    //   .fromFile("csv/part-00001-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob3 = Source
    //   .fromFile("csv/part-00002-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")

    // new PrintWriter("csv1"){write(ob1+ob2+ob3);close()}

    //Dataframe to store results.
    var storedDF = Seq
      .empty[(String, String, Int, Int, Int)]
      .toDF(
        "State Code",
        "Geographic Area Name",
        "Tech Job Total",
        "Population Estimate Total",
        "Tech Ads Proportional to Population"
      )
    
    val bufferedSource = Source.fromFile("test.csv")
    for (line <- bufferedSource.getLines()) {
      val cc = spark.read
        .option("lineSep", "WARC/1.0")
        .text(line)
        .as[String]
        .map((str) => { str.substring(str.indexOf("\n") + 1) })
        .toDF("cut WET")

      val cuttingCrawl = cc
        .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
        .select(
          $"_tmp".getItem(0).as("WARC Header"),
          $"_tmp".getItem(1).as("Plain Text")
        )

      //find job urls
      val techJob = cuttingCrawl
        .filter(
          $"WARC Header" rlike ".*WARC-Target-URI:.*career.*" or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*")
        )
        //filter for tech ads
        .filter(
          $"Plain Text" rlike ".*Frontend.*" or ($"Plain Text" rlike ".*Backendend.*") or ($"Plain Text" rlike ".*Fullstack.*")
            or ($"Plain Text" rlike ".*Cybersecurity.*") or ($"Plain Text" rlike ".*Software.*") or ($"Plain Text" rlike ".*Computer.*")
        )

      // Turning Dataframe into RDD in order to get Key-Value pairs of occurrences of State Codes
      val sqlCrawl = techJob
        .select($"Plain Text")
        .as[String]
        .flatMap(line => line.split(" "))
        .rdd

      val rddCrawl = sqlCrawl
        .map(word => (word, 1))
        .filter({ case (key, value) => key.length < 3 })
        .reduceByKey(_ + _)
        .toDF("State Code", "Tech Job Total")

      // Join earlier combinedCensusData Dataframe to rddCrawl Dataframe in order to determine
      // question: "Where do we see relatively fewer tech ads proportional to population?"
      val combinedCrawl = rddCrawl
        .join(combinedCensusData, ("State Code"))
        .withColumn(
          "Tech Ads Proportional to Population",
          round(($"Tech Job Total" / $"Population Estimate Total" * 100), 8)
        )
        .select(
          $"State Code",
          $"Geographic Area Name",
          $"Tech Job Total",
          $"Population Estimate Total",
          $"Tech Ads Proportional to Population"
        )
        

      storedDF = storedDF.union(combinedCrawl)
    }
    storedDF.groupBy($"State Code").sum("Tech Job Total").show(102,false)
  }
}

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8930114054612414284/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8930114054612414284/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.06 10:34:52 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.06 10:34:53 INFO  time: code lens generation in 2.28s[0m
[0m2021.03.06 10:34:52 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.06 10:34:52 INFO  Attempting to connect to the build server...[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3714931961000635951/bsp.socket'...
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4498394013172644779/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4498394013172644779/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4498394013172644779/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.06 10:34:53 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3714931961000635951/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3714931961000635951/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.06 10:34:53 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.06 10:34:53 INFO  time: Connected to build server in 4.45s[0m
[0m2021.03.06 10:34:53 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.06 10:34:53 INFO  time: Imported build in 0.21s[0m
[0m2021.03.06 10:34:55 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.06 10:34:55 INFO  time: indexed workspace in 2.07s[0m
[0m2021.03.06 10:35:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 10:35:48 INFO  time: compiled root in 4.7s[0m
[0m2021.03.06 10:36:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 10:36:14 INFO  time: compiled root in 2.56s[0m
[0m2021.03.06 11:20:10 INFO  shutting down Metals[0m
[0m2021.03.06 11:20:10 INFO  No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
Shut down connection with build server.[0m
[0m2021.03.06 11:20:10 INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.06 11:20:10 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.06 16:06:34 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.54.1.[0m
[0m2021.03.06 16:06:34 INFO  time: initialize in 0.4s[0m
[0m2021.03.06 16:06:34 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4208529813939673336/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.06 16:06:34 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
[0m2021.03.06 16:06:35 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.06 16:06:36 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.DataFrame
import scala.io.Source
import java.nio.file.{Paths, Files}
import java.io.PrintWriter

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data.
    val rawStateList = Seq(
      ("AL", "Alabama"),
      ("AK", "Alaska"),
      ("AZ", "Arizona"),
      ("AR", "Arkansas"),
      ("CA", "California"),
      ("CO", "Colorado"),
      ("CT", "Connecticut"),
      ("DE", "Delaware"),
      ("DC", "District of Columbia"),
      ("FL", "Florida"),
      ("GA", "Georgia"),
      ("HI", "Hawaii"),
      ("ID", "Idaho"),
      ("IL", "Illinois"),
      ("IN", "Indiana"),
      ("IA", "Iowa"),
      ("KS", "Kansas"),
      ("KY", "Kentucky"),
      ("LA", "Louisiana"),
      ("ME", "Maine"),
      ("MD", "Maryland"),
      ("MA", "Massachusetts"),
      ("MI", "Michigan"),
      ("MN", "Minnesota"),
      ("MS", "Mississippi"),
      ("MO", "Missouri"),
      ("MT", "Montana"),
      ("NE", "Nebraska"),
      ("NV", "Nevada"),
      ("NH", "New Hampshire"),
      ("NJ", "New Jersey"),
      ("NM", "New Mexico"),
      ("NY", "New York"),
      ("NC", "North Carolina"),
      ("ND", "North Dakota"),
      ("OH", "Ohio"),
      ("OK", "Oklahoma"),
      ("OR", "Oregon"),
      ("PA", "Pennsylvania"),
      ("RI", "Rhode Island"),
      ("SC", "South Carolina"),
      ("SD", "South Dakota"),
      ("TN", "Tennessee"),
      ("TX", "Texas"),
      ("UT", "Utah"),
      ("VT", "Vermont"),
      ("VA", "Virginia"),
      ("WA", "Washington"),
      ("WV", "West Virginia"),
      ("WI", "Wisconsin"),
      ("WY", "Wyoming")
    )

    val stateList = rawStateList.toDF("State Code", "State Name")

    // Combined the two dataFrames to get state codes assocaited with area name.

    val combinedCensusData =
      censusData.join(stateList, $"Geographic Area Name" === $"State Name")

//Get a years worth of 2020 segment paths to spark.read for data
    //Read a whole years worth of data - Jan-Dec 2020
    // val ob1 = Source
    //   .fromFile("csv/part-00000-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob2 = Source
    //   .fromFile("csv/part-00001-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob3 = Source
    //   .fromFile("csv/part-00002-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")

    // new PrintWriter("csv1"){write(ob1+ob2+ob3);close()}

    //Dataframe to store results.
    var storedDF = Seq
      .empty[(String, String, Int, Int, Int)]
      .toDF(
        "State Code",
        "Geographic Area Name",
        "Tech Job Total",
        "Population Estimate Total",
        "Tech Ads Proportional to Population"
      )

    val bufferedSource = Source.fromFile("test.csv")
    for (line <- bufferedSource.getLines()) {
      val cc = spark.read
        .option("lineSep", "WARC/1.0")
        .text(line)
        .as[String]
        .map((str) => { str.substring(str.indexOf("\n") + 1) })
        .toDF("cut WET")

      val cuttingCrawl = cc
        .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
        .select(
          $"_tmp".getItem(0).as("WARC Header"),
          $"_tmp".getItem(1).as("Plain Text")
        )

      //find job urls
      val techJob = cuttingCrawl
        .filter(
          $"WARC Header" rlike ".*WARC-Target-URI:.*career.*" or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*")
        )
        //filter for tech ads
        .filter(
          $"Plain Text" rlike ".*Frontend.*" or ($"Plain Text" rlike ".*Backendend.*") or ($"Plain Text" rlike ".*Fullstack.*")
            or ($"Plain Text" rlike ".*Cybersecurity.*") or ($"Plain Text" rlike ".*Software.*") or ($"Plain Text" rlike ".*Computer.*")
        )

      // Turning Dataframe into RDD in order to get Key-Value pairs of occurrences of State Codes
      val sqlCrawl = techJob
        .select($"Plain Text")
        .as[String]
        .flatMap(line => line.split(" "))
        .rdd

      val rddCrawl = sqlCrawl
        .map(word => (word, 1))
        .filter({ case (key, value) => key.length < 3 })
        .reduceByKey(_ + _)
        .toDF("State Code", "Tech Job Total")

      // Join earlier combinedCensusData Dataframe to rddCrawl Dataframe in order to determine
      // question: "Where do we see relatively fewer tech ads proportional to population?"
      val combinedCrawl = rddCrawl
        .join(combinedCensusData, ("State Code"))
        .withColumn(
          "Tech Ads Proportional to Population",
          round(($"Tech Job Total" / $"Population Estimate Total" * 100), 8)
        )
        .select(
          $"State Code",
          $"Geographic Area Name",
          $"Tech Job Total",
          $"Population Estimate Total",
          $"Tech Ads Proportional to Population"
        )

      storedDF = storedDF.union(combinedCrawl)
    }
    storedDF
      .select(
        $"State Code",
        $"Geographic Area Name",
        $"Tech Job Total",
        $"Population Estimate Total",
        $"Tech Ads Proportional to Population"
      )
      .groupBy($"State Code")
      .sum("Tech Job Total")
      .show(102, false)
  }
}

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4208529813939673336/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4208529813939673336/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.06 16:06:39 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.06 16:06:39 INFO  time: code lens generation in 3.15s[0m
[0m2021.03.06 16:06:39 INFO  time: code lens generation in 4.13s[0m
[0m2021.03.06 16:06:39 INFO  time: code lens generation in 2.86s[0m
[0m2021.03.06 16:06:39 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0mOpening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8605921568436696910/bsp.socket'...2021.03.06 16:06:39 INFO  Attempting to connect to the build server...[0m

Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1941292370376501697/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1941292370376501697/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1941292370376501697/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.06 16:06:39 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8605921568436696910/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8605921568436696910/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.06 16:06:39 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.06 16:06:39 INFO  time: Connected to build server in 4.47s[0m
[0m2021.03.06 16:06:39 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.06 16:06:39 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
[0m2021.03.06 16:06:39 INFO  time: Imported build in 0.25s[0m
[0m2021.03.06 16:06:39 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.DataFrame
import scala.io.Source
import java.nio.file.{Paths, Files}
import java.io.PrintWriter

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data.
    val rawStateList = Seq(
      ("AL", "Alabama"),
      ("AK", "Alaska"),
      ("AZ", "Arizona"),
      ("AR", "Arkansas"),
      ("CA", "California"),
      ("CO", "Colorado"),
      ("CT", "Connecticut"),
      ("DE", "Delaware"),
      ("DC", "District of Columbia"),
      ("FL", "Florida"),
      ("GA", "Georgia"),
      ("HI", "Hawaii"),
      ("ID", "Idaho"),
      ("IL", "Illinois"),
      ("IN", "Indiana"),
      ("IA", "Iowa"),
      ("KS", "Kansas"),
      ("KY", "Kentucky"),
      ("LA", "Louisiana"),
      ("ME", "Maine"),
      ("MD", "Maryland"),
      ("MA", "Massachusetts"),
      ("MI", "Michigan"),
      ("MN", "Minnesota"),
      ("MS", "Mississippi"),
      ("MO", "Missouri"),
      ("MT", "Montana"),
      ("NE", "Nebraska"),
      ("NV", "Nevada"),
      ("NH", "New Hampshire"),
      ("NJ", "New Jersey"),
      ("NM", "New Mexico"),
      ("NY", "New York"),
      ("NC", "North Carolina"),
      ("ND", "North Dakota"),
      ("OH", "Ohio"),
      ("OK", "Oklahoma"),
      ("OR", "Oregon"),
      ("PA", "Pennsylvania"),
      ("RI", "Rhode Island"),
      ("SC", "South Carolina"),
      ("SD", "South Dakota"),
      ("TN", "Tennessee"),
      ("TX", "Texas"),
      ("UT", "Utah"),
      ("VT", "Vermont"),
      ("VA", "Virginia"),
      ("WA", "Washington"),
      ("WV", "West Virginia"),
      ("WI", "Wisconsin"),
      ("WY", "Wyoming")
    )

    val stateList = rawStateList.toDF("State Code", "State Name")

    // Combined the two dataFrames to get state codes assocaited with area name.

    val combinedCensusData =
      censusData.join(stateList, $"Geographic Area Name" === $"State Name")

//Get a years worth of 2020 segment paths to spark.read for data
    //Read a whole years worth of data - Jan-Dec 2020
    // val ob1 = Source
    //   .fromFile("csv/part-00000-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob2 = Source
    //   .fromFile("csv/part-00001-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob3 = Source
    //   .fromFile("csv/part-00002-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")

    // new PrintWriter("csv1"){write(ob1+ob2+ob3);close()}

    //Dataframe to store results.
    var storedDF = Seq
      .empty[(String, String, Int, Int, Int)]
      .toDF(
        "State Code",
        "Geographic Area Name",
        "Tech Job Total",
        "Population Estimate Total",
        "Tech Ads Proportional to Population"
      )

    val bufferedSource = Source.fromFile("test.csv")
    for (line <- bufferedSource.getLines()) {
      val cc = spark.read
        .option("lineSep", "WARC/1.0")
        .text(line)
        .as[String]
        .map((str) => { str.substring(str.indexOf("\n") + 1) })
        .toDF("cut WET")

      val cuttingCrawl = cc
        .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
        .select(
          $"_tmp".getItem(0).as("WARC Header"),
          $"_tmp".getItem(1).as("Plain Text")
        )

      //find job urls
      val techJob = cuttingCrawl
        .filter(
          $"WARC Header" rlike ".*WARC-Target-URI:.*career.*" or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*")
        )
        //filter for tech ads
        .filter(
          $"Plain Text" rlike ".*Frontend.*" or ($"Plain Text" rlike ".*Backendend.*") or ($"Plain Text" rlike ".*Fullstack.*")
            or ($"Plain Text" rlike ".*Cybersecurity.*") or ($"Plain Text" rlike ".*Software.*") or ($"Plain Text" rlike ".*Computer.*")
        )

      // Turning Dataframe into RDD in order to get Key-Value pairs of occurrences of State Codes
      val sqlCrawl = techJob
        .select($"Plain Text")
        .as[String]
        .flatMap(line => line.split(" "))
        .rdd

      val rddCrawl = sqlCrawl
        .map(word => (word, 1))
        .filter({ case (key, value) => key.length < 3 })
        .reduceByKey(_ + _)
        .toDF("State Code", "Tech Job Total")

      // Join earlier combinedCensusData Dataframe to rddCrawl Dataframe in order to determine
      // question: "Where do we see relatively fewer tech ads proportional to population?"
      val combinedCrawl = rddCrawl
        .join(combinedCensusData, ("State Code"))
        .withColumn(
          "Tech Ads Proportional to Population",
          round(($"Tech Job Total" / $"Population Estimate Total" * 100), 8)
        )
        .select(
          $"State Code",
          $"Geographic Area Name",
          $"Tech Job Total",
          $"Population Estimate Total",
          $"Tech Ads Proportional to Population"
        )

      storedDF = storedDF.union(combinedCrawl)
    }
    storedDF
      .groupBy($"State Code")
      .sum("Tech Job Total")
      .select(
        $"State Code",
        $"Geographic Area Name",
        $"Tech Job Total",
        $"Population Estimate Total",
        $"Tech Ads Proportional to Population"
      )
      .show(102, false)
  }
}

[0m2021.03.06 16:06:41 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.06 16:06:41 INFO  time: indexed workspace in 2.54s[0m
[0m2021.03.06 16:06:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:06:48 INFO  time: compiled root in 5.94s[0m
Mar 06, 2021 4:08:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 60
Mar 06, 2021 4:08:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 57
Mar 06, 2021 4:08:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 59
[0m2021.03.06 16:08:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:08:29 INFO  time: compiled root in 2.6s[0m
[0m2021.03.06 16:09:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:09:20 INFO  time: compiled root in 2.17s[0m
[0m2021.03.06 16:11:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:11:08 INFO  time: compiled root in 2s[0m
[0m2021.03.06 16:11:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:11:12 INFO  time: compiled root in 1.71s[0m
[0m2021.03.06 16:11:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:11:42 INFO  time: compiled root in 1.54s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.03.06 16:14:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:14:11 INFO  time: compiled root in 1.79s[0m
[0m2021.03.06 16:14:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:14:20 INFO  time: compiled root in 1.26s[0m
[0m2021.03.06 16:15:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:15:19 INFO  time: compiled root in 0.22s[0m
[0m2021.03.06 16:15:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:15:25 INFO  time: compiled root in 0.13s[0m
[0m2021.03.06 16:15:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:15:52 INFO  time: compiled root in 0.14s[0m
[0m2021.03.06 16:15:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:15:56 INFO  time: compiled root in 0.15s[0m
[0m2021.03.06 16:15:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:16:01 INFO  time: compiled root in 1.48s[0m
[0m2021.03.06 16:16:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:16:17 INFO  time: compiled root in 1.22s[0m
[0m2021.03.06 16:19:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:19:39 INFO  time: compiled root in 1.33s[0m
[0m2021.03.06 16:20:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:20:30 INFO  time: compiled root in 1.21s[0m
[0m2021.03.06 16:21:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:21:13 INFO  time: compiled root in 0.12s[0m
[0m2021.03.06 16:21:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:21:18 INFO  time: compiled root in 1.18s[0m
[0m2021.03.06 16:21:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 16:21:26 INFO  time: compiled root in 1.22s[0m
[0m2021.03.06 17:29:36 INFO  shutting down Metals[0m
[0m2021.03.06 17:29:36 INFO  Shut down connection with build server.[0m
[0m2021.03.06 17:29:36 INFO  Shut down connection with build server.[0m
[0m2021.03.06 17:29:36 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.08 09:17:42 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/scala_s3_read' for client vscode 1.54.1.[0m
[0m2021.03.08 09:17:42 INFO  time: initialize in 0.38s[0m
[0m2021.03.08 09:17:42 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5112915734414226403/bsp.socket'...
[0m2021.03.08 09:17:42 WARN  no build target for: /home/amburkee/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
Waiting for the bsp connection to come up...
[0m2021.03.08 09:17:42 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.08 09:17:44 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.DataFrame
import scala.io.Source
import java.nio.file.{Paths, Files}
import java.io.PrintWriter

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scala-s3-read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data.
    val rawStateList = Seq(
      ("AL", "Alabama"),
      ("AK", "Alaska"),
      ("AZ", "Arizona"),
      ("AR", "Arkansas"),
      ("CA", "California"),
      ("CO", "Colorado"),
      ("CT", "Connecticut"),
      ("DE", "Delaware"),
      ("DC", "District of Columbia"),
      ("FL", "Florida"),
      ("GA", "Georgia"),
      ("HI", "Hawaii"),
      ("ID", "Idaho"),
      ("IL", "Illinois"),
      ("IN", "Indiana"),
      ("IA", "Iowa"),
      ("KS", "Kansas"),
      ("KY", "Kentucky"),
      ("LA", "Louisiana"),
      ("ME", "Maine"),
      ("MD", "Maryland"),
      ("MA", "Massachusetts"),
      ("MI", "Michigan"),
      ("MN", "Minnesota"),
      ("MS", "Mississippi"),
      ("MO", "Missouri"),
      ("MT", "Montana"),
      ("NE", "Nebraska"),
      ("NV", "Nevada"),
      ("NH", "New Hampshire"),
      ("NJ", "New Jersey"),
      ("NM", "New Mexico"),
      ("NY", "New York"),
      ("NC", "North Carolina"),
      ("ND", "North Dakota"),
      ("OH", "Ohio"),
      ("OK", "Oklahoma"),
      ("OR", "Oregon"),
      ("PA", "Pennsylvania"),
      ("RI", "Rhode Island"),
      ("SC", "South Carolina"),
      ("SD", "South Dakota"),
      ("TN", "Tennessee"),
      ("TX", "Texas"),
      ("UT", "Utah"),
      ("VT", "Vermont"),
      ("VA", "Virginia"),
      ("WA", "Washington"),
      ("WV", "West Virginia"),
      ("WI", "Wisconsin"),
      ("WY", "Wyoming")
    )

    val stateList = rawStateList.toDF("State Code", "State Name")

    // Combined the two dataFrames to get state codes assocaited with area name.

    val combinedCensusData =
      censusData.join(stateList, $"Geographic Area Name" === $"State Name")

//Get a years worth of 2020 segment paths to spark.read for data
    //Read a whole years worth of data - Jan-Dec 2020
    // val ob1 = Source
    //   .fromFile("csv/part-00000-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob2 = Source
    //   .fromFile("csv/part-00001-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob3 = Source
    //   .fromFile("csv/part-00002-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3a://commoncrawl/" + _)
    //   .mkString("\n")

    // new PrintWriter("csv1"){write(ob1+ob2+ob3);close()}

    //Dataframe to store results.
    var storedDF = Seq
      .empty[(String, String, Int, Int)]
      .toDF(
        "State Code",
        "Geographic Area Name",
        "Tech Job Total",
        "Population Estimate Total"
      )

    val bufferedSource = Source.fromFile("test.csv")
    for (line <- bufferedSource.getLines()) {
      val cc = spark.read
        .option("lineSep", "WARC/1.0")
        .text(line)
        .as[String]
        .map((str) => { str.substring(str.indexOf("\n") + 1) })
        .toDF("cut WET")

      val cuttingCrawl = cc
        .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
        .select(
          $"_tmp".getItem(0).as("WARC Header"),
          $"_tmp".getItem(1).as("Plain Text")
        )

      //find job urls
      val techJob = cuttingCrawl
        .filter(
          $"WARC Header" rlike ".*WARC-Target-URI:.*career.*" or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*")
        )
        //filter for tech ads
        .filter(
          $"Plain Text" rlike ".*Frontend.*" or ($"Plain Text" rlike ".*Backendend.*") or ($"Plain Text" rlike ".*Fullstack.*")
            or ($"Plain Text" rlike ".*Cybersecurity.*") or ($"Plain Text" rlike ".*Software.*") or ($"Plain Text" rlike ".*Computer.*")
        )

      // Turning Dataframe into RDD in order to get Key-Value pairs of occurrences of State Codes
      val sqlCrawl = techJob
        .select($"Plain Text")
        .as[String]
        .flatMap(line => line.split(" "))
        .rdd

      val rddCrawl = sqlCrawl
        .map(word => (word, 1))
        .filter({ case (key, value) => key.length < 3 })
        .reduceByKey(_ + _)
        .toDF("State Code", "Tech Job Total")

      // Join earlier combinedCensusData Dataframe to rddCrawl Dataframe in order to determine
      // question: "Where do we see relatively fewer tech ads proportional to population?"
      val combinedCrawl = rddCrawl
        .join(combinedCensusData, ("State Code"))
        .select(
          $"State Code",
          $"Geographic Area Name",
          $"Tech Job Total",
          $"Population Estimate Total"
        )

      storedDF = storedDF.union(combinedCrawl)
    }
    storedDF
      .groupBy($"State Code", $"Population Estimate Total")
      .sum("Tech Job Total")
      .withColumn(
        "Tech Ads Proportional to Population",
        round(($"sum(Tech Job Total)" / $"Population Estimate Total" * 100), 8)
      )
      .show(102, false)
  }
}

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/amburkee/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5112915734414226403/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5112915734414226403/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.08 09:17:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.08 09:17:46 INFO  time: code lens generation in 3.6s[0m
[0m2021.03.08 09:17:46 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.08 09:17:46 INFO  Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6685577802376353089/bsp.socket'...Attempting to connect to the build server...
[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2043318226515615670/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6685577802376353089/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6685577802376353089/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2043318226515615670/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2043318226515615670/bsp.socket...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.08 09:17:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.08 09:17:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.08 09:17:47 INFO  time: Connected to build server in 4.23s[0m
[0m2021.03.08 09:17:47 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.08 09:17:47 INFO  time: Imported build in 0.21s[0m
[0m2021.03.08 09:17:48 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.08 09:17:48 INFO  time: indexed workspace in 2.01s[0m
[0m2021.03.08 10:06:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 10:06:54 INFO  time: compiled root in 5.7s[0m
[0m2021.03.08 10:07:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 10:07:42 INFO  time: compiled root in 2.53s[0m
[0m2021.03.08 10:24:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 10:24:45 INFO  time: compiled root in 1.89s[0m
[0m2021.03.08 10:25:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 10:25:52 INFO  time: compiled root in 1.85s[0m
[0m2021.03.08 10:28:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 10:28:05 INFO  time: compiled root in 1.68s[0m
[0m2021.03.08 10:31:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 10:31:40 INFO  time: compiled root in 1.49s[0m
Mar 08, 2021 10:39:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 572
[0m2021.03.08 10:40:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 10:40:23 INFO  time: compiled root in 1.6s[0m
Mar 08, 2021 10:40:48 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 656
[0m2021.03.08 10:41:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 10:41:54 INFO  time: compiled root in 1.44s[0m
[0m2021.03.08 11:13:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:13:28 INFO  time: compiled root in 1.46s[0m
Mar 08, 2021 11:18:55 AM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTraceNotification
Mar 08, 2021 11:18:55 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "workspace/didChangeWatchedFiles",
  "params": {
    "changes": [
      {
        "uri": "file:///home/amburkee/scala_s3_read/build.sbt",
        "type": 3
      },
      {
        "uri": "file:///home/amburkee/scala_s3_read/project/metals.sbt",
        "type": 3
      },
      {
        "uri": "file:///home/amburkee/scala_s3_read/project/plugins.sbt",
        "type": 3
      },
      {
        "uri": "file:///home/amburkee/scala_s3_read/project/Dependencies.scala",
        "type": 3
      },
      {
        "uri": "file:///home/amburkee/scala_s3_read/project/project/metals.sbt",
        "type": 3
      },
      {
        "uri": "file:///home/amburkee/scala_s3_read/project/build.properties",
        "type": 3
      }
    ]
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/amburkee/scala_s3_read/build.sbt
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1186)
	... 16 more

[0m2021.03.08 11:19:33 INFO  shutting down Metals[0m
[0m2021.03.08 11:19:33 INFO  Shut down connection with build server.[0m
[0m2021.03.08 11:19:33 INFO  Shut down connection with build server.[0m
[0m2021.03.08 11:19:33 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.08 11:19:39 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/Project3/scala_s3_read' for client vscode 1.54.1.[0m
[0m2021.03.08 11:19:40 INFO  time: initialize in 0.41s[0m
[0m2021.03.08 11:19:43 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6578049494453153381/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.08 11:19:43 WARN  no build target for: /home/amburkee/Project3/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/Project3/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/Project3/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/amburkee/Project3/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6578049494453153381/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6578049494453153381/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.08 11:19:44 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.08 11:19:44 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7267103862717252119/bsp.socket'...
[0m2021.03.08 11:19:44 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1120858922979896360/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1120858922979896360/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1120858922979896360/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7267103862717252119/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7267103862717252119/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
[0m2021.03.08 11:19:44 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.08 11:19:44 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.08 11:19:44 INFO  time: Connected to build server in 0.72s[0m
[0m2021.03.08 11:19:44 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.08 11:19:45 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.DataFrame
import scala.io.Source
import java.nio.file.{Paths, Files}
import java.io.PrintWriter

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("q1_Population_TechAd")
      // .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data.
    val rawStateList = Seq(
      ("AL", "Alabama"),
      ("AK", "Alaska"),
      ("AZ", "Arizona"),
      ("AR", "Arkansas"),
      ("CA", "California"),
      ("CO", "Colorado"),
      ("CT", "Connecticut"),
      ("DE", "Delaware"),
      ("DC", "District of Columbia"),
      ("FL", "Florida"),
      ("GA", "Georgia"),
      ("HI", "Hawaii"),
      ("ID", "Idaho"),
      ("IL", "Illinois"),
      ("IN", "Indiana"),
      ("IA", "Iowa"),
      ("KS", "Kansas"),
      ("KY", "Kentucky"),
      ("LA", "Louisiana"),
      ("ME", "Maine"),
      ("MD", "Maryland"),
      ("MA", "Massachusetts"),
      ("MI", "Michigan"),
      ("MN", "Minnesota"),
      ("MS", "Mississippi"),
      ("MO", "Missouri"),
      ("MT", "Montana"),
      ("NE", "Nebraska"),
      ("NV", "Nevada"),
      ("NH", "New Hampshire"),
      ("NJ", "New Jersey"),
      ("NM", "New Mexico"),
      ("NY", "New York"),
      ("NC", "North Carolina"),
      ("ND", "North Dakota"),
      ("OH", "Ohio"),
      ("OK", "Oklahoma"),
      ("OR", "Oregon"),
      ("PA", "Pennsylvania"),
      ("RI", "Rhode Island"),
      ("SC", "South Carolina"),
      ("SD", "South Dakota"),
      ("TN", "Tennessee"),
      ("TX", "Texas"),
      ("UT", "Utah"),
      ("VT", "Vermont"),
      ("VA", "Virginia"),
      ("WA", "Washington"),
      ("WV", "West Virginia"),
      ("WI", "Wisconsin"),
      ("WY", "Wyoming")
    )

    val stateList = rawStateList.toDF("State Code", "State Name")

    // Combined the two dataFrames to get state codes assocaited with area name.

    val combinedCensusData =
      censusData.join(stateList, $"Geographic Area Name" === $"State Name")

//Get a years worth of 2020 segment paths to spark.read for data
    //Read a whole years worth of data - Jan-Dec 2020
    // val ob1 = Source
    //   .fromFile("csv/part-00000-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob2 = Source
    //   .fromFile("csv/part-00001-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob3 = Source
    //   .fromFile("csv/part-00002-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3://commoncrawl/" + _)
    //   .mkString("\n")

    // new PrintWriter("yearCsv"){write(ob1+ob2+ob3);close()}

    //Dataframe to store results.
    var storedDF = Seq
      .empty[(String, String, Int, Int)]
      .toDF(
        "State Code",
        "Geographic Area Name",
        "Tech Job Total",
        "Population Estimate Total"
      )

      //read each line, extract the data, filter for tech job ads, add to storedDF
    val bufferedSource = Source.fromFile("yearCsv.csv")
    for (line <- bufferedSource.getLines()) {
      val cc = spark.read
        .option("lineSep", "WARC/1.0")
        .text(line)
        .as[String]
        .map((str) => { str.substring(str.indexOf("\n") + 1) })
        .toDF("cut WET")

      val cuttingCrawl = cc
        .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
        .select(
          $"_tmp".getItem(0).as("WARC Header"),
          $"_tmp".getItem(1).as("Plain Text")
        )

      //find job/career/employment in urls
      val techJob = cuttingCrawl
        .filter(
          $"WARC Header" rlike ".*WARC-Target-URI:.*career.*" or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*")
        )
        //filter for tech ads
        .filter(
          $"Plain Text" rlike ".*Frontend.*" or ($"Plain Text" rlike ".*Backendend.*") or ($"Plain Text" rlike ".*Fullstack.*")
            or ($"Plain Text" rlike ".*Cybersecurity.*") or ($"Plain Text" rlike ".*Software.*") or ($"Plain Text" rlike ".*Computer.*")
        )

      // Turning Dataframe into RDD in order to get Key-Value pairs of occurrences of State Codes
      val sqlCrawl = techJob
        .select($"Plain Text")
        .as[String]
        .flatMap(line => line.split(" "))
        .rdd

      val rddCrawl = sqlCrawl
        .map(word => (word, 1))
        .filter({ case (key, value) => key.length < 3 })
        .reduceByKey(_ + _)
        .toDF("State Code", "Tech Job Total")

      // Join earlier combinedCensusData Dataframe to rddCrawl Dataframe in order to determine
      val combinedCrawl = rddCrawl
        .join(combinedCensusData, ("State Code"))
        .select(
          $"State Code",
          $"Geographic Area Name",
          $"Tech Job Total",
          $"Population Estimate Total"
        )

      storedDF = storedDF.union(combinedCrawl)
    }
    //combine and print results from a years worth of common crawl data
    storedDF
      .groupBy($"State Code", $"Population Estimate Total")
      .sum("Tech Job Total")
      .withColumn(
        "Tech Ads Proportional to Population",
        round(($"sum(Tech Job Total)" / $"Population Estimate Total" * 100), 8)
      )
      .show(102, false)
  }
}

[0m2021.03.08 11:19:48 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals4245565076247015701/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.03.08 11:19:48 INFO  time: code lens generation in 2.04s[0m
[0m2021.03.08 11:19:48 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.DataFrame
import scala.io.Source
import java.nio.file.{Paths, Files}
import java.io.PrintWriter

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("q1_Population_TechAd")
      // .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data.
    val rawStateList = Seq(
      ("AL", "Alabama"),
      ("AK", "Alaska"),
      ("AZ", "Arizona"),
      ("AR", "Arkansas"),
      ("CA", "California"),
      ("CO", "Colorado"),
      ("CT", "Connecticut"),
      ("DE", "Delaware"),
      ("DC", "District of Columbia"),
      ("FL", "Florida"),
      ("GA", "Georgia"),
      ("HI", "Hawaii"),
      ("ID", "Idaho"),
      ("IL", "Illinois"),
      ("IN", "Indiana"),
      ("IA", "Iowa"),
      ("KS", "Kansas"),
      ("KY", "Kentucky"),
      ("LA", "Louisiana"),
      ("ME", "Maine"),
      ("MD", "Maryland"),
      ("MA", "Massachusetts"),
      ("MI", "Michigan"),
      ("MN", "Minnesota"),
      ("MS", "Mississippi"),
      ("MO", "Missouri"),
      ("MT", "Montana"),
      ("NE", "Nebraska"),
      ("NV", "Nevada"),
      ("NH", "New Hampshire"),
      ("NJ", "New Jersey"),
      ("NM", "New Mexico"),
      ("NY", "New York"),
      ("NC", "North Carolina"),
      ("ND", "North Dakota"),
      ("OH", "Ohio"),
      ("OK", "Oklahoma"),
      ("OR", "Oregon"),
      ("PA", "Pennsylvania"),
      ("RI", "Rhode Island"),
      ("SC", "South Carolina"),
      ("SD", "South Dakota"),
      ("TN", "Tennessee"),
      ("TX", "Texas"),
      ("UT", "Utah"),
      ("VT", "Vermont"),
      ("VA", "Virginia"),
      ("WA", "Washington"),
      ("WV", "West Virginia"),
      ("WI", "Wisconsin"),
      ("WY", "Wyoming")
    )

    val stateList = rawStateList.toDF("State Code", "State Name")

    // Combined the two dataFrames to get state codes assocaited with area name.

    val combinedCensusData =
      censusData.join(stateList, $"Geographic Area Name" === $"State Name")

//Get a years worth of 2020 segment paths to spark.read for data
    //Read a whole years worth of data - Jan-Dec 2020
    // val ob1 = Source
    //   .fromFile("csv/part-00000-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob2 = Source
    //   .fromFile("csv/part-00001-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob3 = Source
    //   .fromFile("csv/part-00002-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3://commoncrawl/" + _)
    //   .mkString("\n")

    // new PrintWriter("yearCsv"){write(ob1+ob2+ob3);close()}

    //Dataframe to store results.
    var storedDF = Seq
      .empty[(String, String, Int, Int)]
      .toDF(
        "State Code",
        "Geographic Area Name",
        "Tech Job Total",
        "Population Estimate Total"
      )

      //read each line, extract the data, filter for tech job ads, add to storedDF
    val bufferedSource = Source.fromFile("yearCsv.csv")
    for (line <- bufferedSource.getLines()) {
      val cc = spark.read
        .option("lineSep", "WARC/1.0")
        .text(line)
        .as[String]
        .map((str) => { str.substring(str.indexOf("\n") + 1) })
        .toDF("cut WET")

      val cuttingCrawl = cc
        .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
        .select(
          $"_tmp".getItem(0).as("WARC Header"),
          $"_tmp".getItem(1).as("Plain Text")
        )

      //find job/career/employment in urls
      val techJob = cuttingCrawl
        .filter(
          $"WARC Header" rlike ".*WARC-Target-URI:.*career.*" or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*")
        )
        //filter for tech ads
        .filter(
          $"Plain Text" rlike ".*Frontend.*" or ($"Plain Text" rlike ".*Backendend.*") or ($"Plain Text" rlike ".*Fullstack.*")
            or ($"Plain Text" rlike ".*Cybersecurity.*") or ($"Plain Text" rlike ".*Software.*") or ($"Plain Text" rlike ".*Computer.*")
        )

      // Turning Dataframe into RDD in order to get Key-Value pairs of occurrences of State Codes
      val sqlCrawl = techJob
        .select($"Plain Text")
        .as[String]
        .flatMap(line => line.split(" "))
        .rdd

      val rddCrawl = sqlCrawl
        .map(word => (word, 1))
        .filter({ case (key, value) => key.length < 3 })
        .reduceByKey(_ + _)
        .toDF("State Code", "Tech Job Total")

      // Join earlier combinedCensusData Dataframe to rddCrawl Dataframe in order to determine
      val combinedCrawl = rddCrawl
        .join(combinedCensusData, ("State Code"))
        .select(
          $"State Code",
          $"Geographic Area Name",
          $"Tech Job Total",
          $"Population Estimate Total"
        )

      storedDF = storedDF.union(combinedCrawl)
    }
    //combine and print results from a years worth of common crawl data
    storedDF
      .groupBy($"State Code", $"Population Estimate Total")
      .sum("Tech Job Total")
      .withColumn(
        "Tech Ads Proportional to Population",
        round(($"sum(Tech Job Total)" / $"Population Estimate Total" * 100), 8)
      )
      .show(102, false)
  }
}

[0m2021.03.08 11:19:48 INFO  time: code lens generation in 4.6s[0m
[0m2021.03.08 11:19:49 INFO  time: code lens generation in 4.81s[0m
[0m2021.03.08 11:19:49 INFO  time: code lens generation in 4.83s[0m
[0m2021.03.08 11:19:49 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.03.08 11:19:49 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.08 11:19:49 INFO  time: indexed workspace in 4.25s[0m
[0m2021.03.08 11:19:49 WARN  no build target for: /home/amburkee/Project3/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
[0m2021.03.08 11:19:49 INFO  [info] loading settings for project scala_s3_read-build-build-build from metals.sbt ...[0m
[0m2021.03.08 11:19:50 INFO  no build target: using presentation compiler with only scala-library: 2.12.12[0m
[0m2021.03.08 11:19:51 INFO  [info] loading project definition from /home/amburkee/Project3/scala_s3_read/project/project/project[0m
[0m2021.03.08 11:19:51 INFO  [info] loading settings for project scala_s3_read-build-build from metals.sbt ...[0m
[0m2021.03.08 11:19:51 INFO  [info] loading project definition from /home/amburkee/Project3/scala_s3_read/project/project[0m
[0m2021.03.08 11:19:54 INFO  [success] Generated .bloop/scala_s3_read-build-build.json[0m
[0m2021.03.08 11:19:54 INFO  [success] Total time: 2 s, completed Mar 8, 2021 11:19:54 AM[0m
[0m2021.03.08 11:19:54 INFO  [info] loading settings for project scala_s3_read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.03.08 11:19:54 INFO  [info] loading project definition from /home/amburkee/Project3/scala_s3_read/project[0m
[0m2021.03.08 11:19:55 INFO  [success] Generated .bloop/scala_s3_read-build.json[0m
[0m2021.03.08 11:19:55 INFO  [success] Total time: 1 s, completed Mar 8, 2021 11:19:55 AM[0m
[0m2021.03.08 11:19:55 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.03.08 11:19:55 INFO  [info] set current project to scalas3read (in build file:/home/amburkee/Project3/scala_s3_read/)[0m
[0m2021.03.08 11:19:56 INFO  [success] Generated .bloop/root.json[0m
[0m2021.03.08 11:19:56 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.03.08 11:19:56 INFO  [success] Total time: 1 s, completed Mar 8, 2021 11:19:57 AM[0m
[0m2021.03.08 11:19:56 INFO  sbt bloopInstall exit: 0[0m
[0m2021.03.08 11:19:57 INFO  time: ran 'sbt bloopInstall' in 9.56s[0m
[0m2021.03.08 11:19:57 INFO  Disconnecting from Bloop session...[0m
[0m2021.03.08 11:19:57 INFO  Shut down connection with build server.[0m
[0m2021.03.08 11:19:57 INFO  Shut down connection with build server.[0m
[0m2021.03.08 11:19:57 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.08 11:19:57 INFO  Attempting to connect to the build server...[0m
[0m2021.03.08 11:19:57 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.08 11:19:57 INFO  Attempting to connect to the build server...[0m
[0m2021.03.08 11:19:57 INFO  Attempting to connect to the build server...[0m
[0m2021.03.08 11:19:57 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.08 11:19:57 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.08 11:19:57 INFO  time: Connected to build server in 0.14s[0m
[0m2021.03.08 11:19:57 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.08 11:19:59 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.08 11:19:59 INFO  time: indexed workspace in 1.34s[0m
[0m2021.03.08 11:19:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:20:00 INFO  time: compiled root in 1.73s[0m
[0m2021.03.08 11:45:12 INFO  shutting down Metals[0m
[0m2021.03.08 11:45:12 INFO  Shut down connection with build server.[0m
[0m2021.03.08 11:45:12 INFO  Shut down connection with build server.[0m
[0m2021.03.08 11:45:12 INFO  Shut down connection with build server.[0m
[0m2021.03.08 14:08:42 INFO  Started: Metals version 0.10.0 in workspace '/home/amburkee/Project3/scala_s3_read' for client vscode 1.54.1.[0m
[0m2021.03.08 14:08:42 INFO  time: initialize in 0.41s[0m
[0m2021.03.08 14:08:42 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.08 14:08:42 WARN  no build target for: /home/amburkee/Project3/scala_s3_read/src/main/scala/com/revature/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1633904190448282734/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.08 14:08:42 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.08 14:08:44 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature

import org.apache.spark.sql.SparkSession
import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicAWSCredentials
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.DataFrame
import scala.io.Source
import java.nio.file.{Paths, Files}
import java.io.PrintWriter

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("q1_Population_TechAd")
      // .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWSAccessKeyId"))
    val secret = System.getenv(("AWSSecretKey"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3.endpoint", "s3.amazonaws.com")

    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Tech ads proportional to population- Where do we see relatively fewer tech ads proportional to population?

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data.
    val rawStateList = Seq(
      ("AL", "Alabama"),
      ("AK", "Alaska"),
      ("AZ", "Arizona"),
      ("AR", "Arkansas"),
      ("CA", "California"),
      ("CO", "Colorado"),
      ("CT", "Connecticut"),
      ("DE", "Delaware"),
      ("DC", "District of Columbia"),
      ("FL", "Florida"),
      ("GA", "Georgia"),
      ("HI", "Hawaii"),
      ("ID", "Idaho"),
      ("IL", "Illinois"),
      ("IN", "Indiana"),
      ("IA", "Iowa"),
      ("KS", "Kansas"),
      ("KY", "Kentucky"),
      ("LA", "Louisiana"),
      ("ME", "Maine"),
      ("MD", "Maryland"),
      ("MA", "Massachusetts"),
      ("MI", "Michigan"),
      ("MN", "Minnesota"),
      ("MS", "Mississippi"),
      ("MO", "Missouri"),
      ("MT", "Montana"),
      ("NE", "Nebraska"),
      ("NV", "Nevada"),
      ("NH", "New Hampshire"),
      ("NJ", "New Jersey"),
      ("NM", "New Mexico"),
      ("NY", "New York"),
      ("NC", "North Carolina"),
      ("ND", "North Dakota"),
      ("OH", "Ohio"),
      ("OK", "Oklahoma"),
      ("OR", "Oregon"),
      ("PA", "Pennsylvania"),
      ("RI", "Rhode Island"),
      ("SC", "South Carolina"),
      ("SD", "South Dakota"),
      ("TN", "Tennessee"),
      ("TX", "Texas"),
      ("UT", "Utah"),
      ("VT", "Vermont"),
      ("VA", "Virginia"),
      ("WA", "Washington"),
      ("WV", "West Virginia"),
      ("WI", "Wisconsin"),
      ("WY", "Wyoming")
    )

    val stateList = rawStateList.toDF("State Code", "State Name")

    // Combined the two dataFrames to get state codes assocaited with area name.

    val combinedCensusData =
      censusData.join(stateList, $"Geographic Area Name" === $"State Name")

//Get a years worth of 2020 segment paths to spark.read for data
    //Read a whole years worth of data - Jan-Dec 2020
    // val ob1 = Source
    //   .fromFile("csv/part-00000-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob2 = Source
    //   .fromFile("csv/part-00001-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3://commoncrawl/" + _)
    //   .mkString("\n")
    // val ob3 = Source
    //   .fromFile("csv/part-00002-ba7d968d-cce3-4697-bc0c-037e17afb098-c000.csv")
    //   .getLines()
    //   .map("s3://commoncrawl/" + _)
    //   .mkString("\n")

    // new PrintWriter("yearCsv"){write(ob1+ob2+ob3);close()}

    //Dataframe to store results.
    var storedDF = Seq
      .empty[(String, String, Int, Int)]
      .toDF(
        "State Code",
        "Geographic Area Name",
        "Tech Job Total",
        "Population Estimate Total"
      )

      //read each line, extract the data, filter for tech job ads, add to storedDF
    val bufferedSource = Source.fromFile("yearCsv.csv")
    for (line <- bufferedSource.getLines()) {
      val cc = spark.read
        .option("lineSep", "WARC/1.0")
        .text(line)
        .as[String]
        .map((str) => { str.substring(str.indexOf("\n") + 1) })
        .toDF("cut WET")

      val cuttingCrawl = cc
        .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
        .select(
          $"_tmp".getItem(0).as("WARC Header"),
          $"_tmp".getItem(1).as("Plain Text")
        )

      //find job/career/employment in urls
      val techJob = cuttingCrawl
        .filter(
          $"WARC Header" rlike ".*WARC-Target-URI:.*career.*" or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*")
        )
        //filter for tech ads
        .filter(
          $"Plain Text" rlike ".*Frontend.*" or ($"Plain Text" rlike ".*Backendend.*") or ($"Plain Text" rlike ".*Fullstack.*")
            or ($"Plain Text" rlike ".*Cybersecurity.*") or ($"Plain Text" rlike ".*Software.*") or ($"Plain Text" rlike ".*Computer.*")
        )

      // Turning Dataframe into RDD in order to get Key-Value pairs of occurrences of State Codes
      val sqlCrawl = techJob
        .select($"Plain Text")
        .as[String]
        .flatMap(line => line.split(" "))
        .rdd

      val rddCrawl = sqlCrawl
        .map(word => (word, 1))
        .filter({ case (key, value) => key.length < 3 })
        .reduceByKey(_ + _)
        .toDF("State Code", "Tech Job Total")

      // Join earlier combinedCensusData Dataframe to rddCrawl Dataframe in order to determine
      val combinedCrawl = rddCrawl
        .join(combinedCensusData, ("State Code"))
        .select(
          $"State Code",
          $"Geographic Area Name",
          $"Tech Job Total",
          $"Population Estimate Total"
        )

      storedDF = storedDF.union(combinedCrawl)
    }
    //combine and print results from a years worth of common crawl data
    storedDF
      .groupBy($"State Code", $"Population Estimate Total")
      .sum("Tech Job Total")
      .withColumn(
        "Tech Ads Proportional to Population",
        round(($"sum(Tech Job Total)" / $"Population Estimate Total" * 100), 8)
      )
      .show(102, false)
  }
}

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/amburkee/Project3/scala_s3_read/.bloop'...
[0m[32m[D][0m Loading project from '/home/amburkee/Project3/scala_s3_read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/amburkee/Project3/scala_s3_read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/amburkee/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/amburkee/Project3/scala_s3_read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1633904190448282734/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1633904190448282734/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.08 14:08:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.08 14:08:46 INFO  time: code lens generation in 3.55s[0m
[0m2021.03.08 14:08:46 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0mOpening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6191470962236201954/bsp.socket'...
2021.03.08 14:08:46 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher35669376122736929/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher35669376122736929/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher35669376122736929/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.08 14:08:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6191470962236201954/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6191470962236201954/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.08 14:08:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/amburkee/.cache/metals/bsp.trace.json[0m
[0m2021.03.08 14:08:47 INFO  time: Connected to build server in 4.03s[0m
[0m2021.03.08 14:08:47 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.08 14:08:47 INFO  time: Imported build in 0.2s[0m
[0m2021.03.08 14:08:48 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.08 14:08:48 INFO  time: indexed workspace in 2s[0m
